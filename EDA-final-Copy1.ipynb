{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1876fa72",
   "metadata": {},
   "source": [
    "# Modeling NHL Game Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff69baa0",
   "metadata": {},
   "source": [
    "Building binary classification models to predict the winning team"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d60c264",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e0da94",
   "metadata": {},
   "source": [
    "Topline goal: predict the winner of a given NHL game\n",
    "\n",
    "The first purpose of this model is to see if the winner of an NHL game can be predicted with more accuracy than with a naive prediction - choosing the home team to win every time with no other informative input.\n",
    "\n",
    "In this notebook several modeling techniques will be trained/tested using team game log data from the prior three full NHL regular seasons (19'-20', 20'-21', 21'-22). Games played to date from the current season (22'-23') will then be used to evaluate the model's ability to correctly predict the winning team.\n",
    "\n",
    "If I can develop a game prediction model that performs above that naive baseline, the next step will be to adjust the benchmark to a semi naive prediction. The semi naive baseline will be based on choosing the 'Vegas' favorite to win everytime. The recent wave of mobile sports betting legalization has led to massive increase in sports wagers placed both in the US and globally. With the increased popularity of sports betting, sportsbooks odds are an increasingly efficient market i.e. the implied probabilties of sportsbook odds are an unbiased estimator of outcomes.\n",
    "\n",
    "If I can develop a model which is competitive with the betting market, the next step will be to test betting strategies which leverage the predition model to yield postive ROI. Given the 'vig' (i.e. commission) charged by sportsbooks to take wagers, simply beating the market will not be enough to produce a profitable strategy. Instead the model will have to outperform the market by ~5+% to approach profitability.\n",
    "\n",
    "The excess return required to be profitable in sports betting and generally high log loss scores in betting models, necessitates the application of betting strategies to the model's prediction. Rather than betting equal amounts on each event, I will test strategies that aim to identify and capitilize on over/undervalued sportsbooks odds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be8c3d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Packages\n",
    "import pandas as pd\n",
    "# from pandas.testing import assert_frame_equal\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Scraping packages\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import hockey_scraper\n",
    "\n",
    "# Viz Packages\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# Modeling Packages\n",
    "## Modeling Prep\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, KFold, \\\n",
    "GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "## SKLearn Data Prep Modules\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, \\\n",
    "PolynomialFeatures, PowerTransformer, Normalizer, MaxAbsScaler\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "## SKLearn Classification Models\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier,\\\n",
    "ExtraTreesClassifier, VotingClassifier, StackingRegressor, GradientBoostingClassifier\n",
    "\n",
    "## SKLearn Pipeline Setup\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "## SKLearn Model Optimization\n",
    "from sklearn.feature_selection import RFE, f_regression\n",
    "\n",
    "## Boosting\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "## SKLearn Metrics\n",
    "### Classification Scoring/Evaluation\n",
    "from sklearn.metrics import classification_report, accuracy_score, recall_score, precision_score, f1_score, \\\n",
    "ConfusionMatrixDisplay, log_loss, confusion_matrix, RocCurveDisplay, make_scorer, roc_auc_score\n",
    "\n",
    "# ## TensorFlow\n",
    "# #for the Neural Network\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout\n",
    "# from tensorflow.keras.regularizers import l2\n",
    "# from tensorflow.keras.optimizers import SGD\n",
    "# from tensorflow.keras.wrappers import scikit_learn\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from keras.constraints import maxnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f4b1ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook Config\n",
    "from pprintpp import pprint as pp\n",
    "%reload_ext pprintpp\n",
    "from tqdm import tqdm\n",
    "from io import StringIO\n",
    "\n",
    "## Suppress Python Warnings (Future, Deprecation)\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category= FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "## Suppress Pandas Warnings (SettingWithCopy)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "## Pandas Display Config\n",
    "pd.options.display.max_columns = 80\n",
    "# pd.options.display.width = None\n",
    "\n",
    "## Display SKLearn estimators as diagrams\n",
    "from sklearn import set_config\n",
    "set_config(display= 'diagram')\n",
    "\n",
    "## Progress bars\n",
    "from tqdm.notebook import tqdm\n",
    "pbar = tqdm(..., display=False) # disable by default\n",
    "# display(pbar.container) ## display progress bar in desired cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6edfb05",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3655f2a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2055c557",
   "metadata": {},
   "source": [
    "### Data Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c903029b",
   "metadata": {},
   "source": [
    "Data sources/notes:\n",
    "- Game log team statistics are retrieved are retrieved from Natural Stat Trick (NST)\n",
    "- Game log results data is retrieved from the NHL's API\n",
    "- Only games from the regular season are considered\n",
    "    - Ensures all teams are sampled equally\n",
    "    - Avoids potential issues stemming from structural differences in regular and postseason games\n",
    "- Data is available/retrieved from the perspective of both the home and away teams\n",
    "- Metrics describing offensive, defensive and goaltending performance are used to model outcomes\n",
    "- For each team data for the 3 common game strength states is used\n",
    "    - 5v5 adjusted\n",
    "    - 5v4 powerplay (PP)\n",
    "    - 4v5 penalty kill (PK)\n",
    "- Team stats are in 'rates' form rather than standard total counts\n",
    "\n",
    "Rates definitions per NST glossary:\n",
    "- 5v5 Score & Venue Adjusted \n",
    "    - Play where both teams have five skaters and a goalie on the ice, with the event counts adjusted for home ice advantage and leading or trailing score effects. This is done using the method created by Micah Blake McCurdy.\n",
    "- 5 on 4 PP \n",
    "    - Play where the team has five skaters and a goalie on the ice versus four skaters and a goalie their opponent due to penalties.\n",
    "- 4 on 5 PK \n",
    "    - Play where the team has four skaters and a goalie on the ice versus five skaters and a goalie their opponent due to penalties.\n",
    "- Rates\n",
    "    - TOI is presented as TOI/GP. For and against statistics are presented as the counts per 60 minutes of play.\n",
    "  \n",
    "Processing notes:\n",
    "- Static data from prior season is used for initial development\n",
    "- Once the model is built, I will aim to transition to a dynamic/automated ETL process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cef073",
   "metadata": {},
   "source": [
    "### Data Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f494478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# team abbreviation dict from hockey scraper package function\n",
    "# \"_descrition\": \"# All the corresponding tri-codes for team names\",\n",
    "\n",
    "team_dict = {\n",
    "    'Anaheim Ducks': 'ANA',\n",
    "    'Arizona Coyotes' : 'ARI',\n",
    "    'Boston Bruins': 'BOS', \n",
    "    'Buffalo Sabres':'BUF',\n",
    "    'Calgary Flames': 'CGY', \n",
    "    'Carolina Hurricanes': 'CAR', \n",
    "    'Chicago Blackhawks': 'CHI', \n",
    "    'Colorado Avalanche': 'COL',\n",
    "    'Columbus Blue Jackets': 'CBJ',\n",
    "    'Dallas Stars': 'DAL',\n",
    "    'Detroit Red Wings': 'DET',\n",
    "    'Edmonton Oilers': 'EDM',\n",
    "    'Florida Panthers': 'FLA',\n",
    "    'Los Angeles Kings': 'L.A',\n",
    "    'Minnesota Wild': 'MIN',\n",
    "    'Montreal Canadiens': 'MTL',\n",
    "    'Nashville Predators': 'NSH',\n",
    "    'New Jersey Devils': 'N.J',\n",
    "    \"New York Islanders\": 'NYI',\n",
    "    \"New York Rangers\": 'NYR',\n",
    "    'Ottawa Senators': 'OTT',\n",
    "    'Philadelphia Flyers': 'PHI',\n",
    "    'Pittsburgh Penguins': 'PIT',\n",
    "    'San Jose Sharks': 'S.J',\n",
    "    'Seattle Kraken': 'SEA',\n",
    "#     'St. Louis Blues': 'STL',\n",
    "    'St Louis Blues': 'STL',\n",
    "    'Tampa Bay Lightning': 'T.B',\n",
    "    'Toronto Maple Leafs': 'TOR',\n",
    "    'Vancouver Canucks': 'VAN',\n",
    "    'Vegas Golden Knights':'VGK',\n",
    "    'Washington Capitals': 'WSH',\n",
    "    'Winnipeg Jets': 'WPG'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab347245",
   "metadata": {},
   "source": [
    "### Game States"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb62d758",
   "metadata": {},
   "source": [
    "Game strength state definitions per NST glossary:\n",
    "- Even Strength (5v5) Score & Venue Adjusted \n",
    "    - Play where both teams have five skaters and a goalie on the ice, with the event counts adjusted for home ice advantage and leading or trailing score effects. This is done using the method created by Micah Blake McCurdy.\n",
    "- 5 on 4 PP \n",
    "    - Play where the team has five skaters and a goalie on the ice versus four skaters and a goalie their opponent due to penalties.\n",
    "- 4 on 5 PK \n",
    "    - Play where the team has four skaters and a goalie on the ice versus five skaters and a goalie their opponent due to penalties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da1c4a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# game state vars\n",
    "ev = 'sva'\n",
    "pp = 'pp'\n",
    "pk = 'pk'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68f775c",
   "metadata": {},
   "source": [
    "### Team Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec5900a",
   "metadata": {},
   "source": [
    "Feature definitions per NST glossary:\n",
    "- TOI:  \n",
    "    - Total amount of time played\n",
    "- GF/60:\n",
    "    - Rate of Goals for that team per 60 minutes of play. GF*60/TOI\n",
    "- xGF/60:\n",
    "    - Expected version of above\n",
    "- GA/60:\n",
    "    - Rate of Goals against that team per 60 minutes of play. GA*60/TOI\n",
    "- xGF/60:\n",
    "    - Expected version of above\n",
    "- FF%: \n",
    "    - Percentage of total Fenwick in games that team played that are for that team. FF*100/(FF+FA)\n",
    "- GF%:\n",
    "    - Percentage of total Goals in games that team played that are for that team. GF*100/(GF+GA)Percentage of total Goals in games that team played that are for that team. GF*100/(GF+GA)\n",
    "- xGF%:\n",
    "    - Expected version of above. A metric designed to measure the probability of a shot resulting in a goal. Modeled on shots league wide.\n",
    "- HDCF%:\n",
    "    - Percentage of total High Danger Scoring Chances in games that team played that are for that team.\n",
    "- HDSH%:\n",
    "    - Percentage of High Danger Shots for that team that were Goals. HDGF*100/HDSF\n",
    "- HDSV%:\n",
    "    - Percentage of High Danger Shots against that team that were not Goals. 100-(HDGA*100/HDSA)\n",
    "- SH%:\n",
    "    - Percentage of Shots for that team that were Goals. GF*100/SF\n",
    "- SV%: \n",
    "    - Percentage of Shots against that team that were not Goals. 100-(GA*100/SA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69c32227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even strength 5v5 strength state field list\n",
    "ev_features_list = [\n",
    "    'Team',\n",
    "    'FF%',\n",
    "    'GF%',\n",
    "    'xGF%',\n",
    "    'HDCF%',\n",
    "    'HDSH%',\n",
    "    'HDSV%',\n",
    "    'SH%',\n",
    "    'SV%',\n",
    "    'Date', # extracted from 'Game' and used to make 'Game_Key'\n",
    "    'Game_Key',] # descriptor to be dropped after processing\n",
    "#     'Game', # descriptor to be dropped after processing\n",
    "#     'Team', # descriptor to be dropped after processing\n",
    "#    'TOI', ## don't think i need this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c03fff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PP df features list\n",
    "pp_features_list = [\n",
    "    'GF/60', \n",
    "    'xGF/60', \n",
    "    'Game_Key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faa3e37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PK df features list\n",
    "# same as pp df but swap GFs for GAs\n",
    "pk_features_list = [\n",
    "    'GA/60', \n",
    "    'xGA/60', \n",
    "    'Game_Key']\n",
    "#     'TOI', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c00a963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dynamic data loading via NST API\n",
    "# def get_season_stats_df(season_start_id, season_end_id, strength_state):\n",
    "#     # season_id is the concatenated season start and end years ex 20222023\n",
    "#     # strength states defined above - ev, pp, pk\n",
    "#     url = 'https://www.naturalstattrick.com/games.php?\\\n",
    "#     fromseason={}&thruseason={}&stype=2&sit={}&loc=B&team=All&rate=y'\n",
    "#     .format(season_start_id, season_end_id, strenth_state)\n",
    "\n",
    "#     response = requests.get(url)\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#     table = soup.find_all(\"table\")[0]\n",
    "#     df = pd.read_html(str(table))[0]\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e334ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get season stats for 18-19', 19-20', 20-21, 21-22', 22-23'\n",
    "# evget_season_stats_df(20182019)\n",
    "# # 5v5 stats\n",
    "# ev_1819 = get_season_stats_df(20182019, )\n",
    "# ev_1920 = pd.DataFrame(scrape_schedule('2019-10-02', '2020-03-11', preseason=False, not_over=False))\n",
    "# ev_2021 = pd.DataFrame(scrape_schedule('2021-01-13', '2021-05-19', preseason=False, not_over=False))\n",
    "# ev_2122 = pd.DataFrame(scrape_schedule('2021-10-12', '2022-05-01', preseason=False, not_over=False))\n",
    "# ev_2223 = pd.DataFrame(scrape_schedule('2022-10-07', '2023-04-14', preseason=False, not_over=False))\n",
    "\n",
    "# # pp stats\n",
    "\n",
    "\n",
    "\n",
    "# # pk stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc10dcb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stats1920_2223get_season_stats_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstats1920_2223get_season_stats_df\u001b[49m(\u001b[38;5;241m20192020\u001b[39m, \u001b[38;5;241m20222023\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stats1920_2223get_season_stats_df' is not defined"
     ]
    }
   ],
   "source": [
    "# stats1920_2223get_season_stats_df(20192020, 20222023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be72eed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d43e2983",
   "metadata": {},
   "source": [
    "### Data Cleaning/Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98446dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st round cleaning of team stat sets\n",
    "def process_stats(game_log_df):\n",
    "    # extract date from game column\n",
    "    game_log_df['Date'] = pd.to_datetime(game_log_df['Game'].str[:11])\n",
    "    # sub full team names with abbreviated names\n",
    "    game_log_df.replace(team_dict, inplace=True)\n",
    "    # create game log index\n",
    "    game_log_df['Game_Key'] = game_log_df['Team'].astype(str)+'_'+game_log_df['Date'].astype(str)\n",
    "    game_log_df = game_log_df.replace('-', 0)\n",
    "    return game_log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79ef0435",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_df = pd.read_csv('data/19_23-5v5.csv')\n",
    "pp_df = pd.read_csv('data/19_23-5v4.csv')\n",
    "pk_df = pd.read_csv('data/19_23-4v5.csv')\n",
    "\n",
    "print(ev_df.shape, pp_df.shape, pk_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f99e5642",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a1e7f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process stats\n",
    "process_stats(ev_df)\n",
    "process_stats(pp_df)\n",
    "process_stats(pk_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b18a3f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_df = ev_df.replace('-', 0)\n",
    "pp_df = pp_df.replace('-', 0)\n",
    "pk_df = pk_df.replace('-', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71d583e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "581efa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_df = ev_df[ev_features_list]\n",
    "pp_df = pp_df[pp_features_list]\n",
    "pk_df = pk_df[pk_features_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38283597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge team stats (5v5_adj, 5v4_pp, 4v5_pk game states) for each period with the feature column list defined above\n",
    "def merge_strength_states(ev_df, pp_df, pk_df):\n",
    "    # add suffix to all columns in ev_df, pp_df, and pk_df\n",
    "    pp_df = pp_df.rename(columns={col: col + '_pp' if col != 'Game_Key' else col for col in pp_df.columns})\n",
    "    pk_df = pk_df.rename(columns={col: col + '_pk' if col != 'Game_Key' else col for col in pk_df.columns})\n",
    "    \n",
    "    \n",
    "    # left merge the 5v5 and 5v4 dfs on 'Game_key', w/ respective features, add suffixes for shared cols \n",
    "    even_pp_merged = pd.merge(ev_df, pp_df,\n",
    "                              on = 'Game_Key', how = 'left', suffixes=('', '_pp'))\n",
    "\n",
    "    # left merge that df with the 4v5_pk on 'Game_key', selected columns, and suffixes for overlapping columns\n",
    "    all_states_merged = pd.merge(even_pp_merged, pk_df, \n",
    "                                  on = 'Game_Key', how = 'left', suffixes = ('', '_pk'))\n",
    "\n",
    "    return all_states_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1008ee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_float(df):\n",
    "    return df.apply(lambda x: pd.to_numeric(x, errors='ignore') if x.name not in ['Game', 'Team', 'Unnamed: 2','Game_Key', 'Date'] else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5a1cb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats_df = merge_strength_states(ev_df, pp_df, pk_df)\n",
    "all_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "daa1bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79ffdb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats_df = convert_to_float(all_stats_df)\n",
    "print(all_stats_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b62b56c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_features(df, rolling_games=10):\n",
    "#     df['Date'] = df['Date']\n",
    "#     df['season_id'] = df['season_id']\n",
    "#     df['_Team_Won'] = df['_Team_Won']\n",
    "#     df['rolling_avg_pts_pct'] = df.groupby([df['Date'].dt.year, 'Team'])['_pts_pct'].rolling(window=10, min_periods=10).mean().reset_index(0, drop=True)\n",
    "#     df['rolling_avg_away_pts_pct'] = df.groupby([df['Date'].dt.year, 'Team'])['away_pts_pct'].rolling(window=10, min_periods=10).mean().reset_index(0, drop=True)\n",
    "    df['rolling_avg_FF%'] = df.groupby([df['Date'].dt.year, 'Team'])['FF%'].transform(lambda x: x.rolling(rolling_games, rolling_games).mean().shift())\n",
    "    df['rolling_avg_GF%'] = df.groupby([df['Date'].dt.year, 'Team'])['GF%'].transform(lambda x: x.rolling(rolling_games, rolling_games).mean().shift())\n",
    "    df['rolling_avg_xGF%'] = df.groupby([df['Date'].dt.year, 'Team'])['xGF%'].transform(lambda x: x.rolling(rolling_games, rolling_games).mean().shift())\n",
    "    df['rolling_avg_HDCF%'] = df.groupby([df['Date'].dt.year, 'Team'])['HDCF%'].transform(lambda x: x.rolling(rolling_games, rolling_games).mean().shift())\n",
    "    df['rolling_avg_HDSH%'] = df.groupby([df['Date'].dt.year, 'Team'])['HDSH%'].transform(lambda x: x.rolling(rolling_games, rolling_games).mean().shift())\n",
    "    df['rolling_avg_HDSV%'] = df.groupby([df['Date'].dt.year, 'Team'])['HDSV%'].transform(lambda x: x.rolling(rolling_games, rolling_games).mean().shift())\n",
    "    df['rolling_avg_SH%'] = df.groupby([df['Date'].dt.year, 'Team'])['SH%'].transform(lambda x: x.rolling(rolling_games, rolling_games).sum().mean())\n",
    "    df['rolling_avg_SV%'] = df.groupby([df['Date'].dt.year, 'Team'])['SV%'].transform(lambda x: x.rolling(rolling_games, rolling_games).sum().mean())\n",
    "    df['rolling_avg_GF/60_pp'] = df.groupby([df['Date'].dt.year, 'Team'])['GF/60_pp'].transform(lambda x: x.rolling(rolling_games, rolling_games).mean().shift())\n",
    "    df['rolling_avg_xGF/60_pp'] = df.groupby([df['Date'].dt.year, 'Team'])['xGF/60_pp'].transform(lambda x: x.rolling(rolling_games, rolling_games).mean().shift())\n",
    "    df['rolling_avg_GA/60_pk'] = df.groupby([df['Date'].dt.year, 'Team'])['GA/60_pk'].transform(lambda x: x.rolling(rolling_games, rolling_games).mean().shift())\n",
    "    df['rolling_avg_xGA/60_pk'] = df.groupby([df['Date'].dt.year, 'Team'])['xGA/60_pk'].transform(lambda x: x.rolling(rolling_games, rolling_games).mean().shift())\n",
    "#     df['rolling_avg_away_FF%'] = df.groupby([df['Date'].dt.year, 'Team'])['away_FF%'].rolling(window=10, min_periods=10).mean().reset_index(0, drop=True)\n",
    "#     df['rolling_avg_away_GF%'] = df.groupby([df['Date'].dt.year, 'Team'])['away_GF%'].rolling(window=10, min_periods=10).mean().reset_index(0, drop=True)\n",
    "#     df['rolling_avg_away_xGF%'] = df.groupby([df['Date'].dt.year, 'Team'])['away_xGF%'].rolling(window=10, min_periods=10).mean().reset_index(0, drop=True)\n",
    "#     df['rolling_avg_away_HDCF%'] = df.groupby([df['Date'].dt.year, 'Team'])['away_HDCF%'].rolling(window=10, min_periods=10).mean().reset_index(0, drop=True)\n",
    "#     df['rolling_avg_away_HDSH%'] = df.groupby([df['Date'].dt.year, 'Team'])['away_HDSH%'].rolling(window=10, min_periods=10).mean().reset_index(0, drop=True)\n",
    "#     df['rolling_avg_away_HDSV%'] = df.groupby([df['Date'].dt.year, 'Team'])['away_HDSV%'].rolling(window=10, min_periods=10).mean().reset_index(0, drop=True)\n",
    "#     df['rolling_avg_away_SH%'] = df.groupby([df['Date'].dt.year, 'Team'])['away_SH%'].rolling(window=10, min_periods=10).mean().reset_index(0, drop=True)\n",
    "#     df['rolling_avg_away_SV%'] = df.groupby([df['Date'].dt.year, 'Team'])['away_SV%'].rolling(window=10, min_periods=10).mean().reset_index(0, drop=True)\n",
    "#     df['rolling_avg_away_GF/60_pp'] = df.groupby([df['Date'].dt.year, 'Team'])['away_GF/60_pp'].rolling(window=10, min_periods=10).mean().reset_index(0, drop=True)\n",
    "#     df['rolling_avg_away_xGF/60_pp'] = df.groupby([df['Date'].dt.year, 'Team'])['away_xGF/60_pp'].rolling(window=10, min_periods=10).mean().reset_index(0, drop=True)\n",
    "#     df['rolling_avg_away_GA/60_pk'] = df.groupby([df['Date'].dt.year, 'Team'])['away_GA/60_pk'].rolling(window=10, min_periods=10).mean().reset_index(0, drop=True)\n",
    "#     df['rolling_avg_away_xGA/60_pk'] = df.groupby([df['Date'].dt.year, 'Team'])['away_xGA/60_pk'].rolling(window=10, min_periods=10).mean().reset_index(0, drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d756353",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats_df = rolling_features(all_stats_df)\n",
    "all_stats_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f41cc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_stats_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "adc5a1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats_features = ['Team','Date', 'Game_Key','rolling_avg_FF%', 'rolling_avg_GF%', 'rolling_avg_xGF%',\n",
    "       'rolling_avg_HDCF%', 'rolling_avg_HDSH%', 'rolling_avg_HDSV%',\n",
    "       'rolling_avg_SH%', 'rolling_avg_SV%', 'rolling_avg_GF/60_pp',\n",
    "       'rolling_avg_xGF/60_pp', 'rolling_avg_GA/60_pk',\n",
    "       'rolling_avg_xGA/60_pk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d40d4002",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78cd903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats_df = all_stats_df[all_stats_features]\n",
    "all_stats_df = all_stats_df.dropna()\n",
    "all_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aaccbe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def column_dropper(ev_df, ev_features_list, pp_df, pp_features_list, pk_df, pk_features_list):\n",
    "#     ev_df = ev_df[ev_features_list]\n",
    "#     pp_df = pp_df[pp_features_list]\n",
    "#     pk_df = pk_df[pk_features_list]\n",
    "#     ev_df = ev_df.replace('-', 0)\n",
    "#     pp_df = pp_df.replace('-', 0)\n",
    "#     pk_df = pk_df.replace('-', 0)\n",
    "#     return ev_df, pp_df, pk_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c903646",
   "metadata": {},
   "source": [
    "### Add NHL.com schedule/result df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "defcb433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions from the Hockey Scraper API\n",
    "# modified to retrieve additional info \n",
    "\"\"\"\n",
    "This module contains functions to scrape the json schedule for any games or date range\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import time\n",
    "import hockey_scraper.utils.shared as shared\n",
    "\n",
    "\n",
    "# TODO: Currently rescraping page each time since the status of some games may have changed\n",
    "# (e.g. Scraped on 2020-01-20 and game on 2020-01-21 was not Final...when use old page again will still think not Final)\n",
    "# Need to find a more elegant way of doing this (Metadata???)\n",
    "def get_schedule(date_from, date_to):\n",
    "    \"\"\"\n",
    "    Scrapes games in date range\n",
    "    Ex: https://statsapi.web.nhl.com/api/v1/schedule?startDate=2010-10-03&endDate=2011-06-20\n",
    "    \n",
    "    :param date_from: scrape from this date\n",
    "    :param date_to: scrape until this date\n",
    "    \n",
    "    :return: raw json of schedule of date range\n",
    "    \"\"\"\n",
    "    page_info = {\n",
    "        \"url\": 'https://statsapi.web.nhl.com/api/v1/schedule?startDate={a}&endDate={b}'.format(a=date_from, b=date_to),\n",
    "        \"name\": date_from + \"_\" + date_to,\n",
    "        \"type\": \"json_schedule\",\n",
    "        \"season\": shared.get_season(date_from),\n",
    "    }\n",
    "\n",
    "    return json.loads(shared.get_file(page_info, force=True))\n",
    "\n",
    "\n",
    "def chunk_schedule_calls(from_date, to_date):\n",
    "    \"\"\"\n",
    "    The schedule endpoint sucks when handling a big date range. So instead I call in increments of n days.\n",
    "    \n",
    "    :param date_from: scrape from this date\n",
    "    :param date_to: scrape until this date\n",
    "\n",
    "    :return: raw json of schedule of date range\n",
    "    \"\"\"\n",
    "    sched = []\n",
    "    days_per_call = 30\n",
    "\n",
    "    from_date = datetime.strptime(from_date, \"%Y-%m-%d\") \n",
    "    to_date = datetime.strptime(to_date, \"%Y-%m-%d\")\n",
    "    num_days = (to_date - from_date).days + 1  # +1 since difference is looking for total number of days\n",
    "\n",
    "    for offset in range(0, num_days, days_per_call):\n",
    "        f_chunk = datetime.strftime(from_date + timedelta(days=offset), \"%Y-%m-%d\")\n",
    "\n",
    "        # We need the min bec. if the chunks are evenly sized this prevents us from overshooting the max\n",
    "        t_chunk = datetime.strftime(from_date + timedelta(days=min(num_days-1, offset+days_per_call-1)), \"%Y-%m-%d\")\n",
    "\n",
    "        chunk_sched = get_schedule(f_chunk, t_chunk)\n",
    "        sched.append(chunk_sched['dates'])\n",
    "\n",
    "    return sched\n",
    "\n",
    "\n",
    "def get_dates(games):\n",
    "    \"\"\"\n",
    "    Given a list game_ids it returns the dates for each game.\n",
    "\n",
    "    We sort all the games and retrieve the schedule from the beginning of the season from the earliest game\n",
    "    until the end of most recent season.\n",
    "    \n",
    "    :param games: list with game_id's ex: 2016020001\n",
    "    \n",
    "    :return: list with game_id and corresponding date for all games\n",
    "    \"\"\"\n",
    "    today = datetime.today()\n",
    "\n",
    "    # Determine oldest and newest game\n",
    "    games = list(map(str, games))\n",
    "    games.sort()\n",
    "\n",
    "    date_from = shared.season_start_bound(games[0][:4])\n",
    "    year_to = int(games[-1][:4])\n",
    "\n",
    "    # If the last game is part of the ongoing season then only request the schedule until Today\n",
    "    # We get strange errors if we don't do it like this\n",
    "    if year_to == shared.get_season(datetime.strftime(today, \"%Y-%m-%d\")):\n",
    "        date_to = '-'.join([str(today.year), str(today.month), str(today.day)])\n",
    "    else:\n",
    "        date_to = datetime.strftime(shared.season_end_bound(year_to+1), \"%Y-%m-%d\")  # Newest game in sample\n",
    "\n",
    "    # TODO: Assume true is live here -> Workaround\n",
    "    schedule = scrape_schedule(date_from, date_to, preseason=True, not_over=True)\n",
    "\n",
    "    # Only return games we want in range\n",
    "    games_list = []\n",
    "    for game in schedule:\n",
    "        if str(game['game_id']) in games:\n",
    "            games_list.extend([game])\n",
    "    return games_list\n",
    "\n",
    "\n",
    "def scrape_schedule(date_from, date_to, preseason=False, not_over=False):\n",
    "    \"\"\"\n",
    "    Calls getSchedule and scrapes the raw schedule Json\n",
    "    \n",
    "    :param date_from: scrape from this date\n",
    "    :param date_to: scrape until this date\n",
    "    :param preseason: Boolean indicating whether include preseason games (default if False)\n",
    "    :param not_over: Boolean indicating whether we scrape games not finished. \n",
    "                     Means we relax the requirement of checking if the game is over. \n",
    "    \n",
    "    :return: list with all the game id's\n",
    "    \"\"\"\n",
    "    schedule = []\n",
    "    schedule_json = chunk_schedule_calls(date_from, date_to)\n",
    "\n",
    "    for chunk in schedule_json:\n",
    "        for day in chunk:\n",
    "            for game in day['games']:\n",
    "                if game['status']['detailedState'] == 'Final' or not_over:\n",
    "                    game_id = int(str(game['gamePk'])[5:])\n",
    "                    # add game type logic to filter out none regular season games\n",
    "                    if (game_id >= 20000 or preseason) and game_id < 40000:\n",
    "                        schedule.append({\n",
    "                                 \"game_id\": game['gamePk'],\n",
    "                                \"game_type\": game['gameType'],\n",
    "                                 \"season_id\": game['season'],\n",
    "                                 \"date\": day['date'], \n",
    "                                 \"home_score\": game['teams']['home'].get(\"score\"),\n",
    "                                 \"away_score\": game['teams']['away'].get(\"score\"),\n",
    "                                #  \"start_time\": datetime.strptime(game['gameDate'][:-1], \"%Y-%m-%dT%H:%M:%S\"),\n",
    "                                #  \"venue\": game['venue'].get('name'),\n",
    "                                #  \"home_team_id\": game['teams']['home']['team']['id'],\n",
    "                                 \"home_team\": shared.get_team(game['teams']['home']['team']['name']),\n",
    "                                 \"home_wins\": game['teams']['home'].get(\"leagueRecord\").get(\"wins\"),\n",
    "                                 \"home_losses\": game['teams']['home'].get(\"leagueRecord\").get(\"losses\"),\n",
    "                                 \"home_otl\": game['teams']['home'].get(\"leagueRecord\").get(\"ot\"),\n",
    "                                 \"away_wins\": game['teams']['away'].get(\"leagueRecord\").get(\"wins\"),\n",
    "                                 \"away_losses\": game['teams']['away'].get(\"leagueRecord\").get(\"losses\"),\n",
    "                                 \"away_otl\": game['teams']['away'].get(\"leagueRecord\").get(\"ot\"), \n",
    "                                #  \"away_team_id\": game['teams']['away']['team']['id'],\n",
    "                                 \"away_team\": shared.get_team(game['teams']['away']['team']['name']),\n",
    "                                #  \"home_score\": game['teams']['home'].get(\"score\"),\n",
    "                                #  \"away_score\": game['teams']['away'].get(\"score\"),\n",
    "                                 \"status\": game[\"status\"][\"abstractGameState\"]\n",
    "                        })\n",
    "\n",
    "\n",
    "    return schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b482838",
   "metadata": {},
   "source": [
    "NHL regular season date boundaries by season\n",
    "- 2018-2019: October 3, 2018 – June 12, 2019\n",
    "- 2019-2020: October 2, 2019 – March 11, 2020\n",
    "  - Covid caused scheduling issues, so need to make sure only regular season games are factored\n",
    "- 2020-2021: January 13, 2021 - May 19, 2021\n",
    "  - same potential issue noted above\n",
    "- 2021-2022: October 12, 2021 - May 1, 2022\n",
    "- 2022-2023: October 7, 2022 - April 14, 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e10a80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run modified scrape_schedule for 19-20, 20-21, and 21-22 seasons individually \n",
    "# will make for easier processing of imputed features\n",
    "schedule_1920 = pd.DataFrame(scrape_schedule('2019-10-02', '2020-03-11', preseason=False, not_over=False))\n",
    "schedule_2021 = pd.DataFrame(scrape_schedule('2021-01-13', '2021-05-19', preseason=False, not_over=False))\n",
    "schedule_2122 = pd.DataFrame(scrape_schedule('2021-10-12', '2022-05-01', preseason=False, not_over=False))\n",
    "schedule_2223 = pd.DataFrame(scrape_schedule('2022-10-07', '2023-04-14', preseason=False, not_over=False))\n",
    "schedule_1920"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0682bde4",
   "metadata": {},
   "source": [
    "Bulk processing to conduct upon load:\n",
    "\n",
    "- Convert the following fields accordingly\n",
    "  - game_id, season_id to string\n",
    "  - home_otl, away_otl to int\n",
    "- Filter out non regular season games\n",
    "  - game_type = R\n",
    "- Add column to denote winner from perspective of home team\n",
    "  - home_team = 1 denotes home team victory, 0 for loss\n",
    "- Add game keys so schedule dfs can be joined with stats dfs\n",
    "- Add running total column for number of games played so far that season for all teams\n",
    "- Add running standings points total column for all teams\n",
    "    - Win = 2 points\n",
    "    - (Regulation) Loss = 0 points\n",
    "    - Overtime loss = 1 point\n",
    "- Add column for points percentage\n",
    "    - pts_pct = actual points accumulated / potential max points \n",
    "    - potential max points = games played * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9056fd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes sure all games are 'R' type for regular season\n",
    "schedule_1920['game_type'].value_counts()\n",
    "# assert schedule_2021.loc[schedule_2021['game_type'] == 'R']\n",
    "# assert schedule_2122.loc[schedule_2122['game_type'] == 'R']\n",
    "# assert schedule_2223.loc[schedule_2223['game_type'] == 'R']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6039a707",
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule_1920.loc[schedule_1920['game_type'] == 'WA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ddce1891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_df_processing(schedule_df):\n",
    "    # filter out non regular season games - should have 3262 for 19-20 through 21-22 seasons\n",
    "    schedule_df = schedule_df.loc[schedule_df['game_type'] == 'R']\n",
    "    # convert game_id and season_id to str. make sure they aren't used numerically and in case needed for filtering\n",
    "    schedule_df['game_id'] = schedule_df['game_id'].astype(str) # don't need\n",
    "    schedule_df['season_id'] = schedule_df['season_id'].astype(str)\n",
    "    # convert overtime loss columns to int from float as can't be decimal\n",
    "    schedule_df['home_otl'] = schedule_df['home_otl'].astype(int)\n",
    "    schedule_df['away_otl'] = schedule_df['away_otl'].astype(int)\n",
    "    # add column for denoting game winner - denote home team victory in binary, win = 1\n",
    "    schedule_df['Home_Team_Won'] = np.where(schedule_df['home_score'] > schedule_df['away_score'], 1, 0)\n",
    "    # add keys for merging stats dfs\n",
    "    schedule_df['Home_Team_Key'] = schedule_df['home_team'].astype(str)+'_'+schedule_df['date'].astype(str)\n",
    "    schedule_df['Away_Team_Key'] = schedule_df['away_team'].astype(str)+'_'+schedule_df['date'].astype(str)\n",
    "    # add column containing running total games played so far that season\n",
    "    schedule_df['home_gp'] = (schedule_df['home_wins'] + schedule_df['home_losses'] + schedule_df['home_otl']).astype(int)\n",
    "    schedule_df['away_gp'] = (schedule_df['away_wins'] + schedule_df['away_losses'] + schedule_df['away_otl']).astype(int)                              \n",
    "    # add column containing running standing points total for teams. wins = 2, losses = 0, otl = 1\n",
    "    schedule_df['home_points'] = ((schedule_df['home_wins'] * 2) + schedule_df['home_otl']).astype(int)\n",
    "    schedule_df['away_points'] = ((schedule_df['away_wins'] * 2) + schedule_df['away_otl']).astype(int)\n",
    "    # add column for points percentage column\n",
    "    schedule_df['home_pts_pct'] = schedule_df['home_points'] / (schedule_df['home_gp'] * 2)\n",
    "    schedule_df['away_pts_pct'] = schedule_df['away_points'] / (schedule_df['away_gp'] * 2)\n",
    "\n",
    "    return schedule_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5748b090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call processing function on all schedule dfs\n",
    "schedule_1920 = schedule_df_processing(schedule_1920)\n",
    "schedule_2021 = schedule_df_processing(schedule_2021)\n",
    "schedule_2122 = schedule_df_processing(schedule_2122)\n",
    "schedule_2223 = schedule_df_processing(schedule_2223)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38861a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check lengths to make sure the correct number of games are returned\n",
    "# 2542, 1082, 868, 1312, 1312\n",
    "# print(len(schedule_1819))\n",
    "print(len(schedule_1920))\n",
    "print(len(schedule_2021))\n",
    "print(len(schedule_2122))\n",
    "print(len(schedule_2223))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "884362ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule_1920_2123 = pd.concat([schedule_1920, schedule_2021, schedule_2122, schedule_2223], axis=0, ignore_index=True)\n",
    "schedule_1920_2123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82c9d1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(schedule_1920_2123.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76419a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "sched_cols = ['season_id', 'Home_Team_Won', 'Home_Team_Key', 'Away_Team_Key','home_pts_pct', 'away_pts_pct']\n",
    "schedule_1920_2123 = schedule_1920_2123[sched_cols]\n",
    "schedule_1920_2123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2dcd5769",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_df = schedule_1920_2123.merge(all_stats_df.add_prefix('home_'), \n",
    "                                       left_on = 'Home_Team_Key', right_on = 'home_Game_Key', how = 'left')\n",
    "modeling_df = modeling_df.merge(all_stats_df.add_prefix('away_'), \n",
    "                                       left_on = 'Away_Team_Key', right_on = 'away_Game_Key', how = 'left')\n",
    "modeling_df['Date'] = modeling_df['home_Date']\n",
    "modeling_df = modeling_df.drop(columns=['Home_Team_Key', 'Away_Team_Key', 'home_Date', \n",
    "                                        'home_Game_Key', 'away_Date', 'away_Game_Key'])\n",
    "modeling_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "384eeace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rolling_wins(df, rolling_games=10):\n",
    "#     df['rolling_avg_home_pts_pct'] = df.groupby([df['Date'].dt.year, 'home_Team'])['home_pts_pct'].transform(lambda x: x.rolling(rolling_games).mean().shift())\n",
    "#     df['rolling_avg_away_pts_pct'] = df.groupby([df['Date'].dt.year, 'away_Team'])['away_pts_pct'].transform(lambda x: x.rolling(rolling_games).mean().shift())\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "884605c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling_df = rolling_wins(modeling_df)\n",
    "# modeling_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbed26d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd9c3679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling_df['Date'] = modeling_df['home_Date']\n",
    "# modeling_df = modeling_df.drop(columns=['Home_Team_Key', 'Away_Team_Key', 'home_Date', \n",
    "#                                         'home_Game_Key', 'away_Date', 'away_Game_Key'])\n",
    "# modeling_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "95683915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(modeling_df.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7dcdc37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling_df = schedule_1920_2122.merge(home_away_stats[cols_to_prefix].add_prefix('home_'), \n",
    "#                                        left_on = 'Home_Team_Key', right_on = 'home_Game_Key', how = 'left').drop(columns = 'home_Team_Key')\n",
    "\n",
    "# modeling_df = modeling_df.merge(home_away_stats[cols_to_prefix].add_prefix('away_'), \n",
    "#                                        left_on = 'Away_Team_Key', right_on = 'away_Game_Key', how = 'left').drop(columns = 'away_Team_Key')\n",
    "# modeling_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f7388289",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9aa80b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_rolling_avg(df, window_size=10, min_periods=1, default_value=np.nan, shift=1):\n",
    "#     # get column names to calculate rolling average for\n",
    "#     cols = [col for col in df.columns if col not in ['season_id', 'Date', 'Home_Team_']]\n",
    "    \n",
    "#     # calculate rolling average for each column and drop original columns\n",
    "#     for col in cols:\n",
    "#         df[f'rolling_avg_{col}'] = df.groupby([df['Date'].dt.year, 'Home_Team_Won'])[col].rolling(window=window_size, min_periods=min_periods).mean().reset_index(0, drop=True)\n",
    "#         df[f'rolling_avg_{col}'] = df[f'rolling_avg_{col}'].fillna(default_value)\n",
    "#         if shift != 0:\n",
    "#             df[f'rolling_avg_{col}'] = df[f'rolling_avg_{col}'].shift(shift)\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b5510233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling_final_df = calculate_rolling_avg(modeling_df)\n",
    "# modeling_final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ab30746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling_final = rolling_features(modeling_df)\n",
    "# modeling_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6e3f73b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['rolling_avg_goals'] = df.groupby(df['date'].dt.year)['goals'].\n",
    "# rolling(window=rolling_window, min_periods=10).mean().reset_index(0, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "628c151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['rolling_avg_goals'] = df.groupby(df['date'].dt.year)['goals'].\n",
    "# rolling(window=10, min_periods=10).mean().shift(1).reset_index(0, drop=True)\n",
    "\n",
    "# df['rolling_avg_goals'] = df.groupby([df['date'].dt.year, 'team'])['goals'].\n",
    "# rolling(window=10, min_periods=10).mean().reset_index(0, drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbbad03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc2495f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sports",
   "language": "python",
   "name": "sports"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
