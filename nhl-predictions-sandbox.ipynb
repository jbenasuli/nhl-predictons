{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbd39c0a",
   "metadata": {},
   "source": [
    "# Predicting NHL Game Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86afdba",
   "metadata": {},
   "source": [
    "## Sandbox Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9d458e",
   "metadata": {},
   "source": [
    "Working notebook \n",
    "When components (e.g. scraping functions) are working, move them to eventual production home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b088915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Packages\n",
    "import pandas as pd\n",
    "from pandas.testing import assert_frame_equal\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Viz Packages\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# Modeling Packages\n",
    "## Modeling Prep\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, KFold, \\\n",
    "GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "## SKLearn Data Prep Modules\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, \\\n",
    "PolynomialFeatures, PowerTransformer, Normalizer, MaxAbsScaler\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "## SKLearn Classification Models\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier,\\\n",
    "ExtraTreesClassifier, VotingClassifier, StackingRegressor, GradientBoostingClassifier\n",
    "\n",
    "## SKLearn Pipeline Setup\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "## SKLearn Model Optimization\n",
    "from sklearn.feature_selection import RFE, f_regression\n",
    "\n",
    "# ## Boosting\n",
    "# from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "## SKLearn Metrics\n",
    "### Classification Scoring/Evaluation\n",
    "from sklearn.metrics import classification_report, accuracy_score, recall_score, precision_score, f1_score, \\\n",
    "ConfusionMatrixDisplay, log_loss, confusion_matrix, RocCurveDisplay, make_scorer, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3453bb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook Config\n",
    "from pprintpp import pprint as pp\n",
    "%reload_ext pprintpp\n",
    "from tqdm import tqdm\n",
    "from io import StringIO\n",
    "\n",
    "## Suppress Python Warnings (Future, Deprecation)\n",
    "warnings.filterwarnings(\"ignore\", category= FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "## Suppress Pandas Warnings (SettingWithCopy)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "## Pandas Display Config\n",
    "pd.options.display.max_columns = 80\n",
    "# pd.options.display.width = None\n",
    "\n",
    "## Display SKLearn estimators as diagrams\n",
    "from sklearn import set_config\n",
    "set_config(display= 'diagram')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e853a875",
   "metadata": {},
   "source": [
    "##  Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dc4d00d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season_id</th>\n",
       "      <th>home_team</th>\n",
       "      <th>away_team</th>\n",
       "      <th>Home_Team_Won</th>\n",
       "      <th>home_pts_pct</th>\n",
       "      <th>away_pts_pct</th>\n",
       "      <th>home_TOI</th>\n",
       "      <th>home_FF/60</th>\n",
       "      <th>home_FA/60</th>\n",
       "      <th>home_FF%</th>\n",
       "      <th>home_GF/60</th>\n",
       "      <th>home_GA/60</th>\n",
       "      <th>home_GF%</th>\n",
       "      <th>home_xGF/60</th>\n",
       "      <th>home_xGA/60</th>\n",
       "      <th>home_xGF%</th>\n",
       "      <th>home_HDCF/60</th>\n",
       "      <th>home_HDCA/60</th>\n",
       "      <th>home_HDCF%</th>\n",
       "      <th>home_HDSH%</th>\n",
       "      <th>home_HDSV%</th>\n",
       "      <th>home_SH%</th>\n",
       "      <th>home_SV%</th>\n",
       "      <th>home_PDO</th>\n",
       "      <th>home_TOI_pp</th>\n",
       "      <th>home_GF/60_pp</th>\n",
       "      <th>home_xGF/60_pp</th>\n",
       "      <th>home_TOI_pk</th>\n",
       "      <th>home_GA/60_pk</th>\n",
       "      <th>home_xGA/60_pk</th>\n",
       "      <th>away_TOI</th>\n",
       "      <th>away_FF/60</th>\n",
       "      <th>away_FA/60</th>\n",
       "      <th>away_FF%</th>\n",
       "      <th>away_GF/60</th>\n",
       "      <th>away_GA/60</th>\n",
       "      <th>away_GF%</th>\n",
       "      <th>away_xGF/60</th>\n",
       "      <th>away_xGA/60</th>\n",
       "      <th>away_xGF%</th>\n",
       "      <th>away_HDCF/60</th>\n",
       "      <th>away_HDCA/60</th>\n",
       "      <th>away_HDCF%</th>\n",
       "      <th>away_HDSH%</th>\n",
       "      <th>away_HDSV%</th>\n",
       "      <th>away_SH%</th>\n",
       "      <th>away_SV%</th>\n",
       "      <th>away_PDO</th>\n",
       "      <th>away_TOI_pp</th>\n",
       "      <th>away_GF/60_pp</th>\n",
       "      <th>away_xGF/60_pp</th>\n",
       "      <th>away_TOI_pk</th>\n",
       "      <th>away_GA/60_pk</th>\n",
       "      <th>away_xGA/60_pk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20192020</td>\n",
       "      <td>TOR</td>\n",
       "      <td>OTT</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.133333</td>\n",
       "      <td>59.15</td>\n",
       "      <td>43.75</td>\n",
       "      <td>57.48</td>\n",
       "      <td>5.16</td>\n",
       "      <td>4.18</td>\n",
       "      <td>55.27</td>\n",
       "      <td>3.44</td>\n",
       "      <td>2.22</td>\n",
       "      <td>60.73</td>\n",
       "      <td>15.74</td>\n",
       "      <td>8.32</td>\n",
       "      <td>65.42</td>\n",
       "      <td>55.99</td>\n",
       "      <td>24.79</td>\n",
       "      <td>11.94</td>\n",
       "      <td>85.60</td>\n",
       "      <td>0.975</td>\n",
       "      <td>8.166667</td>\n",
       "      <td>7.35</td>\n",
       "      <td>7.47</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.77</td>\n",
       "      <td>44.133333</td>\n",
       "      <td>43.75</td>\n",
       "      <td>59.15</td>\n",
       "      <td>42.52</td>\n",
       "      <td>4.18</td>\n",
       "      <td>5.16</td>\n",
       "      <td>44.73</td>\n",
       "      <td>2.22</td>\n",
       "      <td>3.44</td>\n",
       "      <td>39.27</td>\n",
       "      <td>8.32</td>\n",
       "      <td>15.74</td>\n",
       "      <td>34.58</td>\n",
       "      <td>75.21</td>\n",
       "      <td>44.01</td>\n",
       "      <td>14.40</td>\n",
       "      <td>88.06</td>\n",
       "      <td>1.025</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.77</td>\n",
       "      <td>8.166667</td>\n",
       "      <td>7.35</td>\n",
       "      <td>7.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20192020</td>\n",
       "      <td>STL</td>\n",
       "      <td>WSH</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>50.866667</td>\n",
       "      <td>27.17</td>\n",
       "      <td>39.14</td>\n",
       "      <td>40.97</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.20</td>\n",
       "      <td>48.19</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.26</td>\n",
       "      <td>37.48</td>\n",
       "      <td>5.59</td>\n",
       "      <td>6.14</td>\n",
       "      <td>47.66</td>\n",
       "      <td>19.64</td>\n",
       "      <td>100.00</td>\n",
       "      <td>5.56</td>\n",
       "      <td>95.79</td>\n",
       "      <td>1.014</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>18.46</td>\n",
       "      <td>6.44</td>\n",
       "      <td>5.883333</td>\n",
       "      <td>10.20</td>\n",
       "      <td>7.62</td>\n",
       "      <td>50.866667</td>\n",
       "      <td>39.14</td>\n",
       "      <td>27.17</td>\n",
       "      <td>59.03</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.11</td>\n",
       "      <td>51.81</td>\n",
       "      <td>2.26</td>\n",
       "      <td>1.35</td>\n",
       "      <td>62.52</td>\n",
       "      <td>6.14</td>\n",
       "      <td>5.59</td>\n",
       "      <td>52.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>80.36</td>\n",
       "      <td>4.21</td>\n",
       "      <td>94.44</td>\n",
       "      <td>0.986</td>\n",
       "      <td>5.883333</td>\n",
       "      <td>10.20</td>\n",
       "      <td>7.62</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>18.46</td>\n",
       "      <td>6.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20192020</td>\n",
       "      <td>EDM</td>\n",
       "      <td>VAN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.066667</td>\n",
       "      <td>30.38</td>\n",
       "      <td>48.03</td>\n",
       "      <td>38.75</td>\n",
       "      <td>3.58</td>\n",
       "      <td>2.64</td>\n",
       "      <td>57.61</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.99</td>\n",
       "      <td>46.24</td>\n",
       "      <td>6.04</td>\n",
       "      <td>9.11</td>\n",
       "      <td>39.87</td>\n",
       "      <td>39.29</td>\n",
       "      <td>65.85</td>\n",
       "      <td>14.87</td>\n",
       "      <td>91.33</td>\n",
       "      <td>1.062</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.60</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.09</td>\n",
       "      <td>47.066667</td>\n",
       "      <td>48.03</td>\n",
       "      <td>30.38</td>\n",
       "      <td>61.25</td>\n",
       "      <td>2.64</td>\n",
       "      <td>3.58</td>\n",
       "      <td>42.39</td>\n",
       "      <td>1.99</td>\n",
       "      <td>1.71</td>\n",
       "      <td>53.76</td>\n",
       "      <td>9.11</td>\n",
       "      <td>6.04</td>\n",
       "      <td>60.13</td>\n",
       "      <td>34.15</td>\n",
       "      <td>60.71</td>\n",
       "      <td>8.67</td>\n",
       "      <td>85.13</td>\n",
       "      <td>0.938</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.09</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20192020</td>\n",
       "      <td>VGK</td>\n",
       "      <td>S.J</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.666667</td>\n",
       "      <td>51.94</td>\n",
       "      <td>30.07</td>\n",
       "      <td>63.34</td>\n",
       "      <td>2.60</td>\n",
       "      <td>1.33</td>\n",
       "      <td>66.09</td>\n",
       "      <td>3.84</td>\n",
       "      <td>1.40</td>\n",
       "      <td>73.28</td>\n",
       "      <td>16.02</td>\n",
       "      <td>5.27</td>\n",
       "      <td>75.25</td>\n",
       "      <td>12.06</td>\n",
       "      <td>100.00</td>\n",
       "      <td>8.07</td>\n",
       "      <td>93.28</td>\n",
       "      <td>1.013</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>13.85</td>\n",
       "      <td>8.96</td>\n",
       "      <td>6.983333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.33</td>\n",
       "      <td>45.666667</td>\n",
       "      <td>30.07</td>\n",
       "      <td>51.94</td>\n",
       "      <td>36.66</td>\n",
       "      <td>1.33</td>\n",
       "      <td>2.60</td>\n",
       "      <td>33.91</td>\n",
       "      <td>1.40</td>\n",
       "      <td>3.84</td>\n",
       "      <td>26.72</td>\n",
       "      <td>5.27</td>\n",
       "      <td>16.02</td>\n",
       "      <td>24.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>87.94</td>\n",
       "      <td>6.72</td>\n",
       "      <td>91.93</td>\n",
       "      <td>0.987</td>\n",
       "      <td>6.983333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.33</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>13.85</td>\n",
       "      <td>8.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20192020</td>\n",
       "      <td>T.B</td>\n",
       "      <td>FLA</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.500000</td>\n",
       "      <td>32.45</td>\n",
       "      <td>46.56</td>\n",
       "      <td>41.07</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.33</td>\n",
       "      <td>65.64</td>\n",
       "      <td>1.86</td>\n",
       "      <td>1.69</td>\n",
       "      <td>52.43</td>\n",
       "      <td>11.77</td>\n",
       "      <td>9.26</td>\n",
       "      <td>55.98</td>\n",
       "      <td>19.81</td>\n",
       "      <td>83.02</td>\n",
       "      <td>9.94</td>\n",
       "      <td>96.45</td>\n",
       "      <td>1.064</td>\n",
       "      <td>4.233333</td>\n",
       "      <td>14.17</td>\n",
       "      <td>10.54</td>\n",
       "      <td>6.766667</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.25</td>\n",
       "      <td>45.500000</td>\n",
       "      <td>46.56</td>\n",
       "      <td>32.45</td>\n",
       "      <td>58.93</td>\n",
       "      <td>1.33</td>\n",
       "      <td>2.55</td>\n",
       "      <td>34.36</td>\n",
       "      <td>1.69</td>\n",
       "      <td>1.86</td>\n",
       "      <td>47.57</td>\n",
       "      <td>9.26</td>\n",
       "      <td>11.77</td>\n",
       "      <td>44.02</td>\n",
       "      <td>16.98</td>\n",
       "      <td>80.19</td>\n",
       "      <td>3.55</td>\n",
       "      <td>90.06</td>\n",
       "      <td>0.936</td>\n",
       "      <td>6.766667</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.233333</td>\n",
       "      <td>14.17</td>\n",
       "      <td>10.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3257</th>\n",
       "      <td>20212022</td>\n",
       "      <td>DAL</td>\n",
       "      <td>ANA</td>\n",
       "      <td>1</td>\n",
       "      <td>0.597561</td>\n",
       "      <td>0.463415</td>\n",
       "      <td>54.450000</td>\n",
       "      <td>32.22</td>\n",
       "      <td>30.41</td>\n",
       "      <td>51.44</td>\n",
       "      <td>2.06</td>\n",
       "      <td>2.28</td>\n",
       "      <td>47.45</td>\n",
       "      <td>2.21</td>\n",
       "      <td>1.37</td>\n",
       "      <td>61.63</td>\n",
       "      <td>11.32</td>\n",
       "      <td>5.80</td>\n",
       "      <td>66.12</td>\n",
       "      <td>51.77</td>\n",
       "      <td>50.19</td>\n",
       "      <td>12.11</td>\n",
       "      <td>88.68</td>\n",
       "      <td>1.008</td>\n",
       "      <td>2.566667</td>\n",
       "      <td>23.38</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.57</td>\n",
       "      <td>54.450000</td>\n",
       "      <td>30.41</td>\n",
       "      <td>32.22</td>\n",
       "      <td>48.56</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.06</td>\n",
       "      <td>52.55</td>\n",
       "      <td>1.37</td>\n",
       "      <td>2.21</td>\n",
       "      <td>38.37</td>\n",
       "      <td>5.80</td>\n",
       "      <td>11.32</td>\n",
       "      <td>33.88</td>\n",
       "      <td>49.81</td>\n",
       "      <td>48.23</td>\n",
       "      <td>11.32</td>\n",
       "      <td>87.89</td>\n",
       "      <td>0.992</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.57</td>\n",
       "      <td>2.566667</td>\n",
       "      <td>23.38</td>\n",
       "      <td>1.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>20212022</td>\n",
       "      <td>EDM</td>\n",
       "      <td>VAN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>53.783333</td>\n",
       "      <td>39.31</td>\n",
       "      <td>51.30</td>\n",
       "      <td>43.38</td>\n",
       "      <td>2.06</td>\n",
       "      <td>2.36</td>\n",
       "      <td>46.65</td>\n",
       "      <td>2.56</td>\n",
       "      <td>3.93</td>\n",
       "      <td>39.47</td>\n",
       "      <td>13.96</td>\n",
       "      <td>12.29</td>\n",
       "      <td>53.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>7.13</td>\n",
       "      <td>93.16</td>\n",
       "      <td>1.003</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.50</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.36</td>\n",
       "      <td>53.783333</td>\n",
       "      <td>51.30</td>\n",
       "      <td>39.31</td>\n",
       "      <td>56.62</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.06</td>\n",
       "      <td>53.35</td>\n",
       "      <td>3.93</td>\n",
       "      <td>2.56</td>\n",
       "      <td>60.53</td>\n",
       "      <td>12.29</td>\n",
       "      <td>13.96</td>\n",
       "      <td>46.81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>6.84</td>\n",
       "      <td>92.87</td>\n",
       "      <td>0.997</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.36</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>20212022</td>\n",
       "      <td>SEA</td>\n",
       "      <td>S.J</td>\n",
       "      <td>1</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.469512</td>\n",
       "      <td>52.416667</td>\n",
       "      <td>48.21</td>\n",
       "      <td>26.93</td>\n",
       "      <td>64.16</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>2.42</td>\n",
       "      <td>1.42</td>\n",
       "      <td>62.98</td>\n",
       "      <td>11.58</td>\n",
       "      <td>4.54</td>\n",
       "      <td>71.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>6.82</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.068</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.69</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.05</td>\n",
       "      <td>52.416667</td>\n",
       "      <td>26.93</td>\n",
       "      <td>48.21</td>\n",
       "      <td>35.84</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.42</td>\n",
       "      <td>2.42</td>\n",
       "      <td>37.02</td>\n",
       "      <td>4.54</td>\n",
       "      <td>11.58</td>\n",
       "      <td>28.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>93.18</td>\n",
       "      <td>0.932</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.05</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>20212022</td>\n",
       "      <td>ARI</td>\n",
       "      <td>NSH</td>\n",
       "      <td>1</td>\n",
       "      <td>0.347561</td>\n",
       "      <td>0.591463</td>\n",
       "      <td>48.750000</td>\n",
       "      <td>31.75</td>\n",
       "      <td>58.08</td>\n",
       "      <td>35.34</td>\n",
       "      <td>5.73</td>\n",
       "      <td>5.25</td>\n",
       "      <td>52.18</td>\n",
       "      <td>1.60</td>\n",
       "      <td>3.01</td>\n",
       "      <td>34.78</td>\n",
       "      <td>4.30</td>\n",
       "      <td>13.88</td>\n",
       "      <td>23.67</td>\n",
       "      <td>52.52</td>\n",
       "      <td>51.84</td>\n",
       "      <td>25.11</td>\n",
       "      <td>83.60</td>\n",
       "      <td>1.087</td>\n",
       "      <td>5.950000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.61</td>\n",
       "      <td>3.950000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.76</td>\n",
       "      <td>48.750000</td>\n",
       "      <td>58.08</td>\n",
       "      <td>31.75</td>\n",
       "      <td>64.66</td>\n",
       "      <td>5.25</td>\n",
       "      <td>5.73</td>\n",
       "      <td>47.82</td>\n",
       "      <td>3.01</td>\n",
       "      <td>1.60</td>\n",
       "      <td>65.22</td>\n",
       "      <td>13.88</td>\n",
       "      <td>4.30</td>\n",
       "      <td>76.33</td>\n",
       "      <td>48.16</td>\n",
       "      <td>47.48</td>\n",
       "      <td>16.40</td>\n",
       "      <td>74.89</td>\n",
       "      <td>0.913</td>\n",
       "      <td>3.950000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.76</td>\n",
       "      <td>5.950000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>20212022</td>\n",
       "      <td>WPG</td>\n",
       "      <td>SEA</td>\n",
       "      <td>1</td>\n",
       "      <td>0.542683</td>\n",
       "      <td>0.365854</td>\n",
       "      <td>55.400000</td>\n",
       "      <td>39.14</td>\n",
       "      <td>40.72</td>\n",
       "      <td>49.02</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "      <td>63.82</td>\n",
       "      <td>2.71</td>\n",
       "      <td>2.64</td>\n",
       "      <td>50.69</td>\n",
       "      <td>12.27</td>\n",
       "      <td>14.91</td>\n",
       "      <td>45.14</td>\n",
       "      <td>12.52</td>\n",
       "      <td>81.25</td>\n",
       "      <td>14.91</td>\n",
       "      <td>92.14</td>\n",
       "      <td>1.071</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.95</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>163.64</td>\n",
       "      <td>36.03</td>\n",
       "      <td>55.400000</td>\n",
       "      <td>40.72</td>\n",
       "      <td>39.14</td>\n",
       "      <td>50.98</td>\n",
       "      <td>2.31</td>\n",
       "      <td>4.07</td>\n",
       "      <td>36.18</td>\n",
       "      <td>2.64</td>\n",
       "      <td>2.71</td>\n",
       "      <td>49.31</td>\n",
       "      <td>14.91</td>\n",
       "      <td>12.27</td>\n",
       "      <td>54.86</td>\n",
       "      <td>18.75</td>\n",
       "      <td>87.48</td>\n",
       "      <td>7.86</td>\n",
       "      <td>85.09</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>163.64</td>\n",
       "      <td>36.03</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3262 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      season_id home_team away_team  Home_Team_Won  home_pts_pct  \\\n",
       "0      20192020       TOR       OTT              1      1.000000   \n",
       "1      20192020       STL       WSH              0      0.500000   \n",
       "2      20192020       EDM       VAN              1      1.000000   \n",
       "3      20192020       VGK       S.J              1      1.000000   \n",
       "4      20192020       T.B       FLA              1      1.000000   \n",
       "...         ...       ...       ...            ...           ...   \n",
       "3257   20212022       DAL       ANA              1      0.597561   \n",
       "3258   20212022       EDM       VAN              1      0.634146   \n",
       "3259   20212022       SEA       S.J              1      0.370370   \n",
       "3260   20212022       ARI       NSH              1      0.347561   \n",
       "3261   20212022       WPG       SEA              1      0.542683   \n",
       "\n",
       "      away_pts_pct   home_TOI  home_FF/60  home_FA/60  home_FF%  home_GF/60  \\\n",
       "0         0.000000  44.133333       59.15       43.75     57.48        5.16   \n",
       "1         1.000000  50.866667       27.17       39.14     40.97        1.11   \n",
       "2         0.000000  47.066667       30.38       48.03     38.75        3.58   \n",
       "3         0.000000  45.666667       51.94       30.07     63.34        2.60   \n",
       "4         0.000000  45.500000       32.45       46.56     41.07        2.55   \n",
       "...            ...        ...         ...         ...       ...         ...   \n",
       "3257      0.463415  54.450000       32.22       30.41     51.44        2.06   \n",
       "3258      0.560976  53.783333       39.31       51.30     43.38        2.06   \n",
       "3259      0.469512  52.416667       48.21       26.93     64.16        2.21   \n",
       "3260      0.591463  48.750000       31.75       58.08     35.34        5.73   \n",
       "3261      0.365854  55.400000       39.14       40.72     49.02        4.07   \n",
       "\n",
       "      home_GA/60  home_GF%  home_xGF/60  home_xGA/60  home_xGF%  home_HDCF/60  \\\n",
       "0           4.18     55.27         3.44         2.22      60.73         15.74   \n",
       "1           1.20     48.19         1.35         2.26      37.48          5.59   \n",
       "2           2.64     57.61         1.71         1.99      46.24          6.04   \n",
       "3           1.33     66.09         3.84         1.40      73.28         16.02   \n",
       "4           1.33     65.64         1.86         1.69      52.43         11.77   \n",
       "...          ...       ...          ...          ...        ...           ...   \n",
       "3257        2.28     47.45         2.21         1.37      61.63         11.32   \n",
       "3258        2.36     46.65         2.56         3.93      39.47         13.96   \n",
       "3259        0.00    100.00         2.42         1.42      62.98         11.58   \n",
       "3260        5.25     52.18         1.60         3.01      34.78          4.30   \n",
       "3261        2.31     63.82         2.71         2.64      50.69         12.27   \n",
       "\n",
       "      home_HDCA/60  home_HDCF%  home_HDSH%  home_HDSV%  home_SH%  home_SV%  \\\n",
       "0             8.32       65.42       55.99       24.79     11.94     85.60   \n",
       "1             6.14       47.66       19.64      100.00      5.56     95.79   \n",
       "2             9.11       39.87       39.29       65.85     14.87     91.33   \n",
       "3             5.27       75.25       12.06      100.00      8.07     93.28   \n",
       "4             9.26       55.98       19.81       83.02      9.94     96.45   \n",
       "...            ...         ...         ...         ...       ...       ...   \n",
       "3257          5.80       66.12       51.77       50.19     12.11     88.68   \n",
       "3258         12.29       53.19        0.00      100.00      7.13     93.16   \n",
       "3259          4.54       71.82        0.00      100.00      6.82    100.00   \n",
       "3260         13.88       23.67       52.52       51.84     25.11     83.60   \n",
       "3261         14.91       45.14       12.52       81.25     14.91     92.14   \n",
       "\n",
       "      home_PDO  home_TOI_pp  home_GF/60_pp  home_xGF/60_pp  home_TOI_pk  \\\n",
       "0        0.975     8.166667           7.35            7.47     6.000000   \n",
       "1        1.014     3.250000          18.46            6.44     5.883333   \n",
       "2        1.062     4.000000           0.00            4.60     8.000000   \n",
       "3        1.013     4.333333          13.85            8.96     6.983333   \n",
       "4        1.064     4.233333          14.17           10.54     6.766667   \n",
       "...        ...          ...            ...             ...          ...   \n",
       "3257     1.008     2.566667          23.38            1.87     2.000000   \n",
       "3258     1.003     2.000000           0.00           12.50     4.000000   \n",
       "3259     1.068     2.000000           0.00           12.69     1.600000   \n",
       "3260     1.087     5.950000           0.00           15.61     3.950000   \n",
       "3261     1.071     2.000000           0.00            5.95     0.366667   \n",
       "\n",
       "      home_GA/60_pk  home_xGA/60_pk   away_TOI  away_FF/60  away_FA/60  \\\n",
       "0              0.00            2.77  44.133333       43.75       59.15   \n",
       "1             10.20            7.62  50.866667       39.14       27.17   \n",
       "2              0.00            9.09  47.066667       48.03       30.38   \n",
       "3              0.00            5.33  45.666667       30.07       51.94   \n",
       "4              0.00            4.25  45.500000       46.56       32.45   \n",
       "...             ...             ...        ...         ...         ...   \n",
       "3257           0.00            7.57  54.450000       30.41       32.22   \n",
       "3258           0.00           17.36  53.783333       51.30       39.31   \n",
       "3259           0.00           11.05  52.416667       26.93       48.21   \n",
       "3260           0.00            9.76  48.750000       58.08       31.75   \n",
       "3261         163.64           36.03  55.400000       40.72       39.14   \n",
       "\n",
       "      away_FF%  away_GF/60  away_GA/60  away_GF%  away_xGF/60  away_xGA/60  \\\n",
       "0        42.52        4.18        5.16     44.73         2.22         3.44   \n",
       "1        59.03        1.20        1.11     51.81         2.26         1.35   \n",
       "2        61.25        2.64        3.58     42.39         1.99         1.71   \n",
       "3        36.66        1.33        2.60     33.91         1.40         3.84   \n",
       "4        58.93        1.33        2.55     34.36         1.69         1.86   \n",
       "...        ...         ...         ...       ...          ...          ...   \n",
       "3257     48.56        2.28        2.06     52.55         1.37         2.21   \n",
       "3258     56.62        2.36        2.06     53.35         3.93         2.56   \n",
       "3259     35.84        0.00        2.21      0.00         1.42         2.42   \n",
       "3260     64.66        5.25        5.73     47.82         3.01         1.60   \n",
       "3261     50.98        2.31        4.07     36.18         2.64         2.71   \n",
       "\n",
       "      away_xGF%  away_HDCF/60  away_HDCA/60  away_HDCF%  away_HDSH%  \\\n",
       "0         39.27          8.32         15.74       34.58       75.21   \n",
       "1         62.52          6.14          5.59       52.34        0.00   \n",
       "2         53.76          9.11          6.04       60.13       34.15   \n",
       "3         26.72          5.27         16.02       24.75        0.00   \n",
       "4         47.57          9.26         11.77       44.02       16.98   \n",
       "...         ...           ...           ...         ...         ...   \n",
       "3257      38.37          5.80         11.32       33.88       49.81   \n",
       "3258      60.53         12.29         13.96       46.81        0.00   \n",
       "3259      37.02          4.54         11.58       28.18        0.00   \n",
       "3260      65.22         13.88          4.30       76.33       48.16   \n",
       "3261      49.31         14.91         12.27       54.86       18.75   \n",
       "\n",
       "      away_HDSV%  away_SH%  away_SV%  away_PDO  away_TOI_pp  away_GF/60_pp  \\\n",
       "0          44.01     14.40     88.06     1.025     6.000000           0.00   \n",
       "1          80.36      4.21     94.44     0.986     5.883333          10.20   \n",
       "2          60.71      8.67     85.13     0.938     8.000000           0.00   \n",
       "3          87.94      6.72     91.93     0.987     6.983333           0.00   \n",
       "4          80.19      3.55     90.06     0.936     6.766667           0.00   \n",
       "...          ...       ...       ...       ...          ...            ...   \n",
       "3257       48.23     11.32     87.89     0.992     2.000000           0.00   \n",
       "3258      100.00      6.84     92.87     0.997     4.000000           0.00   \n",
       "3259      100.00      0.00     93.18     0.932     1.600000           0.00   \n",
       "3260       47.48     16.40     74.89     0.913     3.950000           0.00   \n",
       "3261       87.48      7.86     85.09     0.929     0.366667         163.64   \n",
       "\n",
       "      away_xGF/60_pp  away_TOI_pk  away_GA/60_pk  away_xGA/60_pk  \n",
       "0               2.77     8.166667           7.35            7.47  \n",
       "1               7.62     3.250000          18.46            6.44  \n",
       "2               9.09     4.000000           0.00            4.60  \n",
       "3               5.33     4.333333          13.85            8.96  \n",
       "4               4.25     4.233333          14.17           10.54  \n",
       "...              ...          ...            ...             ...  \n",
       "3257            7.57     2.566667          23.38            1.87  \n",
       "3258           17.36     2.000000           0.00           12.50  \n",
       "3259           11.05     2.000000           0.00           12.69  \n",
       "3260            9.76     5.950000           0.00           15.61  \n",
       "3261           36.03     2.000000           0.00            5.95  \n",
       "\n",
       "[3262 rows x 54 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load modeling df\n",
    "modeling_df = pd.read_csv('modeling_data.csv')\n",
    "modeling_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ff048ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3262 entries, 0 to 3261\n",
      "Data columns (total 54 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   season_id       3262 non-null   int64  \n",
      " 1   home_team       3262 non-null   object \n",
      " 2   away_team       3262 non-null   object \n",
      " 3   Home_Team_Won   3262 non-null   int64  \n",
      " 4   home_pts_pct    3262 non-null   float64\n",
      " 5   away_pts_pct    3262 non-null   float64\n",
      " 6   home_TOI        3262 non-null   float64\n",
      " 7   home_FF/60      3262 non-null   float64\n",
      " 8   home_FA/60      3262 non-null   float64\n",
      " 9   home_FF%        3262 non-null   float64\n",
      " 10  home_GF/60      3262 non-null   float64\n",
      " 11  home_GA/60      3262 non-null   float64\n",
      " 12  home_GF%        3262 non-null   float64\n",
      " 13  home_xGF/60     3262 non-null   float64\n",
      " 14  home_xGA/60     3262 non-null   float64\n",
      " 15  home_xGF%       3262 non-null   float64\n",
      " 16  home_HDCF/60    3262 non-null   float64\n",
      " 17  home_HDCA/60    3262 non-null   float64\n",
      " 18  home_HDCF%      3262 non-null   float64\n",
      " 19  home_HDSH%      3262 non-null   float64\n",
      " 20  home_HDSV%      3262 non-null   float64\n",
      " 21  home_SH%        3262 non-null   float64\n",
      " 22  home_SV%        3262 non-null   float64\n",
      " 23  home_PDO        3262 non-null   float64\n",
      " 24  home_TOI_pp     3262 non-null   float64\n",
      " 25  home_GF/60_pp   3262 non-null   float64\n",
      " 26  home_xGF/60_pp  3262 non-null   float64\n",
      " 27  home_TOI_pk     3262 non-null   float64\n",
      " 28  home_GA/60_pk   3262 non-null   float64\n",
      " 29  home_xGA/60_pk  3262 non-null   float64\n",
      " 30  away_TOI        3262 non-null   float64\n",
      " 31  away_FF/60      3262 non-null   float64\n",
      " 32  away_FA/60      3262 non-null   float64\n",
      " 33  away_FF%        3262 non-null   float64\n",
      " 34  away_GF/60      3262 non-null   float64\n",
      " 35  away_GA/60      3262 non-null   float64\n",
      " 36  away_GF%        3262 non-null   float64\n",
      " 37  away_xGF/60     3262 non-null   float64\n",
      " 38  away_xGA/60     3262 non-null   float64\n",
      " 39  away_xGF%       3262 non-null   float64\n",
      " 40  away_HDCF/60    3262 non-null   float64\n",
      " 41  away_HDCA/60    3262 non-null   float64\n",
      " 42  away_HDCF%      3262 non-null   float64\n",
      " 43  away_HDSH%      3262 non-null   float64\n",
      " 44  away_HDSV%      3262 non-null   float64\n",
      " 45  away_SH%        3262 non-null   float64\n",
      " 46  away_SV%        3262 non-null   float64\n",
      " 47  away_PDO        3262 non-null   float64\n",
      " 48  away_TOI_pp     3262 non-null   float64\n",
      " 49  away_GF/60_pp   3262 non-null   float64\n",
      " 50  away_xGF/60_pp  3262 non-null   float64\n",
      " 51  away_TOI_pk     3262 non-null   float64\n",
      " 52  away_GA/60_pk   3262 non-null   float64\n",
      " 53  away_xGA/60_pk  3262 non-null   float64\n",
      "dtypes: float64(50), int64(2), object(2)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# inspect df\n",
    "modeling_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5bb4413d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'str'>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop team names and convert season id back to string\n",
    "modeling_df = modeling_df.drop(columns=['home_team', 'away_team'], axis=1)\n",
    "modeling_df['season_id'] = modeling_df['season_id'].astype(str)\n",
    "type(modeling_df['season_id'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5f14e935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Home_Team_Won</th>\n",
       "      <th>home_pts_pct</th>\n",
       "      <th>away_pts_pct</th>\n",
       "      <th>home_TOI</th>\n",
       "      <th>home_FF/60</th>\n",
       "      <th>home_FA/60</th>\n",
       "      <th>home_FF%</th>\n",
       "      <th>home_GF/60</th>\n",
       "      <th>home_GA/60</th>\n",
       "      <th>home_GF%</th>\n",
       "      <th>home_xGF/60</th>\n",
       "      <th>home_xGA/60</th>\n",
       "      <th>home_xGF%</th>\n",
       "      <th>home_HDCF/60</th>\n",
       "      <th>home_HDCA/60</th>\n",
       "      <th>home_HDCF%</th>\n",
       "      <th>home_HDSH%</th>\n",
       "      <th>home_HDSV%</th>\n",
       "      <th>home_SH%</th>\n",
       "      <th>home_SV%</th>\n",
       "      <th>home_PDO</th>\n",
       "      <th>home_TOI_pp</th>\n",
       "      <th>home_GF/60_pp</th>\n",
       "      <th>home_xGF/60_pp</th>\n",
       "      <th>home_TOI_pk</th>\n",
       "      <th>home_GA/60_pk</th>\n",
       "      <th>home_xGA/60_pk</th>\n",
       "      <th>away_TOI</th>\n",
       "      <th>away_FF/60</th>\n",
       "      <th>away_FA/60</th>\n",
       "      <th>away_FF%</th>\n",
       "      <th>away_GF/60</th>\n",
       "      <th>away_GA/60</th>\n",
       "      <th>away_GF%</th>\n",
       "      <th>away_xGF/60</th>\n",
       "      <th>away_xGA/60</th>\n",
       "      <th>away_xGF%</th>\n",
       "      <th>away_HDCF/60</th>\n",
       "      <th>away_HDCA/60</th>\n",
       "      <th>away_HDCF%</th>\n",
       "      <th>away_HDSH%</th>\n",
       "      <th>away_HDSV%</th>\n",
       "      <th>away_SH%</th>\n",
       "      <th>away_SV%</th>\n",
       "      <th>away_PDO</th>\n",
       "      <th>away_TOI_pp</th>\n",
       "      <th>away_GF/60_pp</th>\n",
       "      <th>away_xGF/60_pp</th>\n",
       "      <th>away_TOI_pk</th>\n",
       "      <th>away_GA/60_pk</th>\n",
       "      <th>away_xGA/60_pk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "      <td>3262.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.534641</td>\n",
       "      <td>0.563403</td>\n",
       "      <td>0.557321</td>\n",
       "      <td>48.439081</td>\n",
       "      <td>41.129801</td>\n",
       "      <td>41.624620</td>\n",
       "      <td>49.702011</td>\n",
       "      <td>2.476438</td>\n",
       "      <td>2.528617</td>\n",
       "      <td>48.374914</td>\n",
       "      <td>2.354442</td>\n",
       "      <td>2.439565</td>\n",
       "      <td>49.218627</td>\n",
       "      <td>10.507174</td>\n",
       "      <td>10.800015</td>\n",
       "      <td>49.578452</td>\n",
       "      <td>18.202290</td>\n",
       "      <td>81.672029</td>\n",
       "      <td>8.330914</td>\n",
       "      <td>91.614939</td>\n",
       "      <td>0.999455</td>\n",
       "      <td>4.809325</td>\n",
       "      <td>9.539620</td>\n",
       "      <td>7.049914</td>\n",
       "      <td>4.481571</td>\n",
       "      <td>11.044651</td>\n",
       "      <td>6.954243</td>\n",
       "      <td>48.439081</td>\n",
       "      <td>41.624620</td>\n",
       "      <td>41.129801</td>\n",
       "      <td>50.297989</td>\n",
       "      <td>2.528617</td>\n",
       "      <td>2.476438</td>\n",
       "      <td>49.908348</td>\n",
       "      <td>2.439565</td>\n",
       "      <td>2.354442</td>\n",
       "      <td>50.781373</td>\n",
       "      <td>10.800015</td>\n",
       "      <td>10.507174</td>\n",
       "      <td>50.421548</td>\n",
       "      <td>17.960098</td>\n",
       "      <td>81.276557</td>\n",
       "      <td>8.385061</td>\n",
       "      <td>91.669086</td>\n",
       "      <td>1.000545</td>\n",
       "      <td>4.481571</td>\n",
       "      <td>11.044651</td>\n",
       "      <td>6.954243</td>\n",
       "      <td>4.809325</td>\n",
       "      <td>9.539620</td>\n",
       "      <td>7.049914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.498875</td>\n",
       "      <td>0.152318</td>\n",
       "      <td>0.154450</td>\n",
       "      <td>4.155143</td>\n",
       "      <td>9.191437</td>\n",
       "      <td>9.207002</td>\n",
       "      <td>8.842655</td>\n",
       "      <td>1.784834</td>\n",
       "      <td>1.785034</td>\n",
       "      <td>28.989772</td>\n",
       "      <td>0.765206</td>\n",
       "      <td>0.803265</td>\n",
       "      <td>11.474428</td>\n",
       "      <td>4.400675</td>\n",
       "      <td>4.718116</td>\n",
       "      <td>15.266833</td>\n",
       "      <td>17.672003</td>\n",
       "      <td>18.471168</td>\n",
       "      <td>5.821084</td>\n",
       "      <td>5.840097</td>\n",
       "      <td>0.082300</td>\n",
       "      <td>2.428310</td>\n",
       "      <td>18.181066</td>\n",
       "      <td>5.878934</td>\n",
       "      <td>2.331274</td>\n",
       "      <td>37.227603</td>\n",
       "      <td>8.295401</td>\n",
       "      <td>4.155143</td>\n",
       "      <td>9.207002</td>\n",
       "      <td>9.191437</td>\n",
       "      <td>8.842655</td>\n",
       "      <td>1.785034</td>\n",
       "      <td>1.784834</td>\n",
       "      <td>29.035155</td>\n",
       "      <td>0.803265</td>\n",
       "      <td>0.765206</td>\n",
       "      <td>11.474428</td>\n",
       "      <td>4.718116</td>\n",
       "      <td>4.400675</td>\n",
       "      <td>15.266833</td>\n",
       "      <td>17.825235</td>\n",
       "      <td>18.579049</td>\n",
       "      <td>5.840097</td>\n",
       "      <td>5.821084</td>\n",
       "      <td>0.082300</td>\n",
       "      <td>2.331274</td>\n",
       "      <td>37.227603</td>\n",
       "      <td>8.295401</td>\n",
       "      <td>2.428310</td>\n",
       "      <td>18.181066</td>\n",
       "      <td>5.878934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.316667</td>\n",
       "      <td>15.680000</td>\n",
       "      <td>16.760000</td>\n",
       "      <td>20.710000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>15.070000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-7.490000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62.160000</td>\n",
       "      <td>0.678000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.316667</td>\n",
       "      <td>16.760000</td>\n",
       "      <td>15.680000</td>\n",
       "      <td>21.510000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>16.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-7.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>53.520000</td>\n",
       "      <td>0.593000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>45.733333</td>\n",
       "      <td>34.662500</td>\n",
       "      <td>35.330000</td>\n",
       "      <td>43.602500</td>\n",
       "      <td>1.160000</td>\n",
       "      <td>1.270000</td>\n",
       "      <td>30.430000</td>\n",
       "      <td>1.810000</td>\n",
       "      <td>1.870000</td>\n",
       "      <td>41.320000</td>\n",
       "      <td>7.350000</td>\n",
       "      <td>7.380000</td>\n",
       "      <td>39.122500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>72.022500</td>\n",
       "      <td>4.160000</td>\n",
       "      <td>88.135000</td>\n",
       "      <td>0.947000</td>\n",
       "      <td>3.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.572500</td>\n",
       "      <td>2.650000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>45.733333</td>\n",
       "      <td>35.330000</td>\n",
       "      <td>34.662500</td>\n",
       "      <td>44.410000</td>\n",
       "      <td>1.270000</td>\n",
       "      <td>1.160000</td>\n",
       "      <td>33.940000</td>\n",
       "      <td>1.870000</td>\n",
       "      <td>1.810000</td>\n",
       "      <td>42.835000</td>\n",
       "      <td>7.380000</td>\n",
       "      <td>7.350000</td>\n",
       "      <td>40.252500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>72.005000</td>\n",
       "      <td>4.402500</td>\n",
       "      <td>88.122500</td>\n",
       "      <td>0.947000</td>\n",
       "      <td>2.650000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>3.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.572500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.569532</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>48.650000</td>\n",
       "      <td>40.540000</td>\n",
       "      <td>41.105000</td>\n",
       "      <td>49.905000</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>2.450000</td>\n",
       "      <td>47.550000</td>\n",
       "      <td>2.240000</td>\n",
       "      <td>2.350000</td>\n",
       "      <td>49.080000</td>\n",
       "      <td>10.100000</td>\n",
       "      <td>10.230000</td>\n",
       "      <td>49.545000</td>\n",
       "      <td>16.180000</td>\n",
       "      <td>83.720000</td>\n",
       "      <td>7.710000</td>\n",
       "      <td>92.430000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>4.450000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.950000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.670000</td>\n",
       "      <td>48.650000</td>\n",
       "      <td>41.105000</td>\n",
       "      <td>40.540000</td>\n",
       "      <td>50.095000</td>\n",
       "      <td>2.450000</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>51.810000</td>\n",
       "      <td>2.350000</td>\n",
       "      <td>2.240000</td>\n",
       "      <td>50.920000</td>\n",
       "      <td>10.230000</td>\n",
       "      <td>10.100000</td>\n",
       "      <td>50.455000</td>\n",
       "      <td>16.145000</td>\n",
       "      <td>83.665000</td>\n",
       "      <td>7.570000</td>\n",
       "      <td>92.290000</td>\n",
       "      <td>1.001000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.670000</td>\n",
       "      <td>4.450000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.662500</td>\n",
       "      <td>0.660630</td>\n",
       "      <td>51.412500</td>\n",
       "      <td>46.975000</td>\n",
       "      <td>47.377500</td>\n",
       "      <td>55.590000</td>\n",
       "      <td>3.520000</td>\n",
       "      <td>3.697500</td>\n",
       "      <td>65.640000</td>\n",
       "      <td>2.790000</td>\n",
       "      <td>2.910000</td>\n",
       "      <td>57.165000</td>\n",
       "      <td>13.180000</td>\n",
       "      <td>13.747500</td>\n",
       "      <td>59.747500</td>\n",
       "      <td>27.600000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>11.877500</td>\n",
       "      <td>95.597500</td>\n",
       "      <td>1.053000</td>\n",
       "      <td>6.100000</td>\n",
       "      <td>13.640000</td>\n",
       "      <td>9.140000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>12.747500</td>\n",
       "      <td>8.707500</td>\n",
       "      <td>51.412500</td>\n",
       "      <td>47.377500</td>\n",
       "      <td>46.975000</td>\n",
       "      <td>56.397500</td>\n",
       "      <td>3.697500</td>\n",
       "      <td>3.520000</td>\n",
       "      <td>69.160000</td>\n",
       "      <td>2.910000</td>\n",
       "      <td>2.790000</td>\n",
       "      <td>58.680000</td>\n",
       "      <td>13.747500</td>\n",
       "      <td>13.180000</td>\n",
       "      <td>60.877500</td>\n",
       "      <td>27.760000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>11.865000</td>\n",
       "      <td>95.840000</td>\n",
       "      <td>1.053000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>12.747500</td>\n",
       "      <td>8.707500</td>\n",
       "      <td>6.100000</td>\n",
       "      <td>13.640000</td>\n",
       "      <td>9.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>68.200000</td>\n",
       "      <td>79.200000</td>\n",
       "      <td>85.170000</td>\n",
       "      <td>78.490000</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>11.510000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>6.120000</td>\n",
       "      <td>6.060000</td>\n",
       "      <td>83.400000</td>\n",
       "      <td>33.230000</td>\n",
       "      <td>35.920000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>107.020000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>46.480000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.407000</td>\n",
       "      <td>17.083333</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>97.820000</td>\n",
       "      <td>14.150000</td>\n",
       "      <td>720.000000</td>\n",
       "      <td>248.980000</td>\n",
       "      <td>68.200000</td>\n",
       "      <td>85.170000</td>\n",
       "      <td>79.200000</td>\n",
       "      <td>79.290000</td>\n",
       "      <td>11.510000</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>6.060000</td>\n",
       "      <td>6.120000</td>\n",
       "      <td>84.930000</td>\n",
       "      <td>35.920000</td>\n",
       "      <td>33.230000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>107.490000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>37.840000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.322000</td>\n",
       "      <td>14.150000</td>\n",
       "      <td>720.000000</td>\n",
       "      <td>248.980000</td>\n",
       "      <td>17.083333</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>97.820000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Home_Team_Won  home_pts_pct  away_pts_pct     home_TOI   home_FF/60  \\\n",
       "count    3262.000000   3262.000000   3262.000000  3262.000000  3262.000000   \n",
       "mean        0.534641      0.563403      0.557321    48.439081    41.129801   \n",
       "std         0.498875      0.152318      0.154450     4.155143     9.191437   \n",
       "min         0.000000      0.000000      0.000000    32.316667    15.680000   \n",
       "25%         0.000000      0.472222      0.470000    45.733333    34.662500   \n",
       "50%         1.000000      0.569532      0.568182    48.650000    40.540000   \n",
       "75%         1.000000      0.662500      0.660630    51.412500    46.975000   \n",
       "max         1.000000      1.000000      1.000000    68.200000    79.200000   \n",
       "\n",
       "        home_FA/60     home_FF%   home_GF/60   home_GA/60     home_GF%  \\\n",
       "count  3262.000000  3262.000000  3262.000000  3262.000000  3262.000000   \n",
       "mean     41.624620    49.702011     2.476438     2.528617    48.374914   \n",
       "std       9.207002     8.842655     1.784834     1.785034    28.989772   \n",
       "min      16.760000    20.710000     0.000000     0.000000     0.000000   \n",
       "25%      35.330000    43.602500     1.160000     1.270000    30.430000   \n",
       "50%      41.105000    49.905000     2.300000     2.450000    47.550000   \n",
       "75%      47.377500    55.590000     3.520000     3.697500    65.640000   \n",
       "max      85.170000    78.490000    11.500000    11.510000   100.000000   \n",
       "\n",
       "       home_xGF/60  home_xGA/60    home_xGF%  home_HDCF/60  home_HDCA/60  \\\n",
       "count  3262.000000  3262.000000  3262.000000   3262.000000   3262.000000   \n",
       "mean      2.354442     2.439565    49.218627     10.507174     10.800015   \n",
       "std       0.765206     0.803265    11.474428      4.400675      4.718116   \n",
       "min       0.570000     0.560000    15.070000      0.000000      0.000000   \n",
       "25%       1.810000     1.870000    41.320000      7.350000      7.380000   \n",
       "50%       2.240000     2.350000    49.080000     10.100000     10.230000   \n",
       "75%       2.790000     2.910000    57.165000     13.180000     13.747500   \n",
       "max       6.120000     6.060000    83.400000     33.230000     35.920000   \n",
       "\n",
       "        home_HDCF%   home_HDSH%   home_HDSV%     home_SH%     home_SV%  \\\n",
       "count  3262.000000  3262.000000  3262.000000  3262.000000  3262.000000   \n",
       "mean     49.578452    18.202290    81.672029     8.330914    91.614939   \n",
       "std      15.266833    17.672003    18.471168     5.821084     5.840097   \n",
       "min       0.000000     0.000000    -7.490000     0.000000    62.160000   \n",
       "25%      39.122500     0.000000    72.022500     4.160000    88.135000   \n",
       "50%      49.545000    16.180000    83.720000     7.710000    92.430000   \n",
       "75%      59.747500    27.600000   100.000000    11.877500    95.597500   \n",
       "max     100.000000   107.020000   100.000000    46.480000   100.000000   \n",
       "\n",
       "          home_PDO  home_TOI_pp  home_GF/60_pp  home_xGF/60_pp  home_TOI_pk  \\\n",
       "count  3262.000000  3262.000000    3262.000000     3262.000000  3262.000000   \n",
       "mean      0.999455     4.809325       9.539620        7.049914     4.481571   \n",
       "std       0.082300     2.428310      18.181066        5.878934     2.331274   \n",
       "min       0.678000     0.000000       0.000000        0.000000     0.000000   \n",
       "25%       0.947000     3.083333       0.000000        3.572500     2.650000   \n",
       "50%       0.999000     4.450000       0.000000        5.950000     4.000000   \n",
       "75%       1.053000     6.100000      13.640000        9.140000     6.000000   \n",
       "max       1.407000    17.083333     300.000000       97.820000    14.150000   \n",
       "\n",
       "       home_GA/60_pk  home_xGA/60_pk     away_TOI   away_FF/60   away_FA/60  \\\n",
       "count    3262.000000     3262.000000  3262.000000  3262.000000  3262.000000   \n",
       "mean       11.044651        6.954243    48.439081    41.624620    41.129801   \n",
       "std        37.227603        8.295401     4.155143     9.207002     9.191437   \n",
       "min         0.000000        0.000000    32.316667    16.760000    15.680000   \n",
       "25%         0.000000        3.320000    45.733333    35.330000    34.662500   \n",
       "50%         0.000000        5.670000    48.650000    41.105000    40.540000   \n",
       "75%        12.747500        8.707500    51.412500    47.377500    46.975000   \n",
       "max       720.000000      248.980000    68.200000    85.170000    79.200000   \n",
       "\n",
       "          away_FF%   away_GF/60   away_GA/60     away_GF%  away_xGF/60  \\\n",
       "count  3262.000000  3262.000000  3262.000000  3262.000000  3262.000000   \n",
       "mean     50.297989     2.528617     2.476438    49.908348     2.439565   \n",
       "std       8.842655     1.785034     1.784834    29.035155     0.803265   \n",
       "min      21.510000     0.000000     0.000000     0.000000     0.560000   \n",
       "25%      44.410000     1.270000     1.160000    33.940000     1.870000   \n",
       "50%      50.095000     2.450000     2.300000    51.810000     2.350000   \n",
       "75%      56.397500     3.697500     3.520000    69.160000     2.910000   \n",
       "max      79.290000    11.510000    11.500000   100.000000     6.060000   \n",
       "\n",
       "       away_xGA/60    away_xGF%  away_HDCF/60  away_HDCA/60   away_HDCF%  \\\n",
       "count  3262.000000  3262.000000   3262.000000   3262.000000  3262.000000   \n",
       "mean      2.354442    50.781373     10.800015     10.507174    50.421548   \n",
       "std       0.765206    11.474428      4.718116      4.400675    15.266833   \n",
       "min       0.570000    16.600000      0.000000      0.000000     0.000000   \n",
       "25%       1.810000    42.835000      7.380000      7.350000    40.252500   \n",
       "50%       2.240000    50.920000     10.230000     10.100000    50.455000   \n",
       "75%       2.790000    58.680000     13.747500     13.180000    60.877500   \n",
       "max       6.120000    84.930000     35.920000     33.230000   100.000000   \n",
       "\n",
       "        away_HDSH%   away_HDSV%     away_SH%     away_SV%     away_PDO  \\\n",
       "count  3262.000000  3262.000000  3262.000000  3262.000000  3262.000000   \n",
       "mean     17.960098    81.276557     8.385061    91.669086     1.000545   \n",
       "std      17.825235    18.579049     5.840097     5.821084     0.082300   \n",
       "min       0.000000    -7.020000     0.000000    53.520000     0.593000   \n",
       "25%       0.000000    72.005000     4.402500    88.122500     0.947000   \n",
       "50%      16.145000    83.665000     7.570000    92.290000     1.001000   \n",
       "75%      27.760000   100.000000    11.865000    95.840000     1.053000   \n",
       "max     107.490000   100.000000    37.840000   100.000000     1.322000   \n",
       "\n",
       "       away_TOI_pp  away_GF/60_pp  away_xGF/60_pp  away_TOI_pk  away_GA/60_pk  \\\n",
       "count  3262.000000    3262.000000     3262.000000  3262.000000    3262.000000   \n",
       "mean      4.481571      11.044651        6.954243     4.809325       9.539620   \n",
       "std       2.331274      37.227603        8.295401     2.428310      18.181066   \n",
       "min       0.000000       0.000000        0.000000     0.000000       0.000000   \n",
       "25%       2.650000       0.000000        3.320000     3.083333       0.000000   \n",
       "50%       4.000000       0.000000        5.670000     4.450000       0.000000   \n",
       "75%       6.000000      12.747500        8.707500     6.100000      13.640000   \n",
       "max      14.150000     720.000000      248.980000    17.083333     300.000000   \n",
       "\n",
       "       away_xGA/60_pk  \n",
       "count     3262.000000  \n",
       "mean         7.049914  \n",
       "std          5.878934  \n",
       "min          0.000000  \n",
       "25%          3.572500  \n",
       "50%          5.950000  \n",
       "75%          9.140000  \n",
       "max         97.820000  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeling_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3e0e44e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "season_id         0\n",
       "Home_Team_Won     0\n",
       "home_pts_pct      0\n",
       "away_pts_pct      0\n",
       "home_TOI          0\n",
       "home_FF/60        0\n",
       "home_FA/60        0\n",
       "home_FF%          0\n",
       "home_GF/60        0\n",
       "home_GA/60        0\n",
       "home_GF%          0\n",
       "home_xGF/60       0\n",
       "home_xGA/60       0\n",
       "home_xGF%         0\n",
       "home_HDCF/60      0\n",
       "home_HDCA/60      0\n",
       "home_HDCF%        0\n",
       "home_HDSH%        0\n",
       "home_HDSV%        0\n",
       "home_SH%          0\n",
       "home_SV%          0\n",
       "home_PDO          0\n",
       "home_TOI_pp       0\n",
       "home_GF/60_pp     0\n",
       "home_xGF/60_pp    0\n",
       "home_TOI_pk       0\n",
       "home_GA/60_pk     0\n",
       "home_xGA/60_pk    0\n",
       "away_TOI          0\n",
       "away_FF/60        0\n",
       "away_FA/60        0\n",
       "away_FF%          0\n",
       "away_GF/60        0\n",
       "away_GA/60        0\n",
       "away_GF%          0\n",
       "away_xGF/60       0\n",
       "away_xGA/60       0\n",
       "away_xGF%         0\n",
       "away_HDCF/60      0\n",
       "away_HDCA/60      0\n",
       "away_HDCF%        0\n",
       "away_HDSH%        0\n",
       "away_HDSV%        0\n",
       "away_SH%          0\n",
       "away_SV%          0\n",
       "away_PDO          0\n",
       "away_TOI_pp       0\n",
       "away_GF/60_pp     0\n",
       "away_xGF/60_pp    0\n",
       "away_TOI_pk       0\n",
       "away_GA/60_pk     0\n",
       "away_xGA/60_pk    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for nans\n",
    "modeling_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8c4369",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefb1435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check correlation of selected features\n",
    "plt.figure(figsize=(20,10))\n",
    "cor = modeling_df.corr()\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224c3fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Table of heatmap values\n",
    "# modeling_df.corr().sort_values('Home_Team_Won') # ignore season_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b09a9cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Home_Team_Won</th>\n",
       "      <th>home_pts_pct</th>\n",
       "      <th>away_pts_pct</th>\n",
       "      <th>home_TOI</th>\n",
       "      <th>home_FF/60</th>\n",
       "      <th>home_FA/60</th>\n",
       "      <th>home_FF%</th>\n",
       "      <th>home_GF/60</th>\n",
       "      <th>home_GA/60</th>\n",
       "      <th>home_GF%</th>\n",
       "      <th>home_xGF/60</th>\n",
       "      <th>home_xGA/60</th>\n",
       "      <th>home_xGF%</th>\n",
       "      <th>home_HDCF/60</th>\n",
       "      <th>home_HDCA/60</th>\n",
       "      <th>home_HDCF%</th>\n",
       "      <th>home_HDSH%</th>\n",
       "      <th>home_HDSV%</th>\n",
       "      <th>home_SH%</th>\n",
       "      <th>home_SV%</th>\n",
       "      <th>home_PDO</th>\n",
       "      <th>home_TOI_pp</th>\n",
       "      <th>home_GF/60_pp</th>\n",
       "      <th>home_xGF/60_pp</th>\n",
       "      <th>home_TOI_pk</th>\n",
       "      <th>home_GA/60_pk</th>\n",
       "      <th>home_xGA/60_pk</th>\n",
       "      <th>away_TOI</th>\n",
       "      <th>away_FF/60</th>\n",
       "      <th>away_FA/60</th>\n",
       "      <th>away_FF%</th>\n",
       "      <th>away_GF/60</th>\n",
       "      <th>away_GA/60</th>\n",
       "      <th>away_GF%</th>\n",
       "      <th>away_xGF/60</th>\n",
       "      <th>away_xGA/60</th>\n",
       "      <th>away_xGF%</th>\n",
       "      <th>away_HDCF/60</th>\n",
       "      <th>away_HDCA/60</th>\n",
       "      <th>away_HDCF%</th>\n",
       "      <th>away_HDSH%</th>\n",
       "      <th>away_HDSV%</th>\n",
       "      <th>away_SH%</th>\n",
       "      <th>away_SV%</th>\n",
       "      <th>away_PDO</th>\n",
       "      <th>away_TOI_pp</th>\n",
       "      <th>away_GF/60_pp</th>\n",
       "      <th>away_xGF/60_pp</th>\n",
       "      <th>away_TOI_pk</th>\n",
       "      <th>away_GA/60_pk</th>\n",
       "      <th>away_xGA/60_pk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>away_GF%</th>\n",
       "      <td>-0.599544</td>\n",
       "      <td>-0.207479</td>\n",
       "      <td>0.196330</td>\n",
       "      <td>0.047650</td>\n",
       "      <td>-0.239146</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>-0.291547</td>\n",
       "      <td>-0.572845</td>\n",
       "      <td>0.631991</td>\n",
       "      <td>-0.899743</td>\n",
       "      <td>-0.253725</td>\n",
       "      <td>0.232837</td>\n",
       "      <td>-0.344472</td>\n",
       "      <td>-0.196734</td>\n",
       "      <td>0.214650</td>\n",
       "      <td>-0.294683</td>\n",
       "      <td>-0.345242</td>\n",
       "      <td>-0.361695</td>\n",
       "      <td>-0.526348</td>\n",
       "      <td>-0.590821</td>\n",
       "      <td>-0.791560</td>\n",
       "      <td>-0.003241</td>\n",
       "      <td>-0.003981</td>\n",
       "      <td>0.009349</td>\n",
       "      <td>-0.087827</td>\n",
       "      <td>-0.008636</td>\n",
       "      <td>-0.011899</td>\n",
       "      <td>0.047650</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>-0.239146</td>\n",
       "      <td>0.291547</td>\n",
       "      <td>0.631991</td>\n",
       "      <td>-0.572845</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.232837</td>\n",
       "      <td>-0.253725</td>\n",
       "      <td>0.344472</td>\n",
       "      <td>0.214650</td>\n",
       "      <td>-0.196734</td>\n",
       "      <td>0.294683</td>\n",
       "      <td>0.394039</td>\n",
       "      <td>0.307241</td>\n",
       "      <td>0.590821</td>\n",
       "      <td>0.526348</td>\n",
       "      <td>0.791560</td>\n",
       "      <td>-0.087827</td>\n",
       "      <td>-0.008636</td>\n",
       "      <td>-0.011899</td>\n",
       "      <td>-0.003241</td>\n",
       "      <td>-0.003981</td>\n",
       "      <td>0.009349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_PDO</th>\n",
       "      <td>-0.567670</td>\n",
       "      <td>-0.148274</td>\n",
       "      <td>0.173449</td>\n",
       "      <td>0.047804</td>\n",
       "      <td>0.001971</td>\n",
       "      <td>-0.019610</td>\n",
       "      <td>0.019134</td>\n",
       "      <td>-0.643301</td>\n",
       "      <td>0.638375</td>\n",
       "      <td>-0.792407</td>\n",
       "      <td>-0.086092</td>\n",
       "      <td>0.061702</td>\n",
       "      <td>-0.105405</td>\n",
       "      <td>-0.082468</td>\n",
       "      <td>0.083431</td>\n",
       "      <td>-0.121824</td>\n",
       "      <td>-0.445838</td>\n",
       "      <td>-0.419038</td>\n",
       "      <td>-0.704524</td>\n",
       "      <td>-0.707018</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.014956</td>\n",
       "      <td>0.009004</td>\n",
       "      <td>0.015955</td>\n",
       "      <td>-0.105642</td>\n",
       "      <td>-0.001989</td>\n",
       "      <td>-0.019569</td>\n",
       "      <td>0.047804</td>\n",
       "      <td>-0.019610</td>\n",
       "      <td>0.001971</td>\n",
       "      <td>-0.019134</td>\n",
       "      <td>0.638375</td>\n",
       "      <td>-0.643301</td>\n",
       "      <td>0.791560</td>\n",
       "      <td>0.061702</td>\n",
       "      <td>-0.086092</td>\n",
       "      <td>0.105405</td>\n",
       "      <td>0.083431</td>\n",
       "      <td>-0.082468</td>\n",
       "      <td>0.121824</td>\n",
       "      <td>0.437913</td>\n",
       "      <td>0.406268</td>\n",
       "      <td>0.707018</td>\n",
       "      <td>0.704524</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.105642</td>\n",
       "      <td>-0.001989</td>\n",
       "      <td>-0.019569</td>\n",
       "      <td>0.014956</td>\n",
       "      <td>0.009004</td>\n",
       "      <td>0.015955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_GA/60</th>\n",
       "      <td>-0.455510</td>\n",
       "      <td>-0.147103</td>\n",
       "      <td>0.149462</td>\n",
       "      <td>0.007840</td>\n",
       "      <td>-0.122315</td>\n",
       "      <td>0.269732</td>\n",
       "      <td>-0.240865</td>\n",
       "      <td>-0.031082</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.549068</td>\n",
       "      <td>-0.102017</td>\n",
       "      <td>0.323574</td>\n",
       "      <td>-0.293418</td>\n",
       "      <td>-0.080265</td>\n",
       "      <td>0.274380</td>\n",
       "      <td>-0.248288</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>-0.512497</td>\n",
       "      <td>0.004810</td>\n",
       "      <td>-0.904281</td>\n",
       "      <td>-0.638375</td>\n",
       "      <td>0.024202</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>-0.052826</td>\n",
       "      <td>-0.004895</td>\n",
       "      <td>-0.019836</td>\n",
       "      <td>0.007840</td>\n",
       "      <td>0.269732</td>\n",
       "      <td>-0.122315</td>\n",
       "      <td>0.240865</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.031082</td>\n",
       "      <td>0.631991</td>\n",
       "      <td>0.323574</td>\n",
       "      <td>-0.102017</td>\n",
       "      <td>0.293418</td>\n",
       "      <td>0.274380</td>\n",
       "      <td>-0.080265</td>\n",
       "      <td>0.248288</td>\n",
       "      <td>0.548959</td>\n",
       "      <td>-0.021792</td>\n",
       "      <td>0.904281</td>\n",
       "      <td>-0.004810</td>\n",
       "      <td>0.638375</td>\n",
       "      <td>-0.052826</td>\n",
       "      <td>-0.004895</td>\n",
       "      <td>-0.019836</td>\n",
       "      <td>0.024202</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.000566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_GF/60</th>\n",
       "      <td>-0.455510</td>\n",
       "      <td>-0.147103</td>\n",
       "      <td>0.149462</td>\n",
       "      <td>0.007840</td>\n",
       "      <td>-0.122315</td>\n",
       "      <td>0.269732</td>\n",
       "      <td>-0.240865</td>\n",
       "      <td>-0.031082</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.549068</td>\n",
       "      <td>-0.102017</td>\n",
       "      <td>0.323574</td>\n",
       "      <td>-0.293418</td>\n",
       "      <td>-0.080265</td>\n",
       "      <td>0.274380</td>\n",
       "      <td>-0.248288</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>-0.512497</td>\n",
       "      <td>0.004810</td>\n",
       "      <td>-0.904281</td>\n",
       "      <td>-0.638375</td>\n",
       "      <td>0.024202</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>-0.052826</td>\n",
       "      <td>-0.004895</td>\n",
       "      <td>-0.019836</td>\n",
       "      <td>0.007840</td>\n",
       "      <td>0.269732</td>\n",
       "      <td>-0.122315</td>\n",
       "      <td>0.240865</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.031082</td>\n",
       "      <td>0.631991</td>\n",
       "      <td>0.323574</td>\n",
       "      <td>-0.102017</td>\n",
       "      <td>0.293418</td>\n",
       "      <td>0.274380</td>\n",
       "      <td>-0.080265</td>\n",
       "      <td>0.248288</td>\n",
       "      <td>0.548959</td>\n",
       "      <td>-0.021792</td>\n",
       "      <td>0.904281</td>\n",
       "      <td>-0.004810</td>\n",
       "      <td>0.638375</td>\n",
       "      <td>-0.052826</td>\n",
       "      <td>-0.004895</td>\n",
       "      <td>-0.019836</td>\n",
       "      <td>0.024202</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.000566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_SV%</th>\n",
       "      <td>-0.410017</td>\n",
       "      <td>-0.117491</td>\n",
       "      <td>0.147067</td>\n",
       "      <td>0.042498</td>\n",
       "      <td>0.035409</td>\n",
       "      <td>0.016086</td>\n",
       "      <td>0.017470</td>\n",
       "      <td>-0.902947</td>\n",
       "      <td>-0.004810</td>\n",
       "      <td>-0.611950</td>\n",
       "      <td>-0.084883</td>\n",
       "      <td>0.007808</td>\n",
       "      <td>-0.066564</td>\n",
       "      <td>-0.089976</td>\n",
       "      <td>0.026361</td>\n",
       "      <td>-0.083623</td>\n",
       "      <td>-0.622819</td>\n",
       "      <td>-0.006435</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>-0.704524</td>\n",
       "      <td>0.003592</td>\n",
       "      <td>-0.003903</td>\n",
       "      <td>0.012812</td>\n",
       "      <td>-0.078463</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>-0.007086</td>\n",
       "      <td>0.042498</td>\n",
       "      <td>0.016086</td>\n",
       "      <td>0.035409</td>\n",
       "      <td>-0.017470</td>\n",
       "      <td>-0.004810</td>\n",
       "      <td>-0.902947</td>\n",
       "      <td>0.526348</td>\n",
       "      <td>0.007808</td>\n",
       "      <td>-0.084883</td>\n",
       "      <td>0.066564</td>\n",
       "      <td>0.026361</td>\n",
       "      <td>-0.089976</td>\n",
       "      <td>0.083623</td>\n",
       "      <td>0.002944</td>\n",
       "      <td>0.583800</td>\n",
       "      <td>-0.003759</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.704524</td>\n",
       "      <td>-0.078463</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>-0.007086</td>\n",
       "      <td>0.003592</td>\n",
       "      <td>-0.003903</td>\n",
       "      <td>0.012812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_SH%</th>\n",
       "      <td>-0.391314</td>\n",
       "      <td>-0.091787</td>\n",
       "      <td>0.097904</td>\n",
       "      <td>0.024981</td>\n",
       "      <td>-0.032620</td>\n",
       "      <td>-0.043672</td>\n",
       "      <td>0.009484</td>\n",
       "      <td>-0.006682</td>\n",
       "      <td>0.904281</td>\n",
       "      <td>-0.506695</td>\n",
       "      <td>-0.036802</td>\n",
       "      <td>0.079045</td>\n",
       "      <td>-0.082184</td>\n",
       "      <td>-0.026528</td>\n",
       "      <td>0.091234</td>\n",
       "      <td>-0.088283</td>\n",
       "      <td>-0.007515</td>\n",
       "      <td>-0.583998</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.707018</td>\n",
       "      <td>0.017539</td>\n",
       "      <td>0.016585</td>\n",
       "      <td>0.009624</td>\n",
       "      <td>-0.070599</td>\n",
       "      <td>-0.003235</td>\n",
       "      <td>-0.020478</td>\n",
       "      <td>0.024981</td>\n",
       "      <td>-0.043672</td>\n",
       "      <td>-0.032620</td>\n",
       "      <td>-0.009484</td>\n",
       "      <td>0.904281</td>\n",
       "      <td>-0.006682</td>\n",
       "      <td>0.590821</td>\n",
       "      <td>0.079045</td>\n",
       "      <td>-0.036802</td>\n",
       "      <td>0.082184</td>\n",
       "      <td>0.091234</td>\n",
       "      <td>-0.026528</td>\n",
       "      <td>0.088283</td>\n",
       "      <td>0.614051</td>\n",
       "      <td>-0.009304</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.003759</td>\n",
       "      <td>0.707018</td>\n",
       "      <td>-0.070599</td>\n",
       "      <td>-0.003235</td>\n",
       "      <td>-0.020478</td>\n",
       "      <td>0.017539</td>\n",
       "      <td>0.016585</td>\n",
       "      <td>0.009624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_pts_pct</th>\n",
       "      <td>-0.309084</td>\n",
       "      <td>-0.299474</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.015278</td>\n",
       "      <td>-0.166939</td>\n",
       "      <td>0.174660</td>\n",
       "      <td>-0.215557</td>\n",
       "      <td>-0.190705</td>\n",
       "      <td>0.149462</td>\n",
       "      <td>-0.199982</td>\n",
       "      <td>-0.135289</td>\n",
       "      <td>0.142496</td>\n",
       "      <td>-0.199874</td>\n",
       "      <td>-0.099663</td>\n",
       "      <td>0.112092</td>\n",
       "      <td>-0.148598</td>\n",
       "      <td>-0.102978</td>\n",
       "      <td>-0.034100</td>\n",
       "      <td>-0.147067</td>\n",
       "      <td>-0.097904</td>\n",
       "      <td>-0.173449</td>\n",
       "      <td>0.006442</td>\n",
       "      <td>-0.045646</td>\n",
       "      <td>-0.046846</td>\n",
       "      <td>-0.001900</td>\n",
       "      <td>0.056362</td>\n",
       "      <td>0.073780</td>\n",
       "      <td>-0.015278</td>\n",
       "      <td>0.174660</td>\n",
       "      <td>-0.166939</td>\n",
       "      <td>0.215557</td>\n",
       "      <td>0.149462</td>\n",
       "      <td>-0.190705</td>\n",
       "      <td>0.196330</td>\n",
       "      <td>0.142496</td>\n",
       "      <td>-0.135289</td>\n",
       "      <td>0.199874</td>\n",
       "      <td>0.112092</td>\n",
       "      <td>-0.099663</td>\n",
       "      <td>0.148598</td>\n",
       "      <td>0.044664</td>\n",
       "      <td>0.098652</td>\n",
       "      <td>0.097904</td>\n",
       "      <td>0.147067</td>\n",
       "      <td>0.173449</td>\n",
       "      <td>-0.001900</td>\n",
       "      <td>0.056362</td>\n",
       "      <td>0.073780</td>\n",
       "      <td>0.006442</td>\n",
       "      <td>-0.045646</td>\n",
       "      <td>-0.046846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_xGF%</th>\n",
       "      <td>-0.308746</td>\n",
       "      <td>-0.222432</td>\n",
       "      <td>0.199874</td>\n",
       "      <td>0.001079</td>\n",
       "      <td>-0.652191</td>\n",
       "      <td>0.671988</td>\n",
       "      <td>-0.833074</td>\n",
       "      <td>-0.275538</td>\n",
       "      <td>0.293418</td>\n",
       "      <td>-0.337718</td>\n",
       "      <td>-0.697641</td>\n",
       "      <td>0.719138</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.579216</td>\n",
       "      <td>0.588725</td>\n",
       "      <td>-0.830989</td>\n",
       "      <td>0.020997</td>\n",
       "      <td>0.050849</td>\n",
       "      <td>-0.066564</td>\n",
       "      <td>-0.082184</td>\n",
       "      <td>-0.105405</td>\n",
       "      <td>-0.027767</td>\n",
       "      <td>-0.004172</td>\n",
       "      <td>-0.004868</td>\n",
       "      <td>0.039837</td>\n",
       "      <td>-0.011859</td>\n",
       "      <td>0.019930</td>\n",
       "      <td>0.001079</td>\n",
       "      <td>0.671988</td>\n",
       "      <td>-0.652191</td>\n",
       "      <td>0.833074</td>\n",
       "      <td>0.293418</td>\n",
       "      <td>-0.275538</td>\n",
       "      <td>0.344472</td>\n",
       "      <td>0.719138</td>\n",
       "      <td>-0.697641</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.588725</td>\n",
       "      <td>-0.579216</td>\n",
       "      <td>0.830989</td>\n",
       "      <td>-0.011762</td>\n",
       "      <td>-0.055970</td>\n",
       "      <td>0.082184</td>\n",
       "      <td>0.066564</td>\n",
       "      <td>0.105405</td>\n",
       "      <td>0.039837</td>\n",
       "      <td>-0.011859</td>\n",
       "      <td>0.019930</td>\n",
       "      <td>-0.027767</td>\n",
       "      <td>-0.004172</td>\n",
       "      <td>-0.004868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_FF%</th>\n",
       "      <td>-0.275023</td>\n",
       "      <td>-0.245508</td>\n",
       "      <td>0.215557</td>\n",
       "      <td>0.005018</td>\n",
       "      <td>-0.794664</td>\n",
       "      <td>0.794296</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.235192</td>\n",
       "      <td>0.240865</td>\n",
       "      <td>-0.278097</td>\n",
       "      <td>-0.607087</td>\n",
       "      <td>0.583697</td>\n",
       "      <td>-0.833074</td>\n",
       "      <td>-0.457646</td>\n",
       "      <td>0.443593</td>\n",
       "      <td>-0.622474</td>\n",
       "      <td>0.047942</td>\n",
       "      <td>0.056099</td>\n",
       "      <td>0.017470</td>\n",
       "      <td>0.009484</td>\n",
       "      <td>0.019134</td>\n",
       "      <td>-0.030603</td>\n",
       "      <td>-0.010545</td>\n",
       "      <td>-0.026725</td>\n",
       "      <td>0.028449</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>0.035192</td>\n",
       "      <td>0.005018</td>\n",
       "      <td>0.794296</td>\n",
       "      <td>-0.794664</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.240865</td>\n",
       "      <td>-0.235192</td>\n",
       "      <td>0.291547</td>\n",
       "      <td>0.583697</td>\n",
       "      <td>-0.607087</td>\n",
       "      <td>0.833074</td>\n",
       "      <td>0.443593</td>\n",
       "      <td>-0.457646</td>\n",
       "      <td>0.622474</td>\n",
       "      <td>-0.023228</td>\n",
       "      <td>-0.065816</td>\n",
       "      <td>-0.009484</td>\n",
       "      <td>-0.017470</td>\n",
       "      <td>-0.019134</td>\n",
       "      <td>0.028449</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>0.035192</td>\n",
       "      <td>-0.030603</td>\n",
       "      <td>-0.010545</td>\n",
       "      <td>-0.026725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_HDCF%</th>\n",
       "      <td>-0.261105</td>\n",
       "      <td>-0.156697</td>\n",
       "      <td>0.148598</td>\n",
       "      <td>-0.004665</td>\n",
       "      <td>-0.485419</td>\n",
       "      <td>0.500645</td>\n",
       "      <td>-0.622474</td>\n",
       "      <td>-0.234563</td>\n",
       "      <td>0.248288</td>\n",
       "      <td>-0.290364</td>\n",
       "      <td>-0.565717</td>\n",
       "      <td>0.595313</td>\n",
       "      <td>-0.830989</td>\n",
       "      <td>-0.649944</td>\n",
       "      <td>0.684735</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.036509</td>\n",
       "      <td>0.074433</td>\n",
       "      <td>-0.083623</td>\n",
       "      <td>-0.088283</td>\n",
       "      <td>-0.121824</td>\n",
       "      <td>-0.013246</td>\n",
       "      <td>-0.002395</td>\n",
       "      <td>-0.002292</td>\n",
       "      <td>0.034001</td>\n",
       "      <td>-0.000671</td>\n",
       "      <td>0.020263</td>\n",
       "      <td>-0.004665</td>\n",
       "      <td>0.500645</td>\n",
       "      <td>-0.485419</td>\n",
       "      <td>0.622474</td>\n",
       "      <td>0.248288</td>\n",
       "      <td>-0.234563</td>\n",
       "      <td>0.294683</td>\n",
       "      <td>0.595313</td>\n",
       "      <td>-0.565717</td>\n",
       "      <td>0.830989</td>\n",
       "      <td>0.684735</td>\n",
       "      <td>-0.649944</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.019779</td>\n",
       "      <td>-0.093704</td>\n",
       "      <td>0.088283</td>\n",
       "      <td>0.083623</td>\n",
       "      <td>0.121824</td>\n",
       "      <td>0.034001</td>\n",
       "      <td>-0.000671</td>\n",
       "      <td>0.020263</td>\n",
       "      <td>-0.013246</td>\n",
       "      <td>-0.002395</td>\n",
       "      <td>-0.002292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_HDSH%</th>\n",
       "      <td>-0.248747</td>\n",
       "      <td>-0.045957</td>\n",
       "      <td>0.044664</td>\n",
       "      <td>0.034530</td>\n",
       "      <td>-0.005284</td>\n",
       "      <td>-0.036177</td>\n",
       "      <td>0.023228</td>\n",
       "      <td>-0.001716</td>\n",
       "      <td>0.548959</td>\n",
       "      <td>-0.334971</td>\n",
       "      <td>-0.029210</td>\n",
       "      <td>-0.037498</td>\n",
       "      <td>0.011762</td>\n",
       "      <td>-0.021528</td>\n",
       "      <td>-0.054165</td>\n",
       "      <td>0.019779</td>\n",
       "      <td>-0.010183</td>\n",
       "      <td>-0.944957</td>\n",
       "      <td>-0.002944</td>\n",
       "      <td>-0.614051</td>\n",
       "      <td>-0.437913</td>\n",
       "      <td>-0.011768</td>\n",
       "      <td>0.027728</td>\n",
       "      <td>0.003580</td>\n",
       "      <td>-0.055291</td>\n",
       "      <td>-0.006306</td>\n",
       "      <td>-0.015768</td>\n",
       "      <td>0.034530</td>\n",
       "      <td>-0.036177</td>\n",
       "      <td>-0.005284</td>\n",
       "      <td>-0.023228</td>\n",
       "      <td>0.548959</td>\n",
       "      <td>-0.001716</td>\n",
       "      <td>0.394039</td>\n",
       "      <td>-0.037498</td>\n",
       "      <td>-0.029210</td>\n",
       "      <td>-0.011762</td>\n",
       "      <td>-0.054165</td>\n",
       "      <td>-0.021528</td>\n",
       "      <td>-0.019779</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002607</td>\n",
       "      <td>0.614051</td>\n",
       "      <td>0.002944</td>\n",
       "      <td>0.437913</td>\n",
       "      <td>-0.055291</td>\n",
       "      <td>-0.006306</td>\n",
       "      <td>-0.015768</td>\n",
       "      <td>-0.011768</td>\n",
       "      <td>0.027728</td>\n",
       "      <td>0.003580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_HDSV%</th>\n",
       "      <td>-0.228224</td>\n",
       "      <td>-0.075734</td>\n",
       "      <td>0.098652</td>\n",
       "      <td>0.042804</td>\n",
       "      <td>0.071520</td>\n",
       "      <td>-0.026069</td>\n",
       "      <td>0.065816</td>\n",
       "      <td>-0.509909</td>\n",
       "      <td>-0.021792</td>\n",
       "      <td>-0.367419</td>\n",
       "      <td>0.071869</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.055970</td>\n",
       "      <td>0.121864</td>\n",
       "      <td>0.021349</td>\n",
       "      <td>0.093704</td>\n",
       "      <td>-0.922278</td>\n",
       "      <td>0.005707</td>\n",
       "      <td>-0.583800</td>\n",
       "      <td>0.009304</td>\n",
       "      <td>-0.406268</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.022214</td>\n",
       "      <td>0.010185</td>\n",
       "      <td>-0.079490</td>\n",
       "      <td>0.005527</td>\n",
       "      <td>0.008312</td>\n",
       "      <td>0.042804</td>\n",
       "      <td>-0.026069</td>\n",
       "      <td>0.071520</td>\n",
       "      <td>-0.065816</td>\n",
       "      <td>-0.021792</td>\n",
       "      <td>-0.509909</td>\n",
       "      <td>0.307241</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.071869</td>\n",
       "      <td>-0.055970</td>\n",
       "      <td>0.021349</td>\n",
       "      <td>0.121864</td>\n",
       "      <td>-0.093704</td>\n",
       "      <td>-0.002607</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.009304</td>\n",
       "      <td>0.583800</td>\n",
       "      <td>0.406268</td>\n",
       "      <td>-0.079490</td>\n",
       "      <td>0.005527</td>\n",
       "      <td>0.008312</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.022214</td>\n",
       "      <td>0.010185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_FA/60</th>\n",
       "      <td>-0.219077</td>\n",
       "      <td>-0.187624</td>\n",
       "      <td>0.174660</td>\n",
       "      <td>-0.041437</td>\n",
       "      <td>-0.286500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.794296</td>\n",
       "      <td>-0.106659</td>\n",
       "      <td>0.269732</td>\n",
       "      <td>-0.209723</td>\n",
       "      <td>-0.217017</td>\n",
       "      <td>0.755230</td>\n",
       "      <td>-0.671988</td>\n",
       "      <td>-0.165019</td>\n",
       "      <td>0.563438</td>\n",
       "      <td>-0.500645</td>\n",
       "      <td>0.015759</td>\n",
       "      <td>0.065495</td>\n",
       "      <td>-0.016086</td>\n",
       "      <td>0.043672</td>\n",
       "      <td>0.019610</td>\n",
       "      <td>-0.000832</td>\n",
       "      <td>-0.021837</td>\n",
       "      <td>-0.022549</td>\n",
       "      <td>0.057308</td>\n",
       "      <td>-0.010184</td>\n",
       "      <td>0.028537</td>\n",
       "      <td>-0.041437</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.286500</td>\n",
       "      <td>0.794296</td>\n",
       "      <td>0.269732</td>\n",
       "      <td>-0.106659</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.755230</td>\n",
       "      <td>-0.217017</td>\n",
       "      <td>0.671988</td>\n",
       "      <td>0.563438</td>\n",
       "      <td>-0.165019</td>\n",
       "      <td>0.500645</td>\n",
       "      <td>-0.036177</td>\n",
       "      <td>-0.026069</td>\n",
       "      <td>-0.043672</td>\n",
       "      <td>0.016086</td>\n",
       "      <td>-0.019610</td>\n",
       "      <td>0.057308</td>\n",
       "      <td>-0.010184</td>\n",
       "      <td>0.028537</td>\n",
       "      <td>-0.000832</td>\n",
       "      <td>-0.021837</td>\n",
       "      <td>-0.022549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_FF/60</th>\n",
       "      <td>-0.219077</td>\n",
       "      <td>-0.187624</td>\n",
       "      <td>0.174660</td>\n",
       "      <td>-0.041437</td>\n",
       "      <td>-0.286500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.794296</td>\n",
       "      <td>-0.106659</td>\n",
       "      <td>0.269732</td>\n",
       "      <td>-0.209723</td>\n",
       "      <td>-0.217017</td>\n",
       "      <td>0.755230</td>\n",
       "      <td>-0.671988</td>\n",
       "      <td>-0.165019</td>\n",
       "      <td>0.563438</td>\n",
       "      <td>-0.500645</td>\n",
       "      <td>0.015759</td>\n",
       "      <td>0.065495</td>\n",
       "      <td>-0.016086</td>\n",
       "      <td>0.043672</td>\n",
       "      <td>0.019610</td>\n",
       "      <td>-0.000832</td>\n",
       "      <td>-0.021837</td>\n",
       "      <td>-0.022549</td>\n",
       "      <td>0.057308</td>\n",
       "      <td>-0.010184</td>\n",
       "      <td>0.028537</td>\n",
       "      <td>-0.041437</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.286500</td>\n",
       "      <td>0.794296</td>\n",
       "      <td>0.269732</td>\n",
       "      <td>-0.106659</td>\n",
       "      <td>0.228527</td>\n",
       "      <td>0.755230</td>\n",
       "      <td>-0.217017</td>\n",
       "      <td>0.671988</td>\n",
       "      <td>0.563438</td>\n",
       "      <td>-0.165019</td>\n",
       "      <td>0.500645</td>\n",
       "      <td>-0.036177</td>\n",
       "      <td>-0.026069</td>\n",
       "      <td>-0.043672</td>\n",
       "      <td>0.016086</td>\n",
       "      <td>-0.019610</td>\n",
       "      <td>0.057308</td>\n",
       "      <td>-0.010184</td>\n",
       "      <td>0.028537</td>\n",
       "      <td>-0.000832</td>\n",
       "      <td>-0.021837</td>\n",
       "      <td>-0.022549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_xGF/60</th>\n",
       "      <td>-0.196675</td>\n",
       "      <td>-0.139928</td>\n",
       "      <td>0.142496</td>\n",
       "      <td>-0.052113</td>\n",
       "      <td>-0.191641</td>\n",
       "      <td>0.755230</td>\n",
       "      <td>-0.583697</td>\n",
       "      <td>-0.066878</td>\n",
       "      <td>0.323574</td>\n",
       "      <td>-0.215864</td>\n",
       "      <td>-0.055239</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.719138</td>\n",
       "      <td>-0.032229</td>\n",
       "      <td>0.831560</td>\n",
       "      <td>-0.595313</td>\n",
       "      <td>-0.007614</td>\n",
       "      <td>0.070361</td>\n",
       "      <td>-0.007808</td>\n",
       "      <td>-0.079045</td>\n",
       "      <td>-0.061702</td>\n",
       "      <td>0.005374</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.034806</td>\n",
       "      <td>0.070046</td>\n",
       "      <td>-0.022791</td>\n",
       "      <td>0.029531</td>\n",
       "      <td>-0.052113</td>\n",
       "      <td>0.755230</td>\n",
       "      <td>-0.191641</td>\n",
       "      <td>0.583697</td>\n",
       "      <td>0.323574</td>\n",
       "      <td>-0.066878</td>\n",
       "      <td>0.232837</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.055239</td>\n",
       "      <td>0.719138</td>\n",
       "      <td>0.831560</td>\n",
       "      <td>-0.032229</td>\n",
       "      <td>0.595313</td>\n",
       "      <td>-0.037498</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.079045</td>\n",
       "      <td>0.007808</td>\n",
       "      <td>0.061702</td>\n",
       "      <td>0.070046</td>\n",
       "      <td>-0.022791</td>\n",
       "      <td>0.029531</td>\n",
       "      <td>0.005374</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.034806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_xGA/60</th>\n",
       "      <td>-0.196675</td>\n",
       "      <td>-0.139928</td>\n",
       "      <td>0.142496</td>\n",
       "      <td>-0.052113</td>\n",
       "      <td>-0.191641</td>\n",
       "      <td>0.755230</td>\n",
       "      <td>-0.583697</td>\n",
       "      <td>-0.066878</td>\n",
       "      <td>0.323574</td>\n",
       "      <td>-0.215864</td>\n",
       "      <td>-0.055239</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.719138</td>\n",
       "      <td>-0.032229</td>\n",
       "      <td>0.831560</td>\n",
       "      <td>-0.595313</td>\n",
       "      <td>-0.007614</td>\n",
       "      <td>0.070361</td>\n",
       "      <td>-0.007808</td>\n",
       "      <td>-0.079045</td>\n",
       "      <td>-0.061702</td>\n",
       "      <td>0.005374</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.034806</td>\n",
       "      <td>0.070046</td>\n",
       "      <td>-0.022791</td>\n",
       "      <td>0.029531</td>\n",
       "      <td>-0.052113</td>\n",
       "      <td>0.755230</td>\n",
       "      <td>-0.191641</td>\n",
       "      <td>0.583697</td>\n",
       "      <td>0.323574</td>\n",
       "      <td>-0.066878</td>\n",
       "      <td>0.232837</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.055239</td>\n",
       "      <td>0.719138</td>\n",
       "      <td>0.831560</td>\n",
       "      <td>-0.032229</td>\n",
       "      <td>0.595313</td>\n",
       "      <td>-0.037498</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.079045</td>\n",
       "      <td>0.007808</td>\n",
       "      <td>0.061702</td>\n",
       "      <td>0.070046</td>\n",
       "      <td>-0.022791</td>\n",
       "      <td>0.029531</td>\n",
       "      <td>0.005374</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.034806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_HDCA/60</th>\n",
       "      <td>-0.163710</td>\n",
       "      <td>-0.090921</td>\n",
       "      <td>0.112092</td>\n",
       "      <td>-0.037430</td>\n",
       "      <td>-0.152971</td>\n",
       "      <td>0.563438</td>\n",
       "      <td>-0.443593</td>\n",
       "      <td>-0.065001</td>\n",
       "      <td>0.274380</td>\n",
       "      <td>-0.194106</td>\n",
       "      <td>-0.026902</td>\n",
       "      <td>0.831560</td>\n",
       "      <td>-0.588725</td>\n",
       "      <td>0.014210</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.684735</td>\n",
       "      <td>-0.020650</td>\n",
       "      <td>0.093310</td>\n",
       "      <td>-0.026361</td>\n",
       "      <td>-0.091234</td>\n",
       "      <td>-0.083431</td>\n",
       "      <td>0.010195</td>\n",
       "      <td>0.013524</td>\n",
       "      <td>0.038420</td>\n",
       "      <td>0.043808</td>\n",
       "      <td>-0.013031</td>\n",
       "      <td>0.021115</td>\n",
       "      <td>-0.037430</td>\n",
       "      <td>0.563438</td>\n",
       "      <td>-0.152971</td>\n",
       "      <td>0.443593</td>\n",
       "      <td>0.274380</td>\n",
       "      <td>-0.065001</td>\n",
       "      <td>0.214650</td>\n",
       "      <td>0.831560</td>\n",
       "      <td>-0.026902</td>\n",
       "      <td>0.588725</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014210</td>\n",
       "      <td>0.684735</td>\n",
       "      <td>-0.054165</td>\n",
       "      <td>0.021349</td>\n",
       "      <td>0.091234</td>\n",
       "      <td>0.026361</td>\n",
       "      <td>0.083431</td>\n",
       "      <td>0.043808</td>\n",
       "      <td>-0.013031</td>\n",
       "      <td>0.021115</td>\n",
       "      <td>0.010195</td>\n",
       "      <td>0.013524</td>\n",
       "      <td>0.038420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_HDCF/60</th>\n",
       "      <td>-0.163710</td>\n",
       "      <td>-0.090921</td>\n",
       "      <td>0.112092</td>\n",
       "      <td>-0.037430</td>\n",
       "      <td>-0.152971</td>\n",
       "      <td>0.563438</td>\n",
       "      <td>-0.443593</td>\n",
       "      <td>-0.065001</td>\n",
       "      <td>0.274380</td>\n",
       "      <td>-0.194106</td>\n",
       "      <td>-0.026902</td>\n",
       "      <td>0.831560</td>\n",
       "      <td>-0.588725</td>\n",
       "      <td>0.014210</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.684735</td>\n",
       "      <td>-0.020650</td>\n",
       "      <td>0.093310</td>\n",
       "      <td>-0.026361</td>\n",
       "      <td>-0.091234</td>\n",
       "      <td>-0.083431</td>\n",
       "      <td>0.010195</td>\n",
       "      <td>0.013524</td>\n",
       "      <td>0.038420</td>\n",
       "      <td>0.043808</td>\n",
       "      <td>-0.013031</td>\n",
       "      <td>0.021115</td>\n",
       "      <td>-0.037430</td>\n",
       "      <td>0.563438</td>\n",
       "      <td>-0.152971</td>\n",
       "      <td>0.443593</td>\n",
       "      <td>0.274380</td>\n",
       "      <td>-0.065001</td>\n",
       "      <td>0.214650</td>\n",
       "      <td>0.831560</td>\n",
       "      <td>-0.026902</td>\n",
       "      <td>0.588725</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014210</td>\n",
       "      <td>0.684735</td>\n",
       "      <td>-0.054165</td>\n",
       "      <td>0.021349</td>\n",
       "      <td>0.091234</td>\n",
       "      <td>0.026361</td>\n",
       "      <td>0.083431</td>\n",
       "      <td>0.043808</td>\n",
       "      <td>-0.013031</td>\n",
       "      <td>0.021115</td>\n",
       "      <td>0.010195</td>\n",
       "      <td>0.013524</td>\n",
       "      <td>0.038420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_GA/60_pk</th>\n",
       "      <td>-0.069168</td>\n",
       "      <td>-0.035844</td>\n",
       "      <td>0.056362</td>\n",
       "      <td>0.152379</td>\n",
       "      <td>-0.017723</td>\n",
       "      <td>-0.010184</td>\n",
       "      <td>-0.003071</td>\n",
       "      <td>-0.006728</td>\n",
       "      <td>-0.004895</td>\n",
       "      <td>0.013607</td>\n",
       "      <td>-0.009513</td>\n",
       "      <td>-0.022791</td>\n",
       "      <td>0.011859</td>\n",
       "      <td>-0.018943</td>\n",
       "      <td>-0.013031</td>\n",
       "      <td>0.000671</td>\n",
       "      <td>-0.001016</td>\n",
       "      <td>0.005220</td>\n",
       "      <td>-0.000561</td>\n",
       "      <td>0.003235</td>\n",
       "      <td>0.001989</td>\n",
       "      <td>-0.008089</td>\n",
       "      <td>-0.001370</td>\n",
       "      <td>-0.003956</td>\n",
       "      <td>-0.228760</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.644702</td>\n",
       "      <td>0.152379</td>\n",
       "      <td>-0.010184</td>\n",
       "      <td>-0.017723</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>-0.004895</td>\n",
       "      <td>-0.006728</td>\n",
       "      <td>-0.008636</td>\n",
       "      <td>-0.022791</td>\n",
       "      <td>-0.009513</td>\n",
       "      <td>-0.011859</td>\n",
       "      <td>-0.013031</td>\n",
       "      <td>-0.018943</td>\n",
       "      <td>-0.000671</td>\n",
       "      <td>-0.006306</td>\n",
       "      <td>0.005527</td>\n",
       "      <td>-0.003235</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>-0.001989</td>\n",
       "      <td>-0.228760</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.644702</td>\n",
       "      <td>-0.008089</td>\n",
       "      <td>-0.001370</td>\n",
       "      <td>-0.003956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_GF/60_pp</th>\n",
       "      <td>-0.069168</td>\n",
       "      <td>-0.035844</td>\n",
       "      <td>0.056362</td>\n",
       "      <td>0.152379</td>\n",
       "      <td>-0.017723</td>\n",
       "      <td>-0.010184</td>\n",
       "      <td>-0.003071</td>\n",
       "      <td>-0.006728</td>\n",
       "      <td>-0.004895</td>\n",
       "      <td>0.013607</td>\n",
       "      <td>-0.009513</td>\n",
       "      <td>-0.022791</td>\n",
       "      <td>0.011859</td>\n",
       "      <td>-0.018943</td>\n",
       "      <td>-0.013031</td>\n",
       "      <td>0.000671</td>\n",
       "      <td>-0.001016</td>\n",
       "      <td>0.005220</td>\n",
       "      <td>-0.000561</td>\n",
       "      <td>0.003235</td>\n",
       "      <td>0.001989</td>\n",
       "      <td>-0.008089</td>\n",
       "      <td>-0.001370</td>\n",
       "      <td>-0.003956</td>\n",
       "      <td>-0.228760</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.644702</td>\n",
       "      <td>0.152379</td>\n",
       "      <td>-0.010184</td>\n",
       "      <td>-0.017723</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>-0.004895</td>\n",
       "      <td>-0.006728</td>\n",
       "      <td>-0.008636</td>\n",
       "      <td>-0.022791</td>\n",
       "      <td>-0.009513</td>\n",
       "      <td>-0.011859</td>\n",
       "      <td>-0.013031</td>\n",
       "      <td>-0.018943</td>\n",
       "      <td>-0.000671</td>\n",
       "      <td>-0.006306</td>\n",
       "      <td>0.005527</td>\n",
       "      <td>-0.003235</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>-0.001989</td>\n",
       "      <td>-0.228760</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.644702</td>\n",
       "      <td>-0.008089</td>\n",
       "      <td>-0.001370</td>\n",
       "      <td>-0.003956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_xGF/60_pp</th>\n",
       "      <td>-0.045863</td>\n",
       "      <td>-0.054559</td>\n",
       "      <td>0.073780</td>\n",
       "      <td>0.058185</td>\n",
       "      <td>-0.032859</td>\n",
       "      <td>0.028537</td>\n",
       "      <td>-0.035192</td>\n",
       "      <td>-0.001534</td>\n",
       "      <td>-0.019836</td>\n",
       "      <td>0.016294</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.029531</td>\n",
       "      <td>-0.019930</td>\n",
       "      <td>-0.008252</td>\n",
       "      <td>0.021115</td>\n",
       "      <td>-0.020263</td>\n",
       "      <td>-0.006420</td>\n",
       "      <td>0.012595</td>\n",
       "      <td>0.007086</td>\n",
       "      <td>0.020478</td>\n",
       "      <td>0.019569</td>\n",
       "      <td>0.012004</td>\n",
       "      <td>0.025009</td>\n",
       "      <td>0.025557</td>\n",
       "      <td>-0.118314</td>\n",
       "      <td>0.644702</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.058185</td>\n",
       "      <td>0.028537</td>\n",
       "      <td>-0.032859</td>\n",
       "      <td>0.035192</td>\n",
       "      <td>-0.019836</td>\n",
       "      <td>-0.001534</td>\n",
       "      <td>-0.011899</td>\n",
       "      <td>0.029531</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.019930</td>\n",
       "      <td>0.021115</td>\n",
       "      <td>-0.008252</td>\n",
       "      <td>0.020263</td>\n",
       "      <td>-0.015768</td>\n",
       "      <td>0.008312</td>\n",
       "      <td>-0.020478</td>\n",
       "      <td>-0.007086</td>\n",
       "      <td>-0.019569</td>\n",
       "      <td>-0.118314</td>\n",
       "      <td>0.644702</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.012004</td>\n",
       "      <td>0.025009</td>\n",
       "      <td>0.025557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_xGA/60_pk</th>\n",
       "      <td>-0.045863</td>\n",
       "      <td>-0.054559</td>\n",
       "      <td>0.073780</td>\n",
       "      <td>0.058185</td>\n",
       "      <td>-0.032859</td>\n",
       "      <td>0.028537</td>\n",
       "      <td>-0.035192</td>\n",
       "      <td>-0.001534</td>\n",
       "      <td>-0.019836</td>\n",
       "      <td>0.016294</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.029531</td>\n",
       "      <td>-0.019930</td>\n",
       "      <td>-0.008252</td>\n",
       "      <td>0.021115</td>\n",
       "      <td>-0.020263</td>\n",
       "      <td>-0.006420</td>\n",
       "      <td>0.012595</td>\n",
       "      <td>0.007086</td>\n",
       "      <td>0.020478</td>\n",
       "      <td>0.019569</td>\n",
       "      <td>0.012004</td>\n",
       "      <td>0.025009</td>\n",
       "      <td>0.025557</td>\n",
       "      <td>-0.118314</td>\n",
       "      <td>0.644702</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.058185</td>\n",
       "      <td>0.028537</td>\n",
       "      <td>-0.032859</td>\n",
       "      <td>0.035192</td>\n",
       "      <td>-0.019836</td>\n",
       "      <td>-0.001534</td>\n",
       "      <td>-0.011899</td>\n",
       "      <td>0.029531</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.019930</td>\n",
       "      <td>0.021115</td>\n",
       "      <td>-0.008252</td>\n",
       "      <td>0.020263</td>\n",
       "      <td>-0.015768</td>\n",
       "      <td>0.008312</td>\n",
       "      <td>-0.020478</td>\n",
       "      <td>-0.007086</td>\n",
       "      <td>-0.019569</td>\n",
       "      <td>-0.118314</td>\n",
       "      <td>0.644702</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.012004</td>\n",
       "      <td>0.025009</td>\n",
       "      <td>0.025557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_TOI</th>\n",
       "      <td>-0.030639</td>\n",
       "      <td>-0.048542</td>\n",
       "      <td>-0.015278</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.055173</td>\n",
       "      <td>-0.041437</td>\n",
       "      <td>-0.005018</td>\n",
       "      <td>-0.053581</td>\n",
       "      <td>0.007840</td>\n",
       "      <td>-0.029788</td>\n",
       "      <td>-0.061678</td>\n",
       "      <td>-0.052113</td>\n",
       "      <td>-0.001079</td>\n",
       "      <td>-0.033917</td>\n",
       "      <td>-0.037430</td>\n",
       "      <td>0.004665</td>\n",
       "      <td>-0.033984</td>\n",
       "      <td>-0.024352</td>\n",
       "      <td>-0.042498</td>\n",
       "      <td>-0.024981</td>\n",
       "      <td>-0.047804</td>\n",
       "      <td>-0.691602</td>\n",
       "      <td>0.123190</td>\n",
       "      <td>0.040873</td>\n",
       "      <td>-0.660751</td>\n",
       "      <td>0.152379</td>\n",
       "      <td>0.058185</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.041437</td>\n",
       "      <td>-0.055173</td>\n",
       "      <td>0.005018</td>\n",
       "      <td>0.007840</td>\n",
       "      <td>-0.053581</td>\n",
       "      <td>0.047650</td>\n",
       "      <td>-0.052113</td>\n",
       "      <td>-0.061678</td>\n",
       "      <td>0.001079</td>\n",
       "      <td>-0.037430</td>\n",
       "      <td>-0.033917</td>\n",
       "      <td>-0.004665</td>\n",
       "      <td>0.034530</td>\n",
       "      <td>0.042804</td>\n",
       "      <td>0.024981</td>\n",
       "      <td>0.042498</td>\n",
       "      <td>0.047804</td>\n",
       "      <td>-0.660751</td>\n",
       "      <td>0.152379</td>\n",
       "      <td>0.058185</td>\n",
       "      <td>-0.691602</td>\n",
       "      <td>0.123190</td>\n",
       "      <td>0.040873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_TOI</th>\n",
       "      <td>-0.030639</td>\n",
       "      <td>-0.048542</td>\n",
       "      <td>-0.015278</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.055173</td>\n",
       "      <td>-0.041437</td>\n",
       "      <td>-0.005018</td>\n",
       "      <td>-0.053581</td>\n",
       "      <td>0.007840</td>\n",
       "      <td>-0.029788</td>\n",
       "      <td>-0.061678</td>\n",
       "      <td>-0.052113</td>\n",
       "      <td>-0.001079</td>\n",
       "      <td>-0.033917</td>\n",
       "      <td>-0.037430</td>\n",
       "      <td>0.004665</td>\n",
       "      <td>-0.033984</td>\n",
       "      <td>-0.024352</td>\n",
       "      <td>-0.042498</td>\n",
       "      <td>-0.024981</td>\n",
       "      <td>-0.047804</td>\n",
       "      <td>-0.691602</td>\n",
       "      <td>0.123190</td>\n",
       "      <td>0.040873</td>\n",
       "      <td>-0.660751</td>\n",
       "      <td>0.152379</td>\n",
       "      <td>0.058185</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.041437</td>\n",
       "      <td>-0.055173</td>\n",
       "      <td>0.005018</td>\n",
       "      <td>0.007840</td>\n",
       "      <td>-0.053581</td>\n",
       "      <td>0.047650</td>\n",
       "      <td>-0.052113</td>\n",
       "      <td>-0.061678</td>\n",
       "      <td>0.001079</td>\n",
       "      <td>-0.037430</td>\n",
       "      <td>-0.033917</td>\n",
       "      <td>-0.004665</td>\n",
       "      <td>0.034530</td>\n",
       "      <td>0.042804</td>\n",
       "      <td>0.024981</td>\n",
       "      <td>0.042498</td>\n",
       "      <td>0.047804</td>\n",
       "      <td>-0.660751</td>\n",
       "      <td>0.152379</td>\n",
       "      <td>0.058185</td>\n",
       "      <td>-0.691602</td>\n",
       "      <td>0.123190</td>\n",
       "      <td>0.040873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_TOI_pp</th>\n",
       "      <td>-0.026110</td>\n",
       "      <td>0.020268</td>\n",
       "      <td>0.006442</td>\n",
       "      <td>-0.691602</td>\n",
       "      <td>0.050635</td>\n",
       "      <td>-0.000832</td>\n",
       "      <td>0.030603</td>\n",
       "      <td>0.016105</td>\n",
       "      <td>0.024202</td>\n",
       "      <td>-0.018453</td>\n",
       "      <td>0.051535</td>\n",
       "      <td>0.005374</td>\n",
       "      <td>0.027767</td>\n",
       "      <td>0.025999</td>\n",
       "      <td>0.010195</td>\n",
       "      <td>0.013246</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>-0.000474</td>\n",
       "      <td>-0.003592</td>\n",
       "      <td>-0.017539</td>\n",
       "      <td>-0.014956</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.241019</td>\n",
       "      <td>-0.096287</td>\n",
       "      <td>0.130937</td>\n",
       "      <td>-0.008089</td>\n",
       "      <td>0.012004</td>\n",
       "      <td>-0.691602</td>\n",
       "      <td>-0.000832</td>\n",
       "      <td>0.050635</td>\n",
       "      <td>-0.030603</td>\n",
       "      <td>0.024202</td>\n",
       "      <td>0.016105</td>\n",
       "      <td>-0.003241</td>\n",
       "      <td>0.005374</td>\n",
       "      <td>0.051535</td>\n",
       "      <td>-0.027767</td>\n",
       "      <td>0.010195</td>\n",
       "      <td>0.025999</td>\n",
       "      <td>-0.013246</td>\n",
       "      <td>-0.011768</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.017539</td>\n",
       "      <td>0.003592</td>\n",
       "      <td>0.014956</td>\n",
       "      <td>0.130937</td>\n",
       "      <td>-0.008089</td>\n",
       "      <td>0.012004</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.241019</td>\n",
       "      <td>-0.096287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_TOI_pk</th>\n",
       "      <td>-0.026110</td>\n",
       "      <td>0.020268</td>\n",
       "      <td>0.006442</td>\n",
       "      <td>-0.691602</td>\n",
       "      <td>0.050635</td>\n",
       "      <td>-0.000832</td>\n",
       "      <td>0.030603</td>\n",
       "      <td>0.016105</td>\n",
       "      <td>0.024202</td>\n",
       "      <td>-0.018453</td>\n",
       "      <td>0.051535</td>\n",
       "      <td>0.005374</td>\n",
       "      <td>0.027767</td>\n",
       "      <td>0.025999</td>\n",
       "      <td>0.010195</td>\n",
       "      <td>0.013246</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>-0.000474</td>\n",
       "      <td>-0.003592</td>\n",
       "      <td>-0.017539</td>\n",
       "      <td>-0.014956</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.241019</td>\n",
       "      <td>-0.096287</td>\n",
       "      <td>0.130937</td>\n",
       "      <td>-0.008089</td>\n",
       "      <td>0.012004</td>\n",
       "      <td>-0.691602</td>\n",
       "      <td>-0.000832</td>\n",
       "      <td>0.050635</td>\n",
       "      <td>-0.030603</td>\n",
       "      <td>0.024202</td>\n",
       "      <td>0.016105</td>\n",
       "      <td>-0.003241</td>\n",
       "      <td>0.005374</td>\n",
       "      <td>0.051535</td>\n",
       "      <td>-0.027767</td>\n",
       "      <td>0.010195</td>\n",
       "      <td>0.025999</td>\n",
       "      <td>-0.013246</td>\n",
       "      <td>-0.011768</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.017539</td>\n",
       "      <td>0.003592</td>\n",
       "      <td>0.014956</td>\n",
       "      <td>0.130937</td>\n",
       "      <td>-0.008089</td>\n",
       "      <td>0.012004</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.241019</td>\n",
       "      <td>-0.096287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_TOI_pp</th>\n",
       "      <td>0.074819</td>\n",
       "      <td>0.050210</td>\n",
       "      <td>-0.001900</td>\n",
       "      <td>-0.660751</td>\n",
       "      <td>0.015539</td>\n",
       "      <td>0.057308</td>\n",
       "      <td>-0.028449</td>\n",
       "      <td>0.079296</td>\n",
       "      <td>-0.052826</td>\n",
       "      <td>0.076387</td>\n",
       "      <td>0.019201</td>\n",
       "      <td>0.070046</td>\n",
       "      <td>-0.039837</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.043808</td>\n",
       "      <td>-0.034001</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.052369</td>\n",
       "      <td>0.078463</td>\n",
       "      <td>0.070599</td>\n",
       "      <td>0.105642</td>\n",
       "      <td>0.130937</td>\n",
       "      <td>0.050402</td>\n",
       "      <td>0.027099</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.228760</td>\n",
       "      <td>-0.118314</td>\n",
       "      <td>-0.660751</td>\n",
       "      <td>0.057308</td>\n",
       "      <td>0.015539</td>\n",
       "      <td>0.028449</td>\n",
       "      <td>-0.052826</td>\n",
       "      <td>0.079296</td>\n",
       "      <td>-0.087827</td>\n",
       "      <td>0.070046</td>\n",
       "      <td>0.019201</td>\n",
       "      <td>0.039837</td>\n",
       "      <td>0.043808</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.034001</td>\n",
       "      <td>-0.055291</td>\n",
       "      <td>-0.079490</td>\n",
       "      <td>-0.070599</td>\n",
       "      <td>-0.078463</td>\n",
       "      <td>-0.105642</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.228760</td>\n",
       "      <td>-0.118314</td>\n",
       "      <td>0.130937</td>\n",
       "      <td>0.050402</td>\n",
       "      <td>0.027099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_TOI_pk</th>\n",
       "      <td>0.074819</td>\n",
       "      <td>0.050210</td>\n",
       "      <td>-0.001900</td>\n",
       "      <td>-0.660751</td>\n",
       "      <td>0.015539</td>\n",
       "      <td>0.057308</td>\n",
       "      <td>-0.028449</td>\n",
       "      <td>0.079296</td>\n",
       "      <td>-0.052826</td>\n",
       "      <td>0.076387</td>\n",
       "      <td>0.019201</td>\n",
       "      <td>0.070046</td>\n",
       "      <td>-0.039837</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.043808</td>\n",
       "      <td>-0.034001</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.052369</td>\n",
       "      <td>0.078463</td>\n",
       "      <td>0.070599</td>\n",
       "      <td>0.105642</td>\n",
       "      <td>0.130937</td>\n",
       "      <td>0.050402</td>\n",
       "      <td>0.027099</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.228760</td>\n",
       "      <td>-0.118314</td>\n",
       "      <td>-0.660751</td>\n",
       "      <td>0.057308</td>\n",
       "      <td>0.015539</td>\n",
       "      <td>0.028449</td>\n",
       "      <td>-0.052826</td>\n",
       "      <td>0.079296</td>\n",
       "      <td>-0.087827</td>\n",
       "      <td>0.070046</td>\n",
       "      <td>0.019201</td>\n",
       "      <td>0.039837</td>\n",
       "      <td>0.043808</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.034001</td>\n",
       "      <td>-0.055291</td>\n",
       "      <td>-0.079490</td>\n",
       "      <td>-0.070599</td>\n",
       "      <td>-0.078463</td>\n",
       "      <td>-0.105642</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.228760</td>\n",
       "      <td>-0.118314</td>\n",
       "      <td>0.130937</td>\n",
       "      <td>0.050402</td>\n",
       "      <td>0.027099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_xGA/60_pk</th>\n",
       "      <td>0.083060</td>\n",
       "      <td>0.061477</td>\n",
       "      <td>-0.046846</td>\n",
       "      <td>0.040873</td>\n",
       "      <td>0.023092</td>\n",
       "      <td>-0.022549</td>\n",
       "      <td>0.026725</td>\n",
       "      <td>-0.018239</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.042165</td>\n",
       "      <td>0.034806</td>\n",
       "      <td>0.004868</td>\n",
       "      <td>0.034069</td>\n",
       "      <td>0.038420</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>-0.007027</td>\n",
       "      <td>-0.003153</td>\n",
       "      <td>-0.012812</td>\n",
       "      <td>-0.009624</td>\n",
       "      <td>-0.015955</td>\n",
       "      <td>-0.096287</td>\n",
       "      <td>0.482805</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027099</td>\n",
       "      <td>-0.003956</td>\n",
       "      <td>0.025557</td>\n",
       "      <td>0.040873</td>\n",
       "      <td>-0.022549</td>\n",
       "      <td>0.023092</td>\n",
       "      <td>-0.026725</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>-0.018239</td>\n",
       "      <td>0.009349</td>\n",
       "      <td>0.034806</td>\n",
       "      <td>0.042165</td>\n",
       "      <td>-0.004868</td>\n",
       "      <td>0.038420</td>\n",
       "      <td>0.034069</td>\n",
       "      <td>-0.002292</td>\n",
       "      <td>0.003580</td>\n",
       "      <td>0.010185</td>\n",
       "      <td>0.009624</td>\n",
       "      <td>0.012812</td>\n",
       "      <td>0.015955</td>\n",
       "      <td>0.027099</td>\n",
       "      <td>-0.003956</td>\n",
       "      <td>0.025557</td>\n",
       "      <td>-0.096287</td>\n",
       "      <td>0.482805</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_xGF/60_pp</th>\n",
       "      <td>0.083060</td>\n",
       "      <td>0.061477</td>\n",
       "      <td>-0.046846</td>\n",
       "      <td>0.040873</td>\n",
       "      <td>0.023092</td>\n",
       "      <td>-0.022549</td>\n",
       "      <td>0.026725</td>\n",
       "      <td>-0.018239</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.042165</td>\n",
       "      <td>0.034806</td>\n",
       "      <td>0.004868</td>\n",
       "      <td>0.034069</td>\n",
       "      <td>0.038420</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>-0.007027</td>\n",
       "      <td>-0.003153</td>\n",
       "      <td>-0.012812</td>\n",
       "      <td>-0.009624</td>\n",
       "      <td>-0.015955</td>\n",
       "      <td>-0.096287</td>\n",
       "      <td>0.482805</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027099</td>\n",
       "      <td>-0.003956</td>\n",
       "      <td>0.025557</td>\n",
       "      <td>0.040873</td>\n",
       "      <td>-0.022549</td>\n",
       "      <td>0.023092</td>\n",
       "      <td>-0.026725</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>-0.018239</td>\n",
       "      <td>0.009349</td>\n",
       "      <td>0.034806</td>\n",
       "      <td>0.042165</td>\n",
       "      <td>-0.004868</td>\n",
       "      <td>0.038420</td>\n",
       "      <td>0.034069</td>\n",
       "      <td>-0.002292</td>\n",
       "      <td>0.003580</td>\n",
       "      <td>0.010185</td>\n",
       "      <td>0.009624</td>\n",
       "      <td>0.012812</td>\n",
       "      <td>0.015955</td>\n",
       "      <td>0.027099</td>\n",
       "      <td>-0.003956</td>\n",
       "      <td>0.025557</td>\n",
       "      <td>-0.096287</td>\n",
       "      <td>0.482805</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_GF/60_pp</th>\n",
       "      <td>0.165618</td>\n",
       "      <td>0.076586</td>\n",
       "      <td>-0.045646</td>\n",
       "      <td>0.123190</td>\n",
       "      <td>-0.002858</td>\n",
       "      <td>-0.021837</td>\n",
       "      <td>0.010545</td>\n",
       "      <td>-0.009721</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.015144</td>\n",
       "      <td>0.005702</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.004172</td>\n",
       "      <td>0.009156</td>\n",
       "      <td>0.013524</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>-0.020554</td>\n",
       "      <td>-0.027889</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>-0.016585</td>\n",
       "      <td>-0.009004</td>\n",
       "      <td>-0.241019</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.482805</td>\n",
       "      <td>0.050402</td>\n",
       "      <td>-0.001370</td>\n",
       "      <td>0.025009</td>\n",
       "      <td>0.123190</td>\n",
       "      <td>-0.021837</td>\n",
       "      <td>-0.002858</td>\n",
       "      <td>-0.010545</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>-0.009721</td>\n",
       "      <td>-0.003981</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.005702</td>\n",
       "      <td>-0.004172</td>\n",
       "      <td>0.013524</td>\n",
       "      <td>0.009156</td>\n",
       "      <td>-0.002395</td>\n",
       "      <td>0.027728</td>\n",
       "      <td>0.022214</td>\n",
       "      <td>0.016585</td>\n",
       "      <td>-0.003903</td>\n",
       "      <td>0.009004</td>\n",
       "      <td>0.050402</td>\n",
       "      <td>-0.001370</td>\n",
       "      <td>0.025009</td>\n",
       "      <td>-0.241019</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.482805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_GA/60_pk</th>\n",
       "      <td>0.165618</td>\n",
       "      <td>0.076586</td>\n",
       "      <td>-0.045646</td>\n",
       "      <td>0.123190</td>\n",
       "      <td>-0.002858</td>\n",
       "      <td>-0.021837</td>\n",
       "      <td>0.010545</td>\n",
       "      <td>-0.009721</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.015144</td>\n",
       "      <td>0.005702</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.004172</td>\n",
       "      <td>0.009156</td>\n",
       "      <td>0.013524</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>-0.020554</td>\n",
       "      <td>-0.027889</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>-0.016585</td>\n",
       "      <td>-0.009004</td>\n",
       "      <td>-0.241019</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.482805</td>\n",
       "      <td>0.050402</td>\n",
       "      <td>-0.001370</td>\n",
       "      <td>0.025009</td>\n",
       "      <td>0.123190</td>\n",
       "      <td>-0.021837</td>\n",
       "      <td>-0.002858</td>\n",
       "      <td>-0.010545</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>-0.009721</td>\n",
       "      <td>-0.003981</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.005702</td>\n",
       "      <td>-0.004172</td>\n",
       "      <td>0.013524</td>\n",
       "      <td>0.009156</td>\n",
       "      <td>-0.002395</td>\n",
       "      <td>0.027728</td>\n",
       "      <td>0.022214</td>\n",
       "      <td>0.016585</td>\n",
       "      <td>-0.003903</td>\n",
       "      <td>0.009004</td>\n",
       "      <td>0.050402</td>\n",
       "      <td>-0.001370</td>\n",
       "      <td>0.025009</td>\n",
       "      <td>-0.241019</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.482805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_HDCF/60</th>\n",
       "      <td>0.214950</td>\n",
       "      <td>0.148468</td>\n",
       "      <td>-0.099663</td>\n",
       "      <td>-0.033917</td>\n",
       "      <td>0.578480</td>\n",
       "      <td>-0.165019</td>\n",
       "      <td>0.457646</td>\n",
       "      <td>0.281645</td>\n",
       "      <td>-0.080265</td>\n",
       "      <td>0.215166</td>\n",
       "      <td>0.835989</td>\n",
       "      <td>-0.032229</td>\n",
       "      <td>0.579216</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014210</td>\n",
       "      <td>0.649944</td>\n",
       "      <td>-0.070292</td>\n",
       "      <td>0.018593</td>\n",
       "      <td>0.089976</td>\n",
       "      <td>0.026528</td>\n",
       "      <td>0.082468</td>\n",
       "      <td>0.025999</td>\n",
       "      <td>0.009156</td>\n",
       "      <td>0.034069</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>-0.018943</td>\n",
       "      <td>-0.008252</td>\n",
       "      <td>-0.033917</td>\n",
       "      <td>-0.165019</td>\n",
       "      <td>0.578480</td>\n",
       "      <td>-0.457646</td>\n",
       "      <td>-0.080265</td>\n",
       "      <td>0.281645</td>\n",
       "      <td>-0.196734</td>\n",
       "      <td>-0.032229</td>\n",
       "      <td>0.835989</td>\n",
       "      <td>-0.579216</td>\n",
       "      <td>0.014210</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.649944</td>\n",
       "      <td>-0.021528</td>\n",
       "      <td>0.121864</td>\n",
       "      <td>-0.026528</td>\n",
       "      <td>-0.089976</td>\n",
       "      <td>-0.082468</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>-0.018943</td>\n",
       "      <td>-0.008252</td>\n",
       "      <td>0.025999</td>\n",
       "      <td>0.009156</td>\n",
       "      <td>0.034069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_HDCA/60</th>\n",
       "      <td>0.214950</td>\n",
       "      <td>0.148468</td>\n",
       "      <td>-0.099663</td>\n",
       "      <td>-0.033917</td>\n",
       "      <td>0.578480</td>\n",
       "      <td>-0.165019</td>\n",
       "      <td>0.457646</td>\n",
       "      <td>0.281645</td>\n",
       "      <td>-0.080265</td>\n",
       "      <td>0.215166</td>\n",
       "      <td>0.835989</td>\n",
       "      <td>-0.032229</td>\n",
       "      <td>0.579216</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014210</td>\n",
       "      <td>0.649944</td>\n",
       "      <td>-0.070292</td>\n",
       "      <td>0.018593</td>\n",
       "      <td>0.089976</td>\n",
       "      <td>0.026528</td>\n",
       "      <td>0.082468</td>\n",
       "      <td>0.025999</td>\n",
       "      <td>0.009156</td>\n",
       "      <td>0.034069</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>-0.018943</td>\n",
       "      <td>-0.008252</td>\n",
       "      <td>-0.033917</td>\n",
       "      <td>-0.165019</td>\n",
       "      <td>0.578480</td>\n",
       "      <td>-0.457646</td>\n",
       "      <td>-0.080265</td>\n",
       "      <td>0.281645</td>\n",
       "      <td>-0.196734</td>\n",
       "      <td>-0.032229</td>\n",
       "      <td>0.835989</td>\n",
       "      <td>-0.579216</td>\n",
       "      <td>0.014210</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.649944</td>\n",
       "      <td>-0.021528</td>\n",
       "      <td>0.121864</td>\n",
       "      <td>-0.026528</td>\n",
       "      <td>-0.089976</td>\n",
       "      <td>-0.082468</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>-0.018943</td>\n",
       "      <td>-0.008252</td>\n",
       "      <td>0.025999</td>\n",
       "      <td>0.009156</td>\n",
       "      <td>0.034069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_HDSV%</th>\n",
       "      <td>0.224793</td>\n",
       "      <td>0.040503</td>\n",
       "      <td>-0.034100</td>\n",
       "      <td>-0.024352</td>\n",
       "      <td>-0.015662</td>\n",
       "      <td>0.065495</td>\n",
       "      <td>-0.056099</td>\n",
       "      <td>-0.001518</td>\n",
       "      <td>-0.512497</td>\n",
       "      <td>0.309210</td>\n",
       "      <td>0.015850</td>\n",
       "      <td>0.070361</td>\n",
       "      <td>-0.050849</td>\n",
       "      <td>0.018593</td>\n",
       "      <td>0.093310</td>\n",
       "      <td>-0.074433</td>\n",
       "      <td>0.005884</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006435</td>\n",
       "      <td>0.583998</td>\n",
       "      <td>0.419038</td>\n",
       "      <td>-0.000474</td>\n",
       "      <td>-0.027889</td>\n",
       "      <td>-0.003153</td>\n",
       "      <td>0.052369</td>\n",
       "      <td>0.005220</td>\n",
       "      <td>0.012595</td>\n",
       "      <td>-0.024352</td>\n",
       "      <td>0.065495</td>\n",
       "      <td>-0.015662</td>\n",
       "      <td>0.056099</td>\n",
       "      <td>-0.512497</td>\n",
       "      <td>-0.001518</td>\n",
       "      <td>-0.361695</td>\n",
       "      <td>0.070361</td>\n",
       "      <td>0.015850</td>\n",
       "      <td>0.050849</td>\n",
       "      <td>0.093310</td>\n",
       "      <td>0.018593</td>\n",
       "      <td>0.074433</td>\n",
       "      <td>-0.944957</td>\n",
       "      <td>0.005707</td>\n",
       "      <td>-0.583998</td>\n",
       "      <td>-0.006435</td>\n",
       "      <td>-0.419038</td>\n",
       "      <td>0.052369</td>\n",
       "      <td>0.005220</td>\n",
       "      <td>0.012595</td>\n",
       "      <td>-0.000474</td>\n",
       "      <td>-0.027889</td>\n",
       "      <td>-0.003153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_FA/60</th>\n",
       "      <td>0.227636</td>\n",
       "      <td>0.212247</td>\n",
       "      <td>-0.166939</td>\n",
       "      <td>-0.055173</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.286500</td>\n",
       "      <td>0.794664</td>\n",
       "      <td>0.279004</td>\n",
       "      <td>-0.122315</td>\n",
       "      <td>0.236749</td>\n",
       "      <td>0.770039</td>\n",
       "      <td>-0.191641</td>\n",
       "      <td>0.652191</td>\n",
       "      <td>0.578480</td>\n",
       "      <td>-0.152971</td>\n",
       "      <td>0.485419</td>\n",
       "      <td>-0.054678</td>\n",
       "      <td>-0.015662</td>\n",
       "      <td>-0.035409</td>\n",
       "      <td>0.032620</td>\n",
       "      <td>-0.001971</td>\n",
       "      <td>0.050635</td>\n",
       "      <td>-0.002858</td>\n",
       "      <td>0.023092</td>\n",
       "      <td>0.015539</td>\n",
       "      <td>-0.017723</td>\n",
       "      <td>-0.032859</td>\n",
       "      <td>-0.055173</td>\n",
       "      <td>-0.286500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.794664</td>\n",
       "      <td>-0.122315</td>\n",
       "      <td>0.279004</td>\n",
       "      <td>-0.239146</td>\n",
       "      <td>-0.191641</td>\n",
       "      <td>0.770039</td>\n",
       "      <td>-0.652191</td>\n",
       "      <td>-0.152971</td>\n",
       "      <td>0.578480</td>\n",
       "      <td>-0.485419</td>\n",
       "      <td>-0.005284</td>\n",
       "      <td>0.071520</td>\n",
       "      <td>-0.032620</td>\n",
       "      <td>0.035409</td>\n",
       "      <td>0.001971</td>\n",
       "      <td>0.015539</td>\n",
       "      <td>-0.017723</td>\n",
       "      <td>-0.032859</td>\n",
       "      <td>0.050635</td>\n",
       "      <td>-0.002858</td>\n",
       "      <td>0.023092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_FF/60</th>\n",
       "      <td>0.227636</td>\n",
       "      <td>0.212247</td>\n",
       "      <td>-0.166939</td>\n",
       "      <td>-0.055173</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.286500</td>\n",
       "      <td>0.794664</td>\n",
       "      <td>0.279004</td>\n",
       "      <td>-0.122315</td>\n",
       "      <td>0.236749</td>\n",
       "      <td>0.770039</td>\n",
       "      <td>-0.191641</td>\n",
       "      <td>0.652191</td>\n",
       "      <td>0.578480</td>\n",
       "      <td>-0.152971</td>\n",
       "      <td>0.485419</td>\n",
       "      <td>-0.054678</td>\n",
       "      <td>-0.015662</td>\n",
       "      <td>-0.035409</td>\n",
       "      <td>0.032620</td>\n",
       "      <td>-0.001971</td>\n",
       "      <td>0.050635</td>\n",
       "      <td>-0.002858</td>\n",
       "      <td>0.023092</td>\n",
       "      <td>0.015539</td>\n",
       "      <td>-0.017723</td>\n",
       "      <td>-0.032859</td>\n",
       "      <td>-0.055173</td>\n",
       "      <td>-0.286500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.794664</td>\n",
       "      <td>-0.122315</td>\n",
       "      <td>0.279004</td>\n",
       "      <td>-0.239146</td>\n",
       "      <td>-0.191641</td>\n",
       "      <td>0.770039</td>\n",
       "      <td>-0.652191</td>\n",
       "      <td>-0.152971</td>\n",
       "      <td>0.578480</td>\n",
       "      <td>-0.485419</td>\n",
       "      <td>-0.005284</td>\n",
       "      <td>0.071520</td>\n",
       "      <td>-0.032620</td>\n",
       "      <td>0.035409</td>\n",
       "      <td>0.001971</td>\n",
       "      <td>0.015539</td>\n",
       "      <td>-0.017723</td>\n",
       "      <td>-0.032859</td>\n",
       "      <td>0.050635</td>\n",
       "      <td>-0.002858</td>\n",
       "      <td>0.023092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_xGF/60</th>\n",
       "      <td>0.250745</td>\n",
       "      <td>0.185113</td>\n",
       "      <td>-0.135289</td>\n",
       "      <td>-0.061678</td>\n",
       "      <td>0.770039</td>\n",
       "      <td>-0.217017</td>\n",
       "      <td>0.607087</td>\n",
       "      <td>0.338300</td>\n",
       "      <td>-0.102017</td>\n",
       "      <td>0.262857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.055239</td>\n",
       "      <td>0.697641</td>\n",
       "      <td>0.835989</td>\n",
       "      <td>-0.026902</td>\n",
       "      <td>0.565717</td>\n",
       "      <td>-0.036473</td>\n",
       "      <td>0.015850</td>\n",
       "      <td>0.084883</td>\n",
       "      <td>0.036802</td>\n",
       "      <td>0.086092</td>\n",
       "      <td>0.051535</td>\n",
       "      <td>0.005702</td>\n",
       "      <td>0.042165</td>\n",
       "      <td>0.019201</td>\n",
       "      <td>-0.009513</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>-0.061678</td>\n",
       "      <td>-0.217017</td>\n",
       "      <td>0.770039</td>\n",
       "      <td>-0.607087</td>\n",
       "      <td>-0.102017</td>\n",
       "      <td>0.338300</td>\n",
       "      <td>-0.253725</td>\n",
       "      <td>-0.055239</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.697641</td>\n",
       "      <td>-0.026902</td>\n",
       "      <td>0.835989</td>\n",
       "      <td>-0.565717</td>\n",
       "      <td>-0.029210</td>\n",
       "      <td>0.071869</td>\n",
       "      <td>-0.036802</td>\n",
       "      <td>-0.084883</td>\n",
       "      <td>-0.086092</td>\n",
       "      <td>0.019201</td>\n",
       "      <td>-0.009513</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.051535</td>\n",
       "      <td>0.005702</td>\n",
       "      <td>0.042165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_xGA/60</th>\n",
       "      <td>0.250745</td>\n",
       "      <td>0.185113</td>\n",
       "      <td>-0.135289</td>\n",
       "      <td>-0.061678</td>\n",
       "      <td>0.770039</td>\n",
       "      <td>-0.217017</td>\n",
       "      <td>0.607087</td>\n",
       "      <td>0.338300</td>\n",
       "      <td>-0.102017</td>\n",
       "      <td>0.262857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.055239</td>\n",
       "      <td>0.697641</td>\n",
       "      <td>0.835989</td>\n",
       "      <td>-0.026902</td>\n",
       "      <td>0.565717</td>\n",
       "      <td>-0.036473</td>\n",
       "      <td>0.015850</td>\n",
       "      <td>0.084883</td>\n",
       "      <td>0.036802</td>\n",
       "      <td>0.086092</td>\n",
       "      <td>0.051535</td>\n",
       "      <td>0.005702</td>\n",
       "      <td>0.042165</td>\n",
       "      <td>0.019201</td>\n",
       "      <td>-0.009513</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>-0.061678</td>\n",
       "      <td>-0.217017</td>\n",
       "      <td>0.770039</td>\n",
       "      <td>-0.607087</td>\n",
       "      <td>-0.102017</td>\n",
       "      <td>0.338300</td>\n",
       "      <td>-0.253725</td>\n",
       "      <td>-0.055239</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.697641</td>\n",
       "      <td>-0.026902</td>\n",
       "      <td>0.835989</td>\n",
       "      <td>-0.565717</td>\n",
       "      <td>-0.029210</td>\n",
       "      <td>0.071869</td>\n",
       "      <td>-0.036802</td>\n",
       "      <td>-0.084883</td>\n",
       "      <td>-0.086092</td>\n",
       "      <td>0.019201</td>\n",
       "      <td>-0.009513</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.051535</td>\n",
       "      <td>0.005702</td>\n",
       "      <td>0.042165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_HDSH%</th>\n",
       "      <td>0.257639</td>\n",
       "      <td>0.085666</td>\n",
       "      <td>-0.102978</td>\n",
       "      <td>-0.033984</td>\n",
       "      <td>-0.054678</td>\n",
       "      <td>0.015759</td>\n",
       "      <td>-0.047942</td>\n",
       "      <td>0.550207</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.406797</td>\n",
       "      <td>-0.036473</td>\n",
       "      <td>-0.007614</td>\n",
       "      <td>-0.020997</td>\n",
       "      <td>-0.070292</td>\n",
       "      <td>-0.020650</td>\n",
       "      <td>-0.036509</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005884</td>\n",
       "      <td>0.622819</td>\n",
       "      <td>0.007515</td>\n",
       "      <td>0.445838</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>-0.020554</td>\n",
       "      <td>-0.007027</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>-0.001016</td>\n",
       "      <td>-0.006420</td>\n",
       "      <td>-0.033984</td>\n",
       "      <td>0.015759</td>\n",
       "      <td>-0.054678</td>\n",
       "      <td>0.047942</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.550207</td>\n",
       "      <td>-0.345242</td>\n",
       "      <td>-0.007614</td>\n",
       "      <td>-0.036473</td>\n",
       "      <td>0.020997</td>\n",
       "      <td>-0.020650</td>\n",
       "      <td>-0.070292</td>\n",
       "      <td>0.036509</td>\n",
       "      <td>-0.010183</td>\n",
       "      <td>-0.922278</td>\n",
       "      <td>-0.007515</td>\n",
       "      <td>-0.622819</td>\n",
       "      <td>-0.445838</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>-0.001016</td>\n",
       "      <td>-0.006420</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>-0.020554</td>\n",
       "      <td>-0.007027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_HDCF%</th>\n",
       "      <td>0.261105</td>\n",
       "      <td>0.156697</td>\n",
       "      <td>-0.148598</td>\n",
       "      <td>0.004665</td>\n",
       "      <td>0.485419</td>\n",
       "      <td>-0.500645</td>\n",
       "      <td>0.622474</td>\n",
       "      <td>0.234563</td>\n",
       "      <td>-0.248288</td>\n",
       "      <td>0.290364</td>\n",
       "      <td>0.565717</td>\n",
       "      <td>-0.595313</td>\n",
       "      <td>0.830989</td>\n",
       "      <td>0.649944</td>\n",
       "      <td>-0.684735</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.036509</td>\n",
       "      <td>-0.074433</td>\n",
       "      <td>0.083623</td>\n",
       "      <td>0.088283</td>\n",
       "      <td>0.121824</td>\n",
       "      <td>0.013246</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>-0.034001</td>\n",
       "      <td>0.000671</td>\n",
       "      <td>-0.020263</td>\n",
       "      <td>0.004665</td>\n",
       "      <td>-0.500645</td>\n",
       "      <td>0.485419</td>\n",
       "      <td>-0.622474</td>\n",
       "      <td>-0.248288</td>\n",
       "      <td>0.234563</td>\n",
       "      <td>-0.294683</td>\n",
       "      <td>-0.595313</td>\n",
       "      <td>0.565717</td>\n",
       "      <td>-0.830989</td>\n",
       "      <td>-0.684735</td>\n",
       "      <td>0.649944</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.019779</td>\n",
       "      <td>0.093704</td>\n",
       "      <td>-0.088283</td>\n",
       "      <td>-0.083623</td>\n",
       "      <td>-0.121824</td>\n",
       "      <td>-0.034001</td>\n",
       "      <td>0.000671</td>\n",
       "      <td>-0.020263</td>\n",
       "      <td>0.013246</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.002292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_FF%</th>\n",
       "      <td>0.275023</td>\n",
       "      <td>0.245508</td>\n",
       "      <td>-0.215557</td>\n",
       "      <td>-0.005018</td>\n",
       "      <td>0.794664</td>\n",
       "      <td>-0.794296</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.235192</td>\n",
       "      <td>-0.240865</td>\n",
       "      <td>0.278097</td>\n",
       "      <td>0.607087</td>\n",
       "      <td>-0.583697</td>\n",
       "      <td>0.833074</td>\n",
       "      <td>0.457646</td>\n",
       "      <td>-0.443593</td>\n",
       "      <td>0.622474</td>\n",
       "      <td>-0.047942</td>\n",
       "      <td>-0.056099</td>\n",
       "      <td>-0.017470</td>\n",
       "      <td>-0.009484</td>\n",
       "      <td>-0.019134</td>\n",
       "      <td>0.030603</td>\n",
       "      <td>0.010545</td>\n",
       "      <td>0.026725</td>\n",
       "      <td>-0.028449</td>\n",
       "      <td>-0.003071</td>\n",
       "      <td>-0.035192</td>\n",
       "      <td>-0.005018</td>\n",
       "      <td>-0.794296</td>\n",
       "      <td>0.794664</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.240865</td>\n",
       "      <td>0.235192</td>\n",
       "      <td>-0.291547</td>\n",
       "      <td>-0.583697</td>\n",
       "      <td>0.607087</td>\n",
       "      <td>-0.833074</td>\n",
       "      <td>-0.443593</td>\n",
       "      <td>0.457646</td>\n",
       "      <td>-0.622474</td>\n",
       "      <td>0.023228</td>\n",
       "      <td>0.065816</td>\n",
       "      <td>0.009484</td>\n",
       "      <td>0.017470</td>\n",
       "      <td>0.019134</td>\n",
       "      <td>-0.028449</td>\n",
       "      <td>-0.003071</td>\n",
       "      <td>-0.035192</td>\n",
       "      <td>0.030603</td>\n",
       "      <td>0.010545</td>\n",
       "      <td>0.026725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_xGF%</th>\n",
       "      <td>0.308746</td>\n",
       "      <td>0.222432</td>\n",
       "      <td>-0.199874</td>\n",
       "      <td>-0.001079</td>\n",
       "      <td>0.652191</td>\n",
       "      <td>-0.671988</td>\n",
       "      <td>0.833074</td>\n",
       "      <td>0.275538</td>\n",
       "      <td>-0.293418</td>\n",
       "      <td>0.337718</td>\n",
       "      <td>0.697641</td>\n",
       "      <td>-0.719138</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.579216</td>\n",
       "      <td>-0.588725</td>\n",
       "      <td>0.830989</td>\n",
       "      <td>-0.020997</td>\n",
       "      <td>-0.050849</td>\n",
       "      <td>0.066564</td>\n",
       "      <td>0.082184</td>\n",
       "      <td>0.105405</td>\n",
       "      <td>0.027767</td>\n",
       "      <td>0.004172</td>\n",
       "      <td>0.004868</td>\n",
       "      <td>-0.039837</td>\n",
       "      <td>0.011859</td>\n",
       "      <td>-0.019930</td>\n",
       "      <td>-0.001079</td>\n",
       "      <td>-0.671988</td>\n",
       "      <td>0.652191</td>\n",
       "      <td>-0.833074</td>\n",
       "      <td>-0.293418</td>\n",
       "      <td>0.275538</td>\n",
       "      <td>-0.344472</td>\n",
       "      <td>-0.719138</td>\n",
       "      <td>0.697641</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.588725</td>\n",
       "      <td>0.579216</td>\n",
       "      <td>-0.830989</td>\n",
       "      <td>0.011762</td>\n",
       "      <td>0.055970</td>\n",
       "      <td>-0.082184</td>\n",
       "      <td>-0.066564</td>\n",
       "      <td>-0.105405</td>\n",
       "      <td>-0.039837</td>\n",
       "      <td>0.011859</td>\n",
       "      <td>-0.019930</td>\n",
       "      <td>0.027767</td>\n",
       "      <td>0.004172</td>\n",
       "      <td>0.004868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_pts_pct</th>\n",
       "      <td>0.311811</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.299474</td>\n",
       "      <td>-0.048542</td>\n",
       "      <td>0.212247</td>\n",
       "      <td>-0.187624</td>\n",
       "      <td>0.245508</td>\n",
       "      <td>0.174385</td>\n",
       "      <td>-0.147103</td>\n",
       "      <td>0.213357</td>\n",
       "      <td>0.185113</td>\n",
       "      <td>-0.139928</td>\n",
       "      <td>0.222432</td>\n",
       "      <td>0.148468</td>\n",
       "      <td>-0.090921</td>\n",
       "      <td>0.156697</td>\n",
       "      <td>0.085666</td>\n",
       "      <td>0.040503</td>\n",
       "      <td>0.117491</td>\n",
       "      <td>0.091787</td>\n",
       "      <td>0.148274</td>\n",
       "      <td>0.020268</td>\n",
       "      <td>0.076586</td>\n",
       "      <td>0.061477</td>\n",
       "      <td>0.050210</td>\n",
       "      <td>-0.035844</td>\n",
       "      <td>-0.054559</td>\n",
       "      <td>-0.048542</td>\n",
       "      <td>-0.187624</td>\n",
       "      <td>0.212247</td>\n",
       "      <td>-0.245508</td>\n",
       "      <td>-0.147103</td>\n",
       "      <td>0.174385</td>\n",
       "      <td>-0.207479</td>\n",
       "      <td>-0.139928</td>\n",
       "      <td>0.185113</td>\n",
       "      <td>-0.222432</td>\n",
       "      <td>-0.090921</td>\n",
       "      <td>0.148468</td>\n",
       "      <td>-0.156697</td>\n",
       "      <td>-0.045957</td>\n",
       "      <td>-0.075734</td>\n",
       "      <td>-0.091787</td>\n",
       "      <td>-0.117491</td>\n",
       "      <td>-0.148274</td>\n",
       "      <td>0.050210</td>\n",
       "      <td>-0.035844</td>\n",
       "      <td>-0.054559</td>\n",
       "      <td>0.020268</td>\n",
       "      <td>0.076586</td>\n",
       "      <td>0.061477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_SV%</th>\n",
       "      <td>0.391314</td>\n",
       "      <td>0.091787</td>\n",
       "      <td>-0.097904</td>\n",
       "      <td>-0.024981</td>\n",
       "      <td>0.032620</td>\n",
       "      <td>0.043672</td>\n",
       "      <td>-0.009484</td>\n",
       "      <td>0.006682</td>\n",
       "      <td>-0.904281</td>\n",
       "      <td>0.506695</td>\n",
       "      <td>0.036802</td>\n",
       "      <td>-0.079045</td>\n",
       "      <td>0.082184</td>\n",
       "      <td>0.026528</td>\n",
       "      <td>-0.091234</td>\n",
       "      <td>0.088283</td>\n",
       "      <td>0.007515</td>\n",
       "      <td>0.583998</td>\n",
       "      <td>-0.003759</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.707018</td>\n",
       "      <td>-0.017539</td>\n",
       "      <td>-0.016585</td>\n",
       "      <td>-0.009624</td>\n",
       "      <td>0.070599</td>\n",
       "      <td>0.003235</td>\n",
       "      <td>0.020478</td>\n",
       "      <td>-0.024981</td>\n",
       "      <td>0.043672</td>\n",
       "      <td>0.032620</td>\n",
       "      <td>0.009484</td>\n",
       "      <td>-0.904281</td>\n",
       "      <td>0.006682</td>\n",
       "      <td>-0.590821</td>\n",
       "      <td>-0.079045</td>\n",
       "      <td>0.036802</td>\n",
       "      <td>-0.082184</td>\n",
       "      <td>-0.091234</td>\n",
       "      <td>0.026528</td>\n",
       "      <td>-0.088283</td>\n",
       "      <td>-0.614051</td>\n",
       "      <td>0.009304</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>-0.707018</td>\n",
       "      <td>0.070599</td>\n",
       "      <td>0.003235</td>\n",
       "      <td>0.020478</td>\n",
       "      <td>-0.017539</td>\n",
       "      <td>-0.016585</td>\n",
       "      <td>-0.009624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_SH%</th>\n",
       "      <td>0.410017</td>\n",
       "      <td>0.117491</td>\n",
       "      <td>-0.147067</td>\n",
       "      <td>-0.042498</td>\n",
       "      <td>-0.035409</td>\n",
       "      <td>-0.016086</td>\n",
       "      <td>-0.017470</td>\n",
       "      <td>0.902947</td>\n",
       "      <td>0.004810</td>\n",
       "      <td>0.611950</td>\n",
       "      <td>0.084883</td>\n",
       "      <td>-0.007808</td>\n",
       "      <td>0.066564</td>\n",
       "      <td>0.089976</td>\n",
       "      <td>-0.026361</td>\n",
       "      <td>0.083623</td>\n",
       "      <td>0.622819</td>\n",
       "      <td>0.006435</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.003759</td>\n",
       "      <td>0.704524</td>\n",
       "      <td>-0.003592</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>-0.012812</td>\n",
       "      <td>0.078463</td>\n",
       "      <td>-0.000561</td>\n",
       "      <td>0.007086</td>\n",
       "      <td>-0.042498</td>\n",
       "      <td>-0.016086</td>\n",
       "      <td>-0.035409</td>\n",
       "      <td>0.017470</td>\n",
       "      <td>0.004810</td>\n",
       "      <td>0.902947</td>\n",
       "      <td>-0.526348</td>\n",
       "      <td>-0.007808</td>\n",
       "      <td>0.084883</td>\n",
       "      <td>-0.066564</td>\n",
       "      <td>-0.026361</td>\n",
       "      <td>0.089976</td>\n",
       "      <td>-0.083623</td>\n",
       "      <td>-0.002944</td>\n",
       "      <td>-0.583800</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.704524</td>\n",
       "      <td>0.078463</td>\n",
       "      <td>-0.000561</td>\n",
       "      <td>0.007086</td>\n",
       "      <td>-0.003592</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>-0.012812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_GF/60</th>\n",
       "      <td>0.471605</td>\n",
       "      <td>0.174385</td>\n",
       "      <td>-0.190705</td>\n",
       "      <td>-0.053581</td>\n",
       "      <td>0.279004</td>\n",
       "      <td>-0.106659</td>\n",
       "      <td>0.235192</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.031082</td>\n",
       "      <td>0.655932</td>\n",
       "      <td>0.338300</td>\n",
       "      <td>-0.066878</td>\n",
       "      <td>0.275538</td>\n",
       "      <td>0.281645</td>\n",
       "      <td>-0.065001</td>\n",
       "      <td>0.234563</td>\n",
       "      <td>0.550207</td>\n",
       "      <td>-0.001518</td>\n",
       "      <td>0.902947</td>\n",
       "      <td>0.006682</td>\n",
       "      <td>0.643301</td>\n",
       "      <td>0.016105</td>\n",
       "      <td>-0.009721</td>\n",
       "      <td>-0.018239</td>\n",
       "      <td>0.079296</td>\n",
       "      <td>-0.006728</td>\n",
       "      <td>-0.001534</td>\n",
       "      <td>-0.053581</td>\n",
       "      <td>-0.106659</td>\n",
       "      <td>0.279004</td>\n",
       "      <td>-0.235192</td>\n",
       "      <td>-0.031082</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.572845</td>\n",
       "      <td>-0.066878</td>\n",
       "      <td>0.338300</td>\n",
       "      <td>-0.275538</td>\n",
       "      <td>-0.065001</td>\n",
       "      <td>0.281645</td>\n",
       "      <td>-0.234563</td>\n",
       "      <td>-0.001716</td>\n",
       "      <td>-0.509909</td>\n",
       "      <td>-0.006682</td>\n",
       "      <td>-0.902947</td>\n",
       "      <td>-0.643301</td>\n",
       "      <td>0.079296</td>\n",
       "      <td>-0.006728</td>\n",
       "      <td>-0.001534</td>\n",
       "      <td>0.016105</td>\n",
       "      <td>-0.009721</td>\n",
       "      <td>-0.018239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_GA/60</th>\n",
       "      <td>0.471605</td>\n",
       "      <td>0.174385</td>\n",
       "      <td>-0.190705</td>\n",
       "      <td>-0.053581</td>\n",
       "      <td>0.279004</td>\n",
       "      <td>-0.106659</td>\n",
       "      <td>0.235192</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.031082</td>\n",
       "      <td>0.655932</td>\n",
       "      <td>0.338300</td>\n",
       "      <td>-0.066878</td>\n",
       "      <td>0.275538</td>\n",
       "      <td>0.281645</td>\n",
       "      <td>-0.065001</td>\n",
       "      <td>0.234563</td>\n",
       "      <td>0.550207</td>\n",
       "      <td>-0.001518</td>\n",
       "      <td>0.902947</td>\n",
       "      <td>0.006682</td>\n",
       "      <td>0.643301</td>\n",
       "      <td>0.016105</td>\n",
       "      <td>-0.009721</td>\n",
       "      <td>-0.018239</td>\n",
       "      <td>0.079296</td>\n",
       "      <td>-0.006728</td>\n",
       "      <td>-0.001534</td>\n",
       "      <td>-0.053581</td>\n",
       "      <td>-0.106659</td>\n",
       "      <td>0.279004</td>\n",
       "      <td>-0.235192</td>\n",
       "      <td>-0.031082</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.572845</td>\n",
       "      <td>-0.066878</td>\n",
       "      <td>0.338300</td>\n",
       "      <td>-0.275538</td>\n",
       "      <td>-0.065001</td>\n",
       "      <td>0.281645</td>\n",
       "      <td>-0.234563</td>\n",
       "      <td>-0.001716</td>\n",
       "      <td>-0.509909</td>\n",
       "      <td>-0.006682</td>\n",
       "      <td>-0.902947</td>\n",
       "      <td>-0.643301</td>\n",
       "      <td>0.079296</td>\n",
       "      <td>-0.006728</td>\n",
       "      <td>-0.001534</td>\n",
       "      <td>0.016105</td>\n",
       "      <td>-0.009721</td>\n",
       "      <td>-0.018239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_PDO</th>\n",
       "      <td>0.567670</td>\n",
       "      <td>0.148274</td>\n",
       "      <td>-0.173449</td>\n",
       "      <td>-0.047804</td>\n",
       "      <td>-0.001971</td>\n",
       "      <td>0.019610</td>\n",
       "      <td>-0.019134</td>\n",
       "      <td>0.643301</td>\n",
       "      <td>-0.638375</td>\n",
       "      <td>0.792407</td>\n",
       "      <td>0.086092</td>\n",
       "      <td>-0.061702</td>\n",
       "      <td>0.105405</td>\n",
       "      <td>0.082468</td>\n",
       "      <td>-0.083431</td>\n",
       "      <td>0.121824</td>\n",
       "      <td>0.445838</td>\n",
       "      <td>0.419038</td>\n",
       "      <td>0.704524</td>\n",
       "      <td>0.707018</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.014956</td>\n",
       "      <td>-0.009004</td>\n",
       "      <td>-0.015955</td>\n",
       "      <td>0.105642</td>\n",
       "      <td>0.001989</td>\n",
       "      <td>0.019569</td>\n",
       "      <td>-0.047804</td>\n",
       "      <td>0.019610</td>\n",
       "      <td>-0.001971</td>\n",
       "      <td>0.019134</td>\n",
       "      <td>-0.638375</td>\n",
       "      <td>0.643301</td>\n",
       "      <td>-0.791560</td>\n",
       "      <td>-0.061702</td>\n",
       "      <td>0.086092</td>\n",
       "      <td>-0.105405</td>\n",
       "      <td>-0.083431</td>\n",
       "      <td>0.082468</td>\n",
       "      <td>-0.121824</td>\n",
       "      <td>-0.437913</td>\n",
       "      <td>-0.406268</td>\n",
       "      <td>-0.707018</td>\n",
       "      <td>-0.704524</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.105642</td>\n",
       "      <td>0.001989</td>\n",
       "      <td>0.019569</td>\n",
       "      <td>-0.014956</td>\n",
       "      <td>-0.009004</td>\n",
       "      <td>-0.015955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_GF%</th>\n",
       "      <td>0.596114</td>\n",
       "      <td>0.213357</td>\n",
       "      <td>-0.199982</td>\n",
       "      <td>-0.029788</td>\n",
       "      <td>0.236749</td>\n",
       "      <td>-0.209723</td>\n",
       "      <td>0.278097</td>\n",
       "      <td>0.655932</td>\n",
       "      <td>-0.549068</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.262857</td>\n",
       "      <td>-0.215864</td>\n",
       "      <td>0.337718</td>\n",
       "      <td>0.215166</td>\n",
       "      <td>-0.194106</td>\n",
       "      <td>0.290364</td>\n",
       "      <td>0.406797</td>\n",
       "      <td>0.309210</td>\n",
       "      <td>0.611950</td>\n",
       "      <td>0.506695</td>\n",
       "      <td>0.792407</td>\n",
       "      <td>-0.018453</td>\n",
       "      <td>0.015144</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.076387</td>\n",
       "      <td>0.013607</td>\n",
       "      <td>0.016294</td>\n",
       "      <td>-0.029788</td>\n",
       "      <td>-0.209723</td>\n",
       "      <td>0.236749</td>\n",
       "      <td>-0.278097</td>\n",
       "      <td>-0.549068</td>\n",
       "      <td>0.655932</td>\n",
       "      <td>-0.899743</td>\n",
       "      <td>-0.215864</td>\n",
       "      <td>0.262857</td>\n",
       "      <td>-0.337718</td>\n",
       "      <td>-0.194106</td>\n",
       "      <td>0.215166</td>\n",
       "      <td>-0.290364</td>\n",
       "      <td>-0.334971</td>\n",
       "      <td>-0.367419</td>\n",
       "      <td>-0.506695</td>\n",
       "      <td>-0.611950</td>\n",
       "      <td>-0.792407</td>\n",
       "      <td>0.076387</td>\n",
       "      <td>0.013607</td>\n",
       "      <td>0.016294</td>\n",
       "      <td>-0.018453</td>\n",
       "      <td>0.015144</td>\n",
       "      <td>0.001134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Home_Team_Won</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.311811</td>\n",
       "      <td>-0.309084</td>\n",
       "      <td>-0.030639</td>\n",
       "      <td>0.227636</td>\n",
       "      <td>-0.219077</td>\n",
       "      <td>0.275023</td>\n",
       "      <td>0.471605</td>\n",
       "      <td>-0.455510</td>\n",
       "      <td>0.596114</td>\n",
       "      <td>0.250745</td>\n",
       "      <td>-0.196675</td>\n",
       "      <td>0.308746</td>\n",
       "      <td>0.214950</td>\n",
       "      <td>-0.163710</td>\n",
       "      <td>0.261105</td>\n",
       "      <td>0.257639</td>\n",
       "      <td>0.224793</td>\n",
       "      <td>0.410017</td>\n",
       "      <td>0.391314</td>\n",
       "      <td>0.567670</td>\n",
       "      <td>-0.026110</td>\n",
       "      <td>0.165618</td>\n",
       "      <td>0.083060</td>\n",
       "      <td>0.074819</td>\n",
       "      <td>-0.069168</td>\n",
       "      <td>-0.045863</td>\n",
       "      <td>-0.030639</td>\n",
       "      <td>-0.219077</td>\n",
       "      <td>0.227636</td>\n",
       "      <td>-0.275023</td>\n",
       "      <td>-0.455510</td>\n",
       "      <td>0.471605</td>\n",
       "      <td>-0.599544</td>\n",
       "      <td>-0.196675</td>\n",
       "      <td>0.250745</td>\n",
       "      <td>-0.308746</td>\n",
       "      <td>-0.163710</td>\n",
       "      <td>0.214950</td>\n",
       "      <td>-0.261105</td>\n",
       "      <td>-0.248747</td>\n",
       "      <td>-0.228224</td>\n",
       "      <td>-0.391314</td>\n",
       "      <td>-0.410017</td>\n",
       "      <td>-0.567670</td>\n",
       "      <td>0.074819</td>\n",
       "      <td>-0.069168</td>\n",
       "      <td>-0.045863</td>\n",
       "      <td>-0.026110</td>\n",
       "      <td>0.165618</td>\n",
       "      <td>0.083060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Home_Team_Won  home_pts_pct  away_pts_pct  home_TOI  \\\n",
       "away_GF%            -0.599544     -0.207479      0.196330  0.047650   \n",
       "away_PDO            -0.567670     -0.148274      0.173449  0.047804   \n",
       "home_GA/60          -0.455510     -0.147103      0.149462  0.007840   \n",
       "away_GF/60          -0.455510     -0.147103      0.149462  0.007840   \n",
       "away_SV%            -0.410017     -0.117491      0.147067  0.042498   \n",
       "away_SH%            -0.391314     -0.091787      0.097904  0.024981   \n",
       "away_pts_pct        -0.309084     -0.299474      1.000000 -0.015278   \n",
       "away_xGF%           -0.308746     -0.222432      0.199874  0.001079   \n",
       "away_FF%            -0.275023     -0.245508      0.215557  0.005018   \n",
       "away_HDCF%          -0.261105     -0.156697      0.148598 -0.004665   \n",
       "away_HDSH%          -0.248747     -0.045957      0.044664  0.034530   \n",
       "away_HDSV%          -0.228224     -0.075734      0.098652  0.042804   \n",
       "home_FA/60          -0.219077     -0.187624      0.174660 -0.041437   \n",
       "away_FF/60          -0.219077     -0.187624      0.174660 -0.041437   \n",
       "away_xGF/60         -0.196675     -0.139928      0.142496 -0.052113   \n",
       "home_xGA/60         -0.196675     -0.139928      0.142496 -0.052113   \n",
       "home_HDCA/60        -0.163710     -0.090921      0.112092 -0.037430   \n",
       "away_HDCF/60        -0.163710     -0.090921      0.112092 -0.037430   \n",
       "home_GA/60_pk       -0.069168     -0.035844      0.056362  0.152379   \n",
       "away_GF/60_pp       -0.069168     -0.035844      0.056362  0.152379   \n",
       "away_xGF/60_pp      -0.045863     -0.054559      0.073780  0.058185   \n",
       "home_xGA/60_pk      -0.045863     -0.054559      0.073780  0.058185   \n",
       "home_TOI            -0.030639     -0.048542     -0.015278  1.000000   \n",
       "away_TOI            -0.030639     -0.048542     -0.015278  1.000000   \n",
       "home_TOI_pp         -0.026110      0.020268      0.006442 -0.691602   \n",
       "away_TOI_pk         -0.026110      0.020268      0.006442 -0.691602   \n",
       "away_TOI_pp          0.074819      0.050210     -0.001900 -0.660751   \n",
       "home_TOI_pk          0.074819      0.050210     -0.001900 -0.660751   \n",
       "away_xGA/60_pk       0.083060      0.061477     -0.046846  0.040873   \n",
       "home_xGF/60_pp       0.083060      0.061477     -0.046846  0.040873   \n",
       "home_GF/60_pp        0.165618      0.076586     -0.045646  0.123190   \n",
       "away_GA/60_pk        0.165618      0.076586     -0.045646  0.123190   \n",
       "home_HDCF/60         0.214950      0.148468     -0.099663 -0.033917   \n",
       "away_HDCA/60         0.214950      0.148468     -0.099663 -0.033917   \n",
       "home_HDSV%           0.224793      0.040503     -0.034100 -0.024352   \n",
       "away_FA/60           0.227636      0.212247     -0.166939 -0.055173   \n",
       "home_FF/60           0.227636      0.212247     -0.166939 -0.055173   \n",
       "home_xGF/60          0.250745      0.185113     -0.135289 -0.061678   \n",
       "away_xGA/60          0.250745      0.185113     -0.135289 -0.061678   \n",
       "home_HDSH%           0.257639      0.085666     -0.102978 -0.033984   \n",
       "home_HDCF%           0.261105      0.156697     -0.148598  0.004665   \n",
       "home_FF%             0.275023      0.245508     -0.215557 -0.005018   \n",
       "home_xGF%            0.308746      0.222432     -0.199874 -0.001079   \n",
       "home_pts_pct         0.311811      1.000000     -0.299474 -0.048542   \n",
       "home_SV%             0.391314      0.091787     -0.097904 -0.024981   \n",
       "home_SH%             0.410017      0.117491     -0.147067 -0.042498   \n",
       "home_GF/60           0.471605      0.174385     -0.190705 -0.053581   \n",
       "away_GA/60           0.471605      0.174385     -0.190705 -0.053581   \n",
       "home_PDO             0.567670      0.148274     -0.173449 -0.047804   \n",
       "home_GF%             0.596114      0.213357     -0.199982 -0.029788   \n",
       "Home_Team_Won        1.000000      0.311811     -0.309084 -0.030639   \n",
       "\n",
       "                home_FF/60  home_FA/60  home_FF%  home_GF/60  home_GA/60  \\\n",
       "away_GF%         -0.239146    0.228527 -0.291547   -0.572845    0.631991   \n",
       "away_PDO          0.001971   -0.019610  0.019134   -0.643301    0.638375   \n",
       "home_GA/60       -0.122315    0.269732 -0.240865   -0.031082    1.000000   \n",
       "away_GF/60       -0.122315    0.269732 -0.240865   -0.031082    1.000000   \n",
       "away_SV%          0.035409    0.016086  0.017470   -0.902947   -0.004810   \n",
       "away_SH%         -0.032620   -0.043672  0.009484   -0.006682    0.904281   \n",
       "away_pts_pct     -0.166939    0.174660 -0.215557   -0.190705    0.149462   \n",
       "away_xGF%        -0.652191    0.671988 -0.833074   -0.275538    0.293418   \n",
       "away_FF%         -0.794664    0.794296 -1.000000   -0.235192    0.240865   \n",
       "away_HDCF%       -0.485419    0.500645 -0.622474   -0.234563    0.248288   \n",
       "away_HDSH%       -0.005284   -0.036177  0.023228   -0.001716    0.548959   \n",
       "away_HDSV%        0.071520   -0.026069  0.065816   -0.509909   -0.021792   \n",
       "home_FA/60       -0.286500    1.000000 -0.794296   -0.106659    0.269732   \n",
       "away_FF/60       -0.286500    1.000000 -0.794296   -0.106659    0.269732   \n",
       "away_xGF/60      -0.191641    0.755230 -0.583697   -0.066878    0.323574   \n",
       "home_xGA/60      -0.191641    0.755230 -0.583697   -0.066878    0.323574   \n",
       "home_HDCA/60     -0.152971    0.563438 -0.443593   -0.065001    0.274380   \n",
       "away_HDCF/60     -0.152971    0.563438 -0.443593   -0.065001    0.274380   \n",
       "home_GA/60_pk    -0.017723   -0.010184 -0.003071   -0.006728   -0.004895   \n",
       "away_GF/60_pp    -0.017723   -0.010184 -0.003071   -0.006728   -0.004895   \n",
       "away_xGF/60_pp   -0.032859    0.028537 -0.035192   -0.001534   -0.019836   \n",
       "home_xGA/60_pk   -0.032859    0.028537 -0.035192   -0.001534   -0.019836   \n",
       "home_TOI         -0.055173   -0.041437 -0.005018   -0.053581    0.007840   \n",
       "away_TOI         -0.055173   -0.041437 -0.005018   -0.053581    0.007840   \n",
       "home_TOI_pp       0.050635   -0.000832  0.030603    0.016105    0.024202   \n",
       "away_TOI_pk       0.050635   -0.000832  0.030603    0.016105    0.024202   \n",
       "away_TOI_pp       0.015539    0.057308 -0.028449    0.079296   -0.052826   \n",
       "home_TOI_pk       0.015539    0.057308 -0.028449    0.079296   -0.052826   \n",
       "away_xGA/60_pk    0.023092   -0.022549  0.026725   -0.018239    0.000566   \n",
       "home_xGF/60_pp    0.023092   -0.022549  0.026725   -0.018239    0.000566   \n",
       "home_GF/60_pp    -0.002858   -0.021837  0.010545   -0.009721    0.000321   \n",
       "away_GA/60_pk    -0.002858   -0.021837  0.010545   -0.009721    0.000321   \n",
       "home_HDCF/60      0.578480   -0.165019  0.457646    0.281645   -0.080265   \n",
       "away_HDCA/60      0.578480   -0.165019  0.457646    0.281645   -0.080265   \n",
       "home_HDSV%       -0.015662    0.065495 -0.056099   -0.001518   -0.512497   \n",
       "away_FA/60        1.000000   -0.286500  0.794664    0.279004   -0.122315   \n",
       "home_FF/60        1.000000   -0.286500  0.794664    0.279004   -0.122315   \n",
       "home_xGF/60       0.770039   -0.217017  0.607087    0.338300   -0.102017   \n",
       "away_xGA/60       0.770039   -0.217017  0.607087    0.338300   -0.102017   \n",
       "home_HDSH%       -0.054678    0.015759 -0.047942    0.550207    0.000276   \n",
       "home_HDCF%        0.485419   -0.500645  0.622474    0.234563   -0.248288   \n",
       "home_FF%          0.794664   -0.794296  1.000000    0.235192   -0.240865   \n",
       "home_xGF%         0.652191   -0.671988  0.833074    0.275538   -0.293418   \n",
       "home_pts_pct      0.212247   -0.187624  0.245508    0.174385   -0.147103   \n",
       "home_SV%          0.032620    0.043672 -0.009484    0.006682   -0.904281   \n",
       "home_SH%         -0.035409   -0.016086 -0.017470    0.902947    0.004810   \n",
       "home_GF/60        0.279004   -0.106659  0.235192    1.000000   -0.031082   \n",
       "away_GA/60        0.279004   -0.106659  0.235192    1.000000   -0.031082   \n",
       "home_PDO         -0.001971    0.019610 -0.019134    0.643301   -0.638375   \n",
       "home_GF%          0.236749   -0.209723  0.278097    0.655932   -0.549068   \n",
       "Home_Team_Won     0.227636   -0.219077  0.275023    0.471605   -0.455510   \n",
       "\n",
       "                home_GF%  home_xGF/60  home_xGA/60  home_xGF%  home_HDCF/60  \\\n",
       "away_GF%       -0.899743    -0.253725     0.232837  -0.344472     -0.196734   \n",
       "away_PDO       -0.792407    -0.086092     0.061702  -0.105405     -0.082468   \n",
       "home_GA/60     -0.549068    -0.102017     0.323574  -0.293418     -0.080265   \n",
       "away_GF/60     -0.549068    -0.102017     0.323574  -0.293418     -0.080265   \n",
       "away_SV%       -0.611950    -0.084883     0.007808  -0.066564     -0.089976   \n",
       "away_SH%       -0.506695    -0.036802     0.079045  -0.082184     -0.026528   \n",
       "away_pts_pct   -0.199982    -0.135289     0.142496  -0.199874     -0.099663   \n",
       "away_xGF%      -0.337718    -0.697641     0.719138  -1.000000     -0.579216   \n",
       "away_FF%       -0.278097    -0.607087     0.583697  -0.833074     -0.457646   \n",
       "away_HDCF%     -0.290364    -0.565717     0.595313  -0.830989     -0.649944   \n",
       "away_HDSH%     -0.334971    -0.029210    -0.037498   0.011762     -0.021528   \n",
       "away_HDSV%     -0.367419     0.071869     0.002049   0.055970      0.121864   \n",
       "home_FA/60     -0.209723    -0.217017     0.755230  -0.671988     -0.165019   \n",
       "away_FF/60     -0.209723    -0.217017     0.755230  -0.671988     -0.165019   \n",
       "away_xGF/60    -0.215864    -0.055239     1.000000  -0.719138     -0.032229   \n",
       "home_xGA/60    -0.215864    -0.055239     1.000000  -0.719138     -0.032229   \n",
       "home_HDCA/60   -0.194106    -0.026902     0.831560  -0.588725      0.014210   \n",
       "away_HDCF/60   -0.194106    -0.026902     0.831560  -0.588725      0.014210   \n",
       "home_GA/60_pk   0.013607    -0.009513    -0.022791   0.011859     -0.018943   \n",
       "away_GF/60_pp   0.013607    -0.009513    -0.022791   0.011859     -0.018943   \n",
       "away_xGF/60_pp  0.016294     0.000068     0.029531  -0.019930     -0.008252   \n",
       "home_xGA/60_pk  0.016294     0.000068     0.029531  -0.019930     -0.008252   \n",
       "home_TOI       -0.029788    -0.061678    -0.052113  -0.001079     -0.033917   \n",
       "away_TOI       -0.029788    -0.061678    -0.052113  -0.001079     -0.033917   \n",
       "home_TOI_pp    -0.018453     0.051535     0.005374   0.027767      0.025999   \n",
       "away_TOI_pk    -0.018453     0.051535     0.005374   0.027767      0.025999   \n",
       "away_TOI_pp     0.076387     0.019201     0.070046  -0.039837      0.000505   \n",
       "home_TOI_pk     0.076387     0.019201     0.070046  -0.039837      0.000505   \n",
       "away_xGA/60_pk  0.001134     0.042165     0.034806   0.004868      0.034069   \n",
       "home_xGF/60_pp  0.001134     0.042165     0.034806   0.004868      0.034069   \n",
       "home_GF/60_pp   0.015144     0.005702     0.000627   0.004172      0.009156   \n",
       "away_GA/60_pk   0.015144     0.005702     0.000627   0.004172      0.009156   \n",
       "home_HDCF/60    0.215166     0.835989    -0.032229   0.579216      1.000000   \n",
       "away_HDCA/60    0.215166     0.835989    -0.032229   0.579216      1.000000   \n",
       "home_HDSV%      0.309210     0.015850     0.070361  -0.050849      0.018593   \n",
       "away_FA/60      0.236749     0.770039    -0.191641   0.652191      0.578480   \n",
       "home_FF/60      0.236749     0.770039    -0.191641   0.652191      0.578480   \n",
       "home_xGF/60     0.262857     1.000000    -0.055239   0.697641      0.835989   \n",
       "away_xGA/60     0.262857     1.000000    -0.055239   0.697641      0.835989   \n",
       "home_HDSH%      0.406797    -0.036473    -0.007614  -0.020997     -0.070292   \n",
       "home_HDCF%      0.290364     0.565717    -0.595313   0.830989      0.649944   \n",
       "home_FF%        0.278097     0.607087    -0.583697   0.833074      0.457646   \n",
       "home_xGF%       0.337718     0.697641    -0.719138   1.000000      0.579216   \n",
       "home_pts_pct    0.213357     0.185113    -0.139928   0.222432      0.148468   \n",
       "home_SV%        0.506695     0.036802    -0.079045   0.082184      0.026528   \n",
       "home_SH%        0.611950     0.084883    -0.007808   0.066564      0.089976   \n",
       "home_GF/60      0.655932     0.338300    -0.066878   0.275538      0.281645   \n",
       "away_GA/60      0.655932     0.338300    -0.066878   0.275538      0.281645   \n",
       "home_PDO        0.792407     0.086092    -0.061702   0.105405      0.082468   \n",
       "home_GF%        1.000000     0.262857    -0.215864   0.337718      0.215166   \n",
       "Home_Team_Won   0.596114     0.250745    -0.196675   0.308746      0.214950   \n",
       "\n",
       "                home_HDCA/60  home_HDCF%  home_HDSH%  home_HDSV%  home_SH%  \\\n",
       "away_GF%            0.214650   -0.294683   -0.345242   -0.361695 -0.526348   \n",
       "away_PDO            0.083431   -0.121824   -0.445838   -0.419038 -0.704524   \n",
       "home_GA/60          0.274380   -0.248288    0.000276   -0.512497  0.004810   \n",
       "away_GF/60          0.274380   -0.248288    0.000276   -0.512497  0.004810   \n",
       "away_SV%            0.026361   -0.083623   -0.622819   -0.006435 -1.000000   \n",
       "away_SH%            0.091234   -0.088283   -0.007515   -0.583998  0.003759   \n",
       "away_pts_pct        0.112092   -0.148598   -0.102978   -0.034100 -0.147067   \n",
       "away_xGF%           0.588725   -0.830989    0.020997    0.050849 -0.066564   \n",
       "away_FF%            0.443593   -0.622474    0.047942    0.056099  0.017470   \n",
       "away_HDCF%          0.684735   -1.000000    0.036509    0.074433 -0.083623   \n",
       "away_HDSH%         -0.054165    0.019779   -0.010183   -0.944957 -0.002944   \n",
       "away_HDSV%          0.021349    0.093704   -0.922278    0.005707 -0.583800   \n",
       "home_FA/60          0.563438   -0.500645    0.015759    0.065495 -0.016086   \n",
       "away_FF/60          0.563438   -0.500645    0.015759    0.065495 -0.016086   \n",
       "away_xGF/60         0.831560   -0.595313   -0.007614    0.070361 -0.007808   \n",
       "home_xGA/60         0.831560   -0.595313   -0.007614    0.070361 -0.007808   \n",
       "home_HDCA/60        1.000000   -0.684735   -0.020650    0.093310 -0.026361   \n",
       "away_HDCF/60        1.000000   -0.684735   -0.020650    0.093310 -0.026361   \n",
       "home_GA/60_pk      -0.013031    0.000671   -0.001016    0.005220 -0.000561   \n",
       "away_GF/60_pp      -0.013031    0.000671   -0.001016    0.005220 -0.000561   \n",
       "away_xGF/60_pp      0.021115   -0.020263   -0.006420    0.012595  0.007086   \n",
       "home_xGA/60_pk      0.021115   -0.020263   -0.006420    0.012595  0.007086   \n",
       "home_TOI           -0.037430    0.004665   -0.033984   -0.024352 -0.042498   \n",
       "away_TOI           -0.037430    0.004665   -0.033984   -0.024352 -0.042498   \n",
       "home_TOI_pp         0.010195    0.013246    0.001825   -0.000474 -0.003592   \n",
       "away_TOI_pk         0.010195    0.013246    0.001825   -0.000474 -0.003592   \n",
       "away_TOI_pp         0.043808   -0.034001    0.061540    0.052369  0.078463   \n",
       "home_TOI_pk         0.043808   -0.034001    0.061540    0.052369  0.078463   \n",
       "away_xGA/60_pk      0.038420    0.002292   -0.007027   -0.003153 -0.012812   \n",
       "home_xGF/60_pp      0.038420    0.002292   -0.007027   -0.003153 -0.012812   \n",
       "home_GF/60_pp       0.013524    0.002395   -0.020554   -0.027889  0.003903   \n",
       "away_GA/60_pk       0.013524    0.002395   -0.020554   -0.027889  0.003903   \n",
       "home_HDCF/60        0.014210    0.649944   -0.070292    0.018593  0.089976   \n",
       "away_HDCA/60        0.014210    0.649944   -0.070292    0.018593  0.089976   \n",
       "home_HDSV%          0.093310   -0.074433    0.005884    1.000000  0.006435   \n",
       "away_FA/60         -0.152971    0.485419   -0.054678   -0.015662 -0.035409   \n",
       "home_FF/60         -0.152971    0.485419   -0.054678   -0.015662 -0.035409   \n",
       "home_xGF/60        -0.026902    0.565717   -0.036473    0.015850  0.084883   \n",
       "away_xGA/60        -0.026902    0.565717   -0.036473    0.015850  0.084883   \n",
       "home_HDSH%         -0.020650   -0.036509    1.000000    0.005884  0.622819   \n",
       "home_HDCF%         -0.684735    1.000000   -0.036509   -0.074433  0.083623   \n",
       "home_FF%           -0.443593    0.622474   -0.047942   -0.056099 -0.017470   \n",
       "home_xGF%          -0.588725    0.830989   -0.020997   -0.050849  0.066564   \n",
       "home_pts_pct       -0.090921    0.156697    0.085666    0.040503  0.117491   \n",
       "home_SV%           -0.091234    0.088283    0.007515    0.583998 -0.003759   \n",
       "home_SH%           -0.026361    0.083623    0.622819    0.006435  1.000000   \n",
       "home_GF/60         -0.065001    0.234563    0.550207   -0.001518  0.902947   \n",
       "away_GA/60         -0.065001    0.234563    0.550207   -0.001518  0.902947   \n",
       "home_PDO           -0.083431    0.121824    0.445838    0.419038  0.704524   \n",
       "home_GF%           -0.194106    0.290364    0.406797    0.309210  0.611950   \n",
       "Home_Team_Won      -0.163710    0.261105    0.257639    0.224793  0.410017   \n",
       "\n",
       "                home_SV%  home_PDO  home_TOI_pp  home_GF/60_pp  \\\n",
       "away_GF%       -0.590821 -0.791560    -0.003241      -0.003981   \n",
       "away_PDO       -0.707018 -1.000000     0.014956       0.009004   \n",
       "home_GA/60     -0.904281 -0.638375     0.024202       0.000321   \n",
       "away_GF/60     -0.904281 -0.638375     0.024202       0.000321   \n",
       "away_SV%        0.003759 -0.704524     0.003592      -0.003903   \n",
       "away_SH%       -1.000000 -0.707018     0.017539       0.016585   \n",
       "away_pts_pct   -0.097904 -0.173449     0.006442      -0.045646   \n",
       "away_xGF%      -0.082184 -0.105405    -0.027767      -0.004172   \n",
       "away_FF%        0.009484  0.019134    -0.030603      -0.010545   \n",
       "away_HDCF%     -0.088283 -0.121824    -0.013246      -0.002395   \n",
       "away_HDSH%     -0.614051 -0.437913    -0.011768       0.027728   \n",
       "away_HDSV%      0.009304 -0.406268     0.000943       0.022214   \n",
       "home_FA/60      0.043672  0.019610    -0.000832      -0.021837   \n",
       "away_FF/60      0.043672  0.019610    -0.000832      -0.021837   \n",
       "away_xGF/60    -0.079045 -0.061702     0.005374       0.000627   \n",
       "home_xGA/60    -0.079045 -0.061702     0.005374       0.000627   \n",
       "home_HDCA/60   -0.091234 -0.083431     0.010195       0.013524   \n",
       "away_HDCF/60   -0.091234 -0.083431     0.010195       0.013524   \n",
       "home_GA/60_pk   0.003235  0.001989    -0.008089      -0.001370   \n",
       "away_GF/60_pp   0.003235  0.001989    -0.008089      -0.001370   \n",
       "away_xGF/60_pp  0.020478  0.019569     0.012004       0.025009   \n",
       "home_xGA/60_pk  0.020478  0.019569     0.012004       0.025009   \n",
       "home_TOI       -0.024981 -0.047804    -0.691602       0.123190   \n",
       "away_TOI       -0.024981 -0.047804    -0.691602       0.123190   \n",
       "home_TOI_pp    -0.017539 -0.014956     1.000000      -0.241019   \n",
       "away_TOI_pk    -0.017539 -0.014956     1.000000      -0.241019   \n",
       "away_TOI_pp     0.070599  0.105642     0.130937       0.050402   \n",
       "home_TOI_pk     0.070599  0.105642     0.130937       0.050402   \n",
       "away_xGA/60_pk -0.009624 -0.015955    -0.096287       0.482805   \n",
       "home_xGF/60_pp -0.009624 -0.015955    -0.096287       0.482805   \n",
       "home_GF/60_pp  -0.016585 -0.009004    -0.241019       1.000000   \n",
       "away_GA/60_pk  -0.016585 -0.009004    -0.241019       1.000000   \n",
       "home_HDCF/60    0.026528  0.082468     0.025999       0.009156   \n",
       "away_HDCA/60    0.026528  0.082468     0.025999       0.009156   \n",
       "home_HDSV%      0.583998  0.419038    -0.000474      -0.027889   \n",
       "away_FA/60      0.032620 -0.001971     0.050635      -0.002858   \n",
       "home_FF/60      0.032620 -0.001971     0.050635      -0.002858   \n",
       "home_xGF/60     0.036802  0.086092     0.051535       0.005702   \n",
       "away_xGA/60     0.036802  0.086092     0.051535       0.005702   \n",
       "home_HDSH%      0.007515  0.445838     0.001825      -0.020554   \n",
       "home_HDCF%      0.088283  0.121824     0.013246       0.002395   \n",
       "home_FF%       -0.009484 -0.019134     0.030603       0.010545   \n",
       "home_xGF%       0.082184  0.105405     0.027767       0.004172   \n",
       "home_pts_pct    0.091787  0.148274     0.020268       0.076586   \n",
       "home_SV%        1.000000  0.707018    -0.017539      -0.016585   \n",
       "home_SH%       -0.003759  0.704524    -0.003592       0.003903   \n",
       "home_GF/60      0.006682  0.643301     0.016105      -0.009721   \n",
       "away_GA/60      0.006682  0.643301     0.016105      -0.009721   \n",
       "home_PDO        0.707018  1.000000    -0.014956      -0.009004   \n",
       "home_GF%        0.506695  0.792407    -0.018453       0.015144   \n",
       "Home_Team_Won   0.391314  0.567670    -0.026110       0.165618   \n",
       "\n",
       "                home_xGF/60_pp  home_TOI_pk  home_GA/60_pk  home_xGA/60_pk  \\\n",
       "away_GF%              0.009349    -0.087827      -0.008636       -0.011899   \n",
       "away_PDO              0.015955    -0.105642      -0.001989       -0.019569   \n",
       "home_GA/60            0.000566    -0.052826      -0.004895       -0.019836   \n",
       "away_GF/60            0.000566    -0.052826      -0.004895       -0.019836   \n",
       "away_SV%              0.012812    -0.078463       0.000561       -0.007086   \n",
       "away_SH%              0.009624    -0.070599      -0.003235       -0.020478   \n",
       "away_pts_pct         -0.046846    -0.001900       0.056362        0.073780   \n",
       "away_xGF%            -0.004868     0.039837      -0.011859        0.019930   \n",
       "away_FF%             -0.026725     0.028449       0.003071        0.035192   \n",
       "away_HDCF%           -0.002292     0.034001      -0.000671        0.020263   \n",
       "away_HDSH%            0.003580    -0.055291      -0.006306       -0.015768   \n",
       "away_HDSV%            0.010185    -0.079490       0.005527        0.008312   \n",
       "home_FA/60           -0.022549     0.057308      -0.010184        0.028537   \n",
       "away_FF/60           -0.022549     0.057308      -0.010184        0.028537   \n",
       "away_xGF/60           0.034806     0.070046      -0.022791        0.029531   \n",
       "home_xGA/60           0.034806     0.070046      -0.022791        0.029531   \n",
       "home_HDCA/60          0.038420     0.043808      -0.013031        0.021115   \n",
       "away_HDCF/60          0.038420     0.043808      -0.013031        0.021115   \n",
       "home_GA/60_pk        -0.003956    -0.228760       1.000000        0.644702   \n",
       "away_GF/60_pp        -0.003956    -0.228760       1.000000        0.644702   \n",
       "away_xGF/60_pp        0.025557    -0.118314       0.644702        1.000000   \n",
       "home_xGA/60_pk        0.025557    -0.118314       0.644702        1.000000   \n",
       "home_TOI              0.040873    -0.660751       0.152379        0.058185   \n",
       "away_TOI              0.040873    -0.660751       0.152379        0.058185   \n",
       "home_TOI_pp          -0.096287     0.130937      -0.008089        0.012004   \n",
       "away_TOI_pk          -0.096287     0.130937      -0.008089        0.012004   \n",
       "away_TOI_pp           0.027099     1.000000      -0.228760       -0.118314   \n",
       "home_TOI_pk           0.027099     1.000000      -0.228760       -0.118314   \n",
       "away_xGA/60_pk        1.000000     0.027099      -0.003956        0.025557   \n",
       "home_xGF/60_pp        1.000000     0.027099      -0.003956        0.025557   \n",
       "home_GF/60_pp         0.482805     0.050402      -0.001370        0.025009   \n",
       "away_GA/60_pk         0.482805     0.050402      -0.001370        0.025009   \n",
       "home_HDCF/60          0.034069     0.000505      -0.018943       -0.008252   \n",
       "away_HDCA/60          0.034069     0.000505      -0.018943       -0.008252   \n",
       "home_HDSV%           -0.003153     0.052369       0.005220        0.012595   \n",
       "away_FA/60            0.023092     0.015539      -0.017723       -0.032859   \n",
       "home_FF/60            0.023092     0.015539      -0.017723       -0.032859   \n",
       "home_xGF/60           0.042165     0.019201      -0.009513        0.000068   \n",
       "away_xGA/60           0.042165     0.019201      -0.009513        0.000068   \n",
       "home_HDSH%           -0.007027     0.061540      -0.001016       -0.006420   \n",
       "home_HDCF%            0.002292    -0.034001       0.000671       -0.020263   \n",
       "home_FF%              0.026725    -0.028449      -0.003071       -0.035192   \n",
       "home_xGF%             0.004868    -0.039837       0.011859       -0.019930   \n",
       "home_pts_pct          0.061477     0.050210      -0.035844       -0.054559   \n",
       "home_SV%             -0.009624     0.070599       0.003235        0.020478   \n",
       "home_SH%             -0.012812     0.078463      -0.000561        0.007086   \n",
       "home_GF/60           -0.018239     0.079296      -0.006728       -0.001534   \n",
       "away_GA/60           -0.018239     0.079296      -0.006728       -0.001534   \n",
       "home_PDO             -0.015955     0.105642       0.001989        0.019569   \n",
       "home_GF%              0.001134     0.076387       0.013607        0.016294   \n",
       "Home_Team_Won         0.083060     0.074819      -0.069168       -0.045863   \n",
       "\n",
       "                away_TOI  away_FF/60  away_FA/60  away_FF%  away_GF/60  \\\n",
       "away_GF%        0.047650    0.228527   -0.239146  0.291547    0.631991   \n",
       "away_PDO        0.047804   -0.019610    0.001971 -0.019134    0.638375   \n",
       "home_GA/60      0.007840    0.269732   -0.122315  0.240865    1.000000   \n",
       "away_GF/60      0.007840    0.269732   -0.122315  0.240865    1.000000   \n",
       "away_SV%        0.042498    0.016086    0.035409 -0.017470   -0.004810   \n",
       "away_SH%        0.024981   -0.043672   -0.032620 -0.009484    0.904281   \n",
       "away_pts_pct   -0.015278    0.174660   -0.166939  0.215557    0.149462   \n",
       "away_xGF%       0.001079    0.671988   -0.652191  0.833074    0.293418   \n",
       "away_FF%        0.005018    0.794296   -0.794664  1.000000    0.240865   \n",
       "away_HDCF%     -0.004665    0.500645   -0.485419  0.622474    0.248288   \n",
       "away_HDSH%      0.034530   -0.036177   -0.005284 -0.023228    0.548959   \n",
       "away_HDSV%      0.042804   -0.026069    0.071520 -0.065816   -0.021792   \n",
       "home_FA/60     -0.041437    1.000000   -0.286500  0.794296    0.269732   \n",
       "away_FF/60     -0.041437    1.000000   -0.286500  0.794296    0.269732   \n",
       "away_xGF/60    -0.052113    0.755230   -0.191641  0.583697    0.323574   \n",
       "home_xGA/60    -0.052113    0.755230   -0.191641  0.583697    0.323574   \n",
       "home_HDCA/60   -0.037430    0.563438   -0.152971  0.443593    0.274380   \n",
       "away_HDCF/60   -0.037430    0.563438   -0.152971  0.443593    0.274380   \n",
       "home_GA/60_pk   0.152379   -0.010184   -0.017723  0.003071   -0.004895   \n",
       "away_GF/60_pp   0.152379   -0.010184   -0.017723  0.003071   -0.004895   \n",
       "away_xGF/60_pp  0.058185    0.028537   -0.032859  0.035192   -0.019836   \n",
       "home_xGA/60_pk  0.058185    0.028537   -0.032859  0.035192   -0.019836   \n",
       "home_TOI        1.000000   -0.041437   -0.055173  0.005018    0.007840   \n",
       "away_TOI        1.000000   -0.041437   -0.055173  0.005018    0.007840   \n",
       "home_TOI_pp    -0.691602   -0.000832    0.050635 -0.030603    0.024202   \n",
       "away_TOI_pk    -0.691602   -0.000832    0.050635 -0.030603    0.024202   \n",
       "away_TOI_pp    -0.660751    0.057308    0.015539  0.028449   -0.052826   \n",
       "home_TOI_pk    -0.660751    0.057308    0.015539  0.028449   -0.052826   \n",
       "away_xGA/60_pk  0.040873   -0.022549    0.023092 -0.026725    0.000566   \n",
       "home_xGF/60_pp  0.040873   -0.022549    0.023092 -0.026725    0.000566   \n",
       "home_GF/60_pp   0.123190   -0.021837   -0.002858 -0.010545    0.000321   \n",
       "away_GA/60_pk   0.123190   -0.021837   -0.002858 -0.010545    0.000321   \n",
       "home_HDCF/60   -0.033917   -0.165019    0.578480 -0.457646   -0.080265   \n",
       "away_HDCA/60   -0.033917   -0.165019    0.578480 -0.457646   -0.080265   \n",
       "home_HDSV%     -0.024352    0.065495   -0.015662  0.056099   -0.512497   \n",
       "away_FA/60     -0.055173   -0.286500    1.000000 -0.794664   -0.122315   \n",
       "home_FF/60     -0.055173   -0.286500    1.000000 -0.794664   -0.122315   \n",
       "home_xGF/60    -0.061678   -0.217017    0.770039 -0.607087   -0.102017   \n",
       "away_xGA/60    -0.061678   -0.217017    0.770039 -0.607087   -0.102017   \n",
       "home_HDSH%     -0.033984    0.015759   -0.054678  0.047942    0.000276   \n",
       "home_HDCF%      0.004665   -0.500645    0.485419 -0.622474   -0.248288   \n",
       "home_FF%       -0.005018   -0.794296    0.794664 -1.000000   -0.240865   \n",
       "home_xGF%      -0.001079   -0.671988    0.652191 -0.833074   -0.293418   \n",
       "home_pts_pct   -0.048542   -0.187624    0.212247 -0.245508   -0.147103   \n",
       "home_SV%       -0.024981    0.043672    0.032620  0.009484   -0.904281   \n",
       "home_SH%       -0.042498   -0.016086   -0.035409  0.017470    0.004810   \n",
       "home_GF/60     -0.053581   -0.106659    0.279004 -0.235192   -0.031082   \n",
       "away_GA/60     -0.053581   -0.106659    0.279004 -0.235192   -0.031082   \n",
       "home_PDO       -0.047804    0.019610   -0.001971  0.019134   -0.638375   \n",
       "home_GF%       -0.029788   -0.209723    0.236749 -0.278097   -0.549068   \n",
       "Home_Team_Won  -0.030639   -0.219077    0.227636 -0.275023   -0.455510   \n",
       "\n",
       "                away_GA/60  away_GF%  away_xGF/60  away_xGA/60  away_xGF%  \\\n",
       "away_GF%         -0.572845  1.000000     0.232837    -0.253725   0.344472   \n",
       "away_PDO         -0.643301  0.791560     0.061702    -0.086092   0.105405   \n",
       "home_GA/60       -0.031082  0.631991     0.323574    -0.102017   0.293418   \n",
       "away_GF/60       -0.031082  0.631991     0.323574    -0.102017   0.293418   \n",
       "away_SV%         -0.902947  0.526348     0.007808    -0.084883   0.066564   \n",
       "away_SH%         -0.006682  0.590821     0.079045    -0.036802   0.082184   \n",
       "away_pts_pct     -0.190705  0.196330     0.142496    -0.135289   0.199874   \n",
       "away_xGF%        -0.275538  0.344472     0.719138    -0.697641   1.000000   \n",
       "away_FF%         -0.235192  0.291547     0.583697    -0.607087   0.833074   \n",
       "away_HDCF%       -0.234563  0.294683     0.595313    -0.565717   0.830989   \n",
       "away_HDSH%       -0.001716  0.394039    -0.037498    -0.029210  -0.011762   \n",
       "away_HDSV%       -0.509909  0.307241     0.002049     0.071869  -0.055970   \n",
       "home_FA/60       -0.106659  0.228527     0.755230    -0.217017   0.671988   \n",
       "away_FF/60       -0.106659  0.228527     0.755230    -0.217017   0.671988   \n",
       "away_xGF/60      -0.066878  0.232837     1.000000    -0.055239   0.719138   \n",
       "home_xGA/60      -0.066878  0.232837     1.000000    -0.055239   0.719138   \n",
       "home_HDCA/60     -0.065001  0.214650     0.831560    -0.026902   0.588725   \n",
       "away_HDCF/60     -0.065001  0.214650     0.831560    -0.026902   0.588725   \n",
       "home_GA/60_pk    -0.006728 -0.008636    -0.022791    -0.009513  -0.011859   \n",
       "away_GF/60_pp    -0.006728 -0.008636    -0.022791    -0.009513  -0.011859   \n",
       "away_xGF/60_pp   -0.001534 -0.011899     0.029531     0.000068   0.019930   \n",
       "home_xGA/60_pk   -0.001534 -0.011899     0.029531     0.000068   0.019930   \n",
       "home_TOI         -0.053581  0.047650    -0.052113    -0.061678   0.001079   \n",
       "away_TOI         -0.053581  0.047650    -0.052113    -0.061678   0.001079   \n",
       "home_TOI_pp       0.016105 -0.003241     0.005374     0.051535  -0.027767   \n",
       "away_TOI_pk       0.016105 -0.003241     0.005374     0.051535  -0.027767   \n",
       "away_TOI_pp       0.079296 -0.087827     0.070046     0.019201   0.039837   \n",
       "home_TOI_pk       0.079296 -0.087827     0.070046     0.019201   0.039837   \n",
       "away_xGA/60_pk   -0.018239  0.009349     0.034806     0.042165  -0.004868   \n",
       "home_xGF/60_pp   -0.018239  0.009349     0.034806     0.042165  -0.004868   \n",
       "home_GF/60_pp    -0.009721 -0.003981     0.000627     0.005702  -0.004172   \n",
       "away_GA/60_pk    -0.009721 -0.003981     0.000627     0.005702  -0.004172   \n",
       "home_HDCF/60      0.281645 -0.196734    -0.032229     0.835989  -0.579216   \n",
       "away_HDCA/60      0.281645 -0.196734    -0.032229     0.835989  -0.579216   \n",
       "home_HDSV%       -0.001518 -0.361695     0.070361     0.015850   0.050849   \n",
       "away_FA/60        0.279004 -0.239146    -0.191641     0.770039  -0.652191   \n",
       "home_FF/60        0.279004 -0.239146    -0.191641     0.770039  -0.652191   \n",
       "home_xGF/60       0.338300 -0.253725    -0.055239     1.000000  -0.697641   \n",
       "away_xGA/60       0.338300 -0.253725    -0.055239     1.000000  -0.697641   \n",
       "home_HDSH%        0.550207 -0.345242    -0.007614    -0.036473   0.020997   \n",
       "home_HDCF%        0.234563 -0.294683    -0.595313     0.565717  -0.830989   \n",
       "home_FF%          0.235192 -0.291547    -0.583697     0.607087  -0.833074   \n",
       "home_xGF%         0.275538 -0.344472    -0.719138     0.697641  -1.000000   \n",
       "home_pts_pct      0.174385 -0.207479    -0.139928     0.185113  -0.222432   \n",
       "home_SV%          0.006682 -0.590821    -0.079045     0.036802  -0.082184   \n",
       "home_SH%          0.902947 -0.526348    -0.007808     0.084883  -0.066564   \n",
       "home_GF/60        1.000000 -0.572845    -0.066878     0.338300  -0.275538   \n",
       "away_GA/60        1.000000 -0.572845    -0.066878     0.338300  -0.275538   \n",
       "home_PDO          0.643301 -0.791560    -0.061702     0.086092  -0.105405   \n",
       "home_GF%          0.655932 -0.899743    -0.215864     0.262857  -0.337718   \n",
       "Home_Team_Won     0.471605 -0.599544    -0.196675     0.250745  -0.308746   \n",
       "\n",
       "                away_HDCF/60  away_HDCA/60  away_HDCF%  away_HDSH%  \\\n",
       "away_GF%            0.214650     -0.196734    0.294683    0.394039   \n",
       "away_PDO            0.083431     -0.082468    0.121824    0.437913   \n",
       "home_GA/60          0.274380     -0.080265    0.248288    0.548959   \n",
       "away_GF/60          0.274380     -0.080265    0.248288    0.548959   \n",
       "away_SV%            0.026361     -0.089976    0.083623    0.002944   \n",
       "away_SH%            0.091234     -0.026528    0.088283    0.614051   \n",
       "away_pts_pct        0.112092     -0.099663    0.148598    0.044664   \n",
       "away_xGF%           0.588725     -0.579216    0.830989   -0.011762   \n",
       "away_FF%            0.443593     -0.457646    0.622474   -0.023228   \n",
       "away_HDCF%          0.684735     -0.649944    1.000000   -0.019779   \n",
       "away_HDSH%         -0.054165     -0.021528   -0.019779    1.000000   \n",
       "away_HDSV%          0.021349      0.121864   -0.093704   -0.002607   \n",
       "home_FA/60          0.563438     -0.165019    0.500645   -0.036177   \n",
       "away_FF/60          0.563438     -0.165019    0.500645   -0.036177   \n",
       "away_xGF/60         0.831560     -0.032229    0.595313   -0.037498   \n",
       "home_xGA/60         0.831560     -0.032229    0.595313   -0.037498   \n",
       "home_HDCA/60        1.000000      0.014210    0.684735   -0.054165   \n",
       "away_HDCF/60        1.000000      0.014210    0.684735   -0.054165   \n",
       "home_GA/60_pk      -0.013031     -0.018943   -0.000671   -0.006306   \n",
       "away_GF/60_pp      -0.013031     -0.018943   -0.000671   -0.006306   \n",
       "away_xGF/60_pp      0.021115     -0.008252    0.020263   -0.015768   \n",
       "home_xGA/60_pk      0.021115     -0.008252    0.020263   -0.015768   \n",
       "home_TOI           -0.037430     -0.033917   -0.004665    0.034530   \n",
       "away_TOI           -0.037430     -0.033917   -0.004665    0.034530   \n",
       "home_TOI_pp         0.010195      0.025999   -0.013246   -0.011768   \n",
       "away_TOI_pk         0.010195      0.025999   -0.013246   -0.011768   \n",
       "away_TOI_pp         0.043808      0.000505    0.034001   -0.055291   \n",
       "home_TOI_pk         0.043808      0.000505    0.034001   -0.055291   \n",
       "away_xGA/60_pk      0.038420      0.034069   -0.002292    0.003580   \n",
       "home_xGF/60_pp      0.038420      0.034069   -0.002292    0.003580   \n",
       "home_GF/60_pp       0.013524      0.009156   -0.002395    0.027728   \n",
       "away_GA/60_pk       0.013524      0.009156   -0.002395    0.027728   \n",
       "home_HDCF/60        0.014210      1.000000   -0.649944   -0.021528   \n",
       "away_HDCA/60        0.014210      1.000000   -0.649944   -0.021528   \n",
       "home_HDSV%          0.093310      0.018593    0.074433   -0.944957   \n",
       "away_FA/60         -0.152971      0.578480   -0.485419   -0.005284   \n",
       "home_FF/60         -0.152971      0.578480   -0.485419   -0.005284   \n",
       "home_xGF/60        -0.026902      0.835989   -0.565717   -0.029210   \n",
       "away_xGA/60        -0.026902      0.835989   -0.565717   -0.029210   \n",
       "home_HDSH%         -0.020650     -0.070292    0.036509   -0.010183   \n",
       "home_HDCF%         -0.684735      0.649944   -1.000000    0.019779   \n",
       "home_FF%           -0.443593      0.457646   -0.622474    0.023228   \n",
       "home_xGF%          -0.588725      0.579216   -0.830989    0.011762   \n",
       "home_pts_pct       -0.090921      0.148468   -0.156697   -0.045957   \n",
       "home_SV%           -0.091234      0.026528   -0.088283   -0.614051   \n",
       "home_SH%           -0.026361      0.089976   -0.083623   -0.002944   \n",
       "home_GF/60         -0.065001      0.281645   -0.234563   -0.001716   \n",
       "away_GA/60         -0.065001      0.281645   -0.234563   -0.001716   \n",
       "home_PDO           -0.083431      0.082468   -0.121824   -0.437913   \n",
       "home_GF%           -0.194106      0.215166   -0.290364   -0.334971   \n",
       "Home_Team_Won      -0.163710      0.214950   -0.261105   -0.248747   \n",
       "\n",
       "                away_HDSV%  away_SH%  away_SV%  away_PDO  away_TOI_pp  \\\n",
       "away_GF%          0.307241  0.590821  0.526348  0.791560    -0.087827   \n",
       "away_PDO          0.406268  0.707018  0.704524  1.000000    -0.105642   \n",
       "home_GA/60       -0.021792  0.904281 -0.004810  0.638375    -0.052826   \n",
       "away_GF/60       -0.021792  0.904281 -0.004810  0.638375    -0.052826   \n",
       "away_SV%          0.583800 -0.003759  1.000000  0.704524    -0.078463   \n",
       "away_SH%         -0.009304  1.000000 -0.003759  0.707018    -0.070599   \n",
       "away_pts_pct      0.098652  0.097904  0.147067  0.173449    -0.001900   \n",
       "away_xGF%        -0.055970  0.082184  0.066564  0.105405     0.039837   \n",
       "away_FF%         -0.065816 -0.009484 -0.017470 -0.019134     0.028449   \n",
       "away_HDCF%       -0.093704  0.088283  0.083623  0.121824     0.034001   \n",
       "away_HDSH%       -0.002607  0.614051  0.002944  0.437913    -0.055291   \n",
       "away_HDSV%        1.000000 -0.009304  0.583800  0.406268    -0.079490   \n",
       "home_FA/60       -0.026069 -0.043672  0.016086 -0.019610     0.057308   \n",
       "away_FF/60       -0.026069 -0.043672  0.016086 -0.019610     0.057308   \n",
       "away_xGF/60       0.002049  0.079045  0.007808  0.061702     0.070046   \n",
       "home_xGA/60       0.002049  0.079045  0.007808  0.061702     0.070046   \n",
       "home_HDCA/60      0.021349  0.091234  0.026361  0.083431     0.043808   \n",
       "away_HDCF/60      0.021349  0.091234  0.026361  0.083431     0.043808   \n",
       "home_GA/60_pk     0.005527 -0.003235  0.000561 -0.001989    -0.228760   \n",
       "away_GF/60_pp     0.005527 -0.003235  0.000561 -0.001989    -0.228760   \n",
       "away_xGF/60_pp    0.008312 -0.020478 -0.007086 -0.019569    -0.118314   \n",
       "home_xGA/60_pk    0.008312 -0.020478 -0.007086 -0.019569    -0.118314   \n",
       "home_TOI          0.042804  0.024981  0.042498  0.047804    -0.660751   \n",
       "away_TOI          0.042804  0.024981  0.042498  0.047804    -0.660751   \n",
       "home_TOI_pp       0.000943  0.017539  0.003592  0.014956     0.130937   \n",
       "away_TOI_pk       0.000943  0.017539  0.003592  0.014956     0.130937   \n",
       "away_TOI_pp      -0.079490 -0.070599 -0.078463 -0.105642     1.000000   \n",
       "home_TOI_pk      -0.079490 -0.070599 -0.078463 -0.105642     1.000000   \n",
       "away_xGA/60_pk    0.010185  0.009624  0.012812  0.015955     0.027099   \n",
       "home_xGF/60_pp    0.010185  0.009624  0.012812  0.015955     0.027099   \n",
       "home_GF/60_pp     0.022214  0.016585 -0.003903  0.009004     0.050402   \n",
       "away_GA/60_pk     0.022214  0.016585 -0.003903  0.009004     0.050402   \n",
       "home_HDCF/60      0.121864 -0.026528 -0.089976 -0.082468     0.000505   \n",
       "away_HDCA/60      0.121864 -0.026528 -0.089976 -0.082468     0.000505   \n",
       "home_HDSV%        0.005707 -0.583998 -0.006435 -0.419038     0.052369   \n",
       "away_FA/60        0.071520 -0.032620  0.035409  0.001971     0.015539   \n",
       "home_FF/60        0.071520 -0.032620  0.035409  0.001971     0.015539   \n",
       "home_xGF/60       0.071869 -0.036802 -0.084883 -0.086092     0.019201   \n",
       "away_xGA/60       0.071869 -0.036802 -0.084883 -0.086092     0.019201   \n",
       "home_HDSH%       -0.922278 -0.007515 -0.622819 -0.445838     0.061540   \n",
       "home_HDCF%        0.093704 -0.088283 -0.083623 -0.121824    -0.034001   \n",
       "home_FF%          0.065816  0.009484  0.017470  0.019134    -0.028449   \n",
       "home_xGF%         0.055970 -0.082184 -0.066564 -0.105405    -0.039837   \n",
       "home_pts_pct     -0.075734 -0.091787 -0.117491 -0.148274     0.050210   \n",
       "home_SV%          0.009304 -1.000000  0.003759 -0.707018     0.070599   \n",
       "home_SH%         -0.583800  0.003759 -1.000000 -0.704524     0.078463   \n",
       "home_GF/60       -0.509909 -0.006682 -0.902947 -0.643301     0.079296   \n",
       "away_GA/60       -0.509909 -0.006682 -0.902947 -0.643301     0.079296   \n",
       "home_PDO         -0.406268 -0.707018 -0.704524 -1.000000     0.105642   \n",
       "home_GF%         -0.367419 -0.506695 -0.611950 -0.792407     0.076387   \n",
       "Home_Team_Won    -0.228224 -0.391314 -0.410017 -0.567670     0.074819   \n",
       "\n",
       "                away_GF/60_pp  away_xGF/60_pp  away_TOI_pk  away_GA/60_pk  \\\n",
       "away_GF%            -0.008636       -0.011899    -0.003241      -0.003981   \n",
       "away_PDO            -0.001989       -0.019569     0.014956       0.009004   \n",
       "home_GA/60          -0.004895       -0.019836     0.024202       0.000321   \n",
       "away_GF/60          -0.004895       -0.019836     0.024202       0.000321   \n",
       "away_SV%             0.000561       -0.007086     0.003592      -0.003903   \n",
       "away_SH%            -0.003235       -0.020478     0.017539       0.016585   \n",
       "away_pts_pct         0.056362        0.073780     0.006442      -0.045646   \n",
       "away_xGF%           -0.011859        0.019930    -0.027767      -0.004172   \n",
       "away_FF%             0.003071        0.035192    -0.030603      -0.010545   \n",
       "away_HDCF%          -0.000671        0.020263    -0.013246      -0.002395   \n",
       "away_HDSH%          -0.006306       -0.015768    -0.011768       0.027728   \n",
       "away_HDSV%           0.005527        0.008312     0.000943       0.022214   \n",
       "home_FA/60          -0.010184        0.028537    -0.000832      -0.021837   \n",
       "away_FF/60          -0.010184        0.028537    -0.000832      -0.021837   \n",
       "away_xGF/60         -0.022791        0.029531     0.005374       0.000627   \n",
       "home_xGA/60         -0.022791        0.029531     0.005374       0.000627   \n",
       "home_HDCA/60        -0.013031        0.021115     0.010195       0.013524   \n",
       "away_HDCF/60        -0.013031        0.021115     0.010195       0.013524   \n",
       "home_GA/60_pk        1.000000        0.644702    -0.008089      -0.001370   \n",
       "away_GF/60_pp        1.000000        0.644702    -0.008089      -0.001370   \n",
       "away_xGF/60_pp       0.644702        1.000000     0.012004       0.025009   \n",
       "home_xGA/60_pk       0.644702        1.000000     0.012004       0.025009   \n",
       "home_TOI             0.152379        0.058185    -0.691602       0.123190   \n",
       "away_TOI             0.152379        0.058185    -0.691602       0.123190   \n",
       "home_TOI_pp         -0.008089        0.012004     1.000000      -0.241019   \n",
       "away_TOI_pk         -0.008089        0.012004     1.000000      -0.241019   \n",
       "away_TOI_pp         -0.228760       -0.118314     0.130937       0.050402   \n",
       "home_TOI_pk         -0.228760       -0.118314     0.130937       0.050402   \n",
       "away_xGA/60_pk      -0.003956        0.025557    -0.096287       0.482805   \n",
       "home_xGF/60_pp      -0.003956        0.025557    -0.096287       0.482805   \n",
       "home_GF/60_pp       -0.001370        0.025009    -0.241019       1.000000   \n",
       "away_GA/60_pk       -0.001370        0.025009    -0.241019       1.000000   \n",
       "home_HDCF/60        -0.018943       -0.008252     0.025999       0.009156   \n",
       "away_HDCA/60        -0.018943       -0.008252     0.025999       0.009156   \n",
       "home_HDSV%           0.005220        0.012595    -0.000474      -0.027889   \n",
       "away_FA/60          -0.017723       -0.032859     0.050635      -0.002858   \n",
       "home_FF/60          -0.017723       -0.032859     0.050635      -0.002858   \n",
       "home_xGF/60         -0.009513        0.000068     0.051535       0.005702   \n",
       "away_xGA/60         -0.009513        0.000068     0.051535       0.005702   \n",
       "home_HDSH%          -0.001016       -0.006420     0.001825      -0.020554   \n",
       "home_HDCF%           0.000671       -0.020263     0.013246       0.002395   \n",
       "home_FF%            -0.003071       -0.035192     0.030603       0.010545   \n",
       "home_xGF%            0.011859       -0.019930     0.027767       0.004172   \n",
       "home_pts_pct        -0.035844       -0.054559     0.020268       0.076586   \n",
       "home_SV%             0.003235        0.020478    -0.017539      -0.016585   \n",
       "home_SH%            -0.000561        0.007086    -0.003592       0.003903   \n",
       "home_GF/60          -0.006728       -0.001534     0.016105      -0.009721   \n",
       "away_GA/60          -0.006728       -0.001534     0.016105      -0.009721   \n",
       "home_PDO             0.001989        0.019569    -0.014956      -0.009004   \n",
       "home_GF%             0.013607        0.016294    -0.018453       0.015144   \n",
       "Home_Team_Won       -0.069168       -0.045863    -0.026110       0.165618   \n",
       "\n",
       "                away_xGA/60_pk  \n",
       "away_GF%              0.009349  \n",
       "away_PDO              0.015955  \n",
       "home_GA/60            0.000566  \n",
       "away_GF/60            0.000566  \n",
       "away_SV%              0.012812  \n",
       "away_SH%              0.009624  \n",
       "away_pts_pct         -0.046846  \n",
       "away_xGF%            -0.004868  \n",
       "away_FF%             -0.026725  \n",
       "away_HDCF%           -0.002292  \n",
       "away_HDSH%            0.003580  \n",
       "away_HDSV%            0.010185  \n",
       "home_FA/60           -0.022549  \n",
       "away_FF/60           -0.022549  \n",
       "away_xGF/60           0.034806  \n",
       "home_xGA/60           0.034806  \n",
       "home_HDCA/60          0.038420  \n",
       "away_HDCF/60          0.038420  \n",
       "home_GA/60_pk        -0.003956  \n",
       "away_GF/60_pp        -0.003956  \n",
       "away_xGF/60_pp        0.025557  \n",
       "home_xGA/60_pk        0.025557  \n",
       "home_TOI              0.040873  \n",
       "away_TOI              0.040873  \n",
       "home_TOI_pp          -0.096287  \n",
       "away_TOI_pk          -0.096287  \n",
       "away_TOI_pp           0.027099  \n",
       "home_TOI_pk           0.027099  \n",
       "away_xGA/60_pk        1.000000  \n",
       "home_xGF/60_pp        1.000000  \n",
       "home_GF/60_pp         0.482805  \n",
       "away_GA/60_pk         0.482805  \n",
       "home_HDCF/60          0.034069  \n",
       "away_HDCA/60          0.034069  \n",
       "home_HDSV%           -0.003153  \n",
       "away_FA/60            0.023092  \n",
       "home_FF/60            0.023092  \n",
       "home_xGF/60           0.042165  \n",
       "away_xGA/60           0.042165  \n",
       "home_HDSH%           -0.007027  \n",
       "home_HDCF%            0.002292  \n",
       "home_FF%              0.026725  \n",
       "home_xGF%             0.004868  \n",
       "home_pts_pct          0.061477  \n",
       "home_SV%             -0.009624  \n",
       "home_SH%             -0.012812  \n",
       "home_GF/60           -0.018239  \n",
       "away_GA/60           -0.018239  \n",
       "home_PDO             -0.015955  \n",
       "home_GF%              0.001134  \n",
       "Home_Team_Won         0.083060  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_df = modeling_df.corr().sort_values('Home_Team_Won') # ignore season_id\n",
    "corr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6854cbc8",
   "metadata": {},
   "source": [
    "- there are a lot of mirrored values as we have the home and away perspectictive\n",
    "- which do we delete?\n",
    "- isolate away variation for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3f9af8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method IndexOpsMixin.tolist of Index(['away_GF%', 'away_PDO', 'home_GA/60', 'away_GF/60', 'away_SV%',\n",
      "       'away_SH%', 'away_pts_pct', 'away_xGF%', 'away_FF%', 'away_HDCF%',\n",
      "       'away_HDSH%', 'away_HDSV%', 'home_FA/60', 'away_FF/60', 'away_xGF/60',\n",
      "       'home_xGA/60', 'home_HDCA/60', 'away_HDCF/60', 'home_GA/60_pk',\n",
      "       'away_GF/60_pp', 'away_xGF/60_pp', 'home_xGA/60_pk', 'home_TOI',\n",
      "       'away_TOI', 'home_TOI_pp', 'away_TOI_pk', 'away_TOI_pp', 'home_TOI_pk',\n",
      "       'away_xGA/60_pk', 'home_xGF/60_pp', 'home_GF/60_pp', 'away_GA/60_pk',\n",
      "       'home_HDCF/60', 'away_HDCA/60', 'home_HDSV%', 'away_FA/60',\n",
      "       'home_FF/60', 'home_xGF/60', 'away_xGA/60', 'home_HDSH%', 'home_HDCF%',\n",
      "       'home_FF%', 'home_xGF%', 'home_pts_pct', 'home_SV%', 'home_SH%',\n",
      "       'home_GF/60', 'away_GA/60', 'home_PDO', 'home_GF%', 'Home_Team_Won'],\n",
      "      dtype='object')>\n"
     ]
    }
   ],
   "source": [
    "print(corr_df.index.to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "fde96ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Home_Team_Won</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>away_GF%</th>\n",
       "      <td>-0.599544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_PDO</th>\n",
       "      <td>-0.567670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_GA/60</th>\n",
       "      <td>-0.455510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_GF/60</th>\n",
       "      <td>-0.455510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_SV%</th>\n",
       "      <td>-0.410017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_SH%</th>\n",
       "      <td>-0.391314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_pts_pct</th>\n",
       "      <td>-0.309084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_xGF%</th>\n",
       "      <td>-0.308746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_FF%</th>\n",
       "      <td>-0.275023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_HDCF%</th>\n",
       "      <td>-0.261105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_HDSH%</th>\n",
       "      <td>-0.248747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_HDSV%</th>\n",
       "      <td>-0.228224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_FA/60</th>\n",
       "      <td>-0.219077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_FF/60</th>\n",
       "      <td>-0.219077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_xGF/60</th>\n",
       "      <td>-0.196675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_xGA/60</th>\n",
       "      <td>-0.196675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_HDCA/60</th>\n",
       "      <td>-0.163710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_HDCF/60</th>\n",
       "      <td>-0.163710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_GA/60_pk</th>\n",
       "      <td>-0.069168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_GF/60_pp</th>\n",
       "      <td>-0.069168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_xGF/60_pp</th>\n",
       "      <td>-0.045863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_xGA/60_pk</th>\n",
       "      <td>-0.045863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_TOI</th>\n",
       "      <td>-0.030639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_TOI</th>\n",
       "      <td>-0.030639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_TOI_pp</th>\n",
       "      <td>-0.026110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_TOI_pk</th>\n",
       "      <td>-0.026110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_TOI_pp</th>\n",
       "      <td>0.074819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_TOI_pk</th>\n",
       "      <td>0.074819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_xGA/60_pk</th>\n",
       "      <td>0.083060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_xGF/60_pp</th>\n",
       "      <td>0.083060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_GF/60_pp</th>\n",
       "      <td>0.165618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_GA/60_pk</th>\n",
       "      <td>0.165618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_HDCF/60</th>\n",
       "      <td>0.214950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_HDCA/60</th>\n",
       "      <td>0.214950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_HDSV%</th>\n",
       "      <td>0.224793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_FA/60</th>\n",
       "      <td>0.227636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_FF/60</th>\n",
       "      <td>0.227636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_xGF/60</th>\n",
       "      <td>0.250745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_xGA/60</th>\n",
       "      <td>0.250745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_HDSH%</th>\n",
       "      <td>0.257639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_HDCF%</th>\n",
       "      <td>0.261105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_FF%</th>\n",
       "      <td>0.275023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_xGF%</th>\n",
       "      <td>0.308746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_pts_pct</th>\n",
       "      <td>0.311811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_SV%</th>\n",
       "      <td>0.391314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_SH%</th>\n",
       "      <td>0.410017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_GF/60</th>\n",
       "      <td>0.471605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away_GA/60</th>\n",
       "      <td>0.471605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_PDO</th>\n",
       "      <td>0.567670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_GF%</th>\n",
       "      <td>0.596114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Home_Team_Won</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Home_Team_Won\n",
       "away_GF%            -0.599544\n",
       "away_PDO            -0.567670\n",
       "home_GA/60          -0.455510\n",
       "away_GF/60          -0.455510\n",
       "away_SV%            -0.410017\n",
       "away_SH%            -0.391314\n",
       "away_pts_pct        -0.309084\n",
       "away_xGF%           -0.308746\n",
       "away_FF%            -0.275023\n",
       "away_HDCF%          -0.261105\n",
       "away_HDSH%          -0.248747\n",
       "away_HDSV%          -0.228224\n",
       "home_FA/60          -0.219077\n",
       "away_FF/60          -0.219077\n",
       "away_xGF/60         -0.196675\n",
       "home_xGA/60         -0.196675\n",
       "home_HDCA/60        -0.163710\n",
       "away_HDCF/60        -0.163710\n",
       "home_GA/60_pk       -0.069168\n",
       "away_GF/60_pp       -0.069168\n",
       "away_xGF/60_pp      -0.045863\n",
       "home_xGA/60_pk      -0.045863\n",
       "home_TOI            -0.030639\n",
       "away_TOI            -0.030639\n",
       "home_TOI_pp         -0.026110\n",
       "away_TOI_pk         -0.026110\n",
       "away_TOI_pp          0.074819\n",
       "home_TOI_pk          0.074819\n",
       "away_xGA/60_pk       0.083060\n",
       "home_xGF/60_pp       0.083060\n",
       "home_GF/60_pp        0.165618\n",
       "away_GA/60_pk        0.165618\n",
       "home_HDCF/60         0.214950\n",
       "away_HDCA/60         0.214950\n",
       "home_HDSV%           0.224793\n",
       "away_FA/60           0.227636\n",
       "home_FF/60           0.227636\n",
       "home_xGF/60          0.250745\n",
       "away_xGA/60          0.250745\n",
       "home_HDSH%           0.257639\n",
       "home_HDCF%           0.261105\n",
       "home_FF%             0.275023\n",
       "home_xGF%            0.308746\n",
       "home_pts_pct         0.311811\n",
       "home_SV%             0.391314\n",
       "home_SH%             0.410017\n",
       "home_GF/60           0.471605\n",
       "away_GA/60           0.471605\n",
       "home_PDO             0.567670\n",
       "home_GF%             0.596114\n",
       "Home_Team_Won        1.000000"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mirrored_stats = ['away_GF/60', 'away_FF/60', 'away_xGF/60', 'away_HDCF/60', 'away_GF/60_pp', 'away_xGF/60_pp', \n",
    "                 'away_TOI', 'away_TOI_pk', 'away_TOI_pp', 'away_xGA/60_pk', 'away_GA/60_pk', 'away_HDCA/60',\n",
    "                  'away_FA/60', 'away_xGA/60','away_GA/60']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "437566a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>home_PDO</th>\n",
       "      <th>away_PDO</th>\n",
       "      <th>home_GF%</th>\n",
       "      <th>away_GF%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.975</td>\n",
       "      <td>1.025</td>\n",
       "      <td>55.27</td>\n",
       "      <td>44.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.014</td>\n",
       "      <td>0.986</td>\n",
       "      <td>48.19</td>\n",
       "      <td>51.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.062</td>\n",
       "      <td>0.938</td>\n",
       "      <td>57.61</td>\n",
       "      <td>42.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.013</td>\n",
       "      <td>0.987</td>\n",
       "      <td>66.09</td>\n",
       "      <td>33.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.064</td>\n",
       "      <td>0.936</td>\n",
       "      <td>65.64</td>\n",
       "      <td>34.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3257</th>\n",
       "      <td>1.008</td>\n",
       "      <td>0.992</td>\n",
       "      <td>47.45</td>\n",
       "      <td>52.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>1.003</td>\n",
       "      <td>0.997</td>\n",
       "      <td>46.65</td>\n",
       "      <td>53.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>1.068</td>\n",
       "      <td>0.932</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>1.087</td>\n",
       "      <td>0.913</td>\n",
       "      <td>52.18</td>\n",
       "      <td>47.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>1.071</td>\n",
       "      <td>0.929</td>\n",
       "      <td>63.82</td>\n",
       "      <td>36.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3262 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      home_PDO  away_PDO  home_GF%  away_GF%\n",
       "0        0.975     1.025     55.27     44.73\n",
       "1        1.014     0.986     48.19     51.81\n",
       "2        1.062     0.938     57.61     42.39\n",
       "3        1.013     0.987     66.09     33.91\n",
       "4        1.064     0.936     65.64     34.36\n",
       "...        ...       ...       ...       ...\n",
       "3257     1.008     0.992     47.45     52.55\n",
       "3258     1.003     0.997     46.65     53.35\n",
       "3259     1.068     0.932    100.00      0.00\n",
       "3260     1.087     0.913     52.18     47.82\n",
       "3261     1.071     0.929     63.82     36.18\n",
       "\n",
       "[3262 rows x 4 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# home_PDO and home_GF% are the absolute value of the away variation\n",
    "# drill down to examine further\n",
    "abs_fields = modeling_df[['home_PDO', 'away_PDO', 'home_GF%', 'away_GF%']]\n",
    "abs_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3c8de5",
   "metadata": {},
   "source": [
    "PDO:\n",
    "- home and away PDO always sum to 2, but are not equal\n",
    "- contains info about actual goals and shots for and against\n",
    "- this is clearly useful information to capture, but given the fixed relationship we might not need both\n",
    "\n",
    "GF%\n",
    "- home and away values are inverses of each other\n",
    "- will look to handle similarly to PDO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9754f552",
   "metadata": {},
   "source": [
    "## Baseline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efac59a4",
   "metadata": {},
   "source": [
    "### Naive Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07226999",
   "metadata": {},
   "source": [
    "Choose the home team to win every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb0fe8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db2afa1a",
   "metadata": {},
   "source": [
    "### Basic Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "846aaa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 2019-2020 and 2020-2021 for training\n",
    "# Use 2021-2022 for test set \n",
    "X_train = modeling_df.loc[modeling_df['season_id'] != '20212022'].drop(['season_id', 'Home_Team_Won'], axis=1)\n",
    "y_train = modeling_df.loc[modeling_df['season_id'] != '20212022']['Home_Team_Won']\n",
    "X_test = modeling_df.loc[modeling_df['season_id'] == '20212022'].drop(['season_id', 'Home_Team_Won'], axis=1)\n",
    "y_test = modeling_df.loc[modeling_df['season_id'] == '20212022']['Home_Team_Won']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a22580e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1950, 50)\n",
      "(1950,)\n",
      "(1312, 50)\n",
      "(1312,)\n"
     ]
    }
   ],
   "source": [
    "# Check df shapes\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape) \n",
    "# shape align, but the split is very close to 50/50 because of covid -  might need to reach back one more year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "42c1a521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>home_pts_pct</th>\n",
       "      <th>away_pts_pct</th>\n",
       "      <th>home_TOI</th>\n",
       "      <th>home_FF/60</th>\n",
       "      <th>home_FA/60</th>\n",
       "      <th>home_FF%</th>\n",
       "      <th>home_GF/60</th>\n",
       "      <th>home_GA/60</th>\n",
       "      <th>home_GF%</th>\n",
       "      <th>home_xGF/60</th>\n",
       "      <th>home_xGA/60</th>\n",
       "      <th>home_xGF%</th>\n",
       "      <th>home_HDCF/60</th>\n",
       "      <th>home_HDCA/60</th>\n",
       "      <th>home_HDCF%</th>\n",
       "      <th>home_HDSH%</th>\n",
       "      <th>home_HDSV%</th>\n",
       "      <th>home_SH%</th>\n",
       "      <th>home_SV%</th>\n",
       "      <th>home_PDO</th>\n",
       "      <th>home_TOI_pp</th>\n",
       "      <th>home_GF/60_pp</th>\n",
       "      <th>home_xGF/60_pp</th>\n",
       "      <th>home_TOI_pk</th>\n",
       "      <th>home_GA/60_pk</th>\n",
       "      <th>home_xGA/60_pk</th>\n",
       "      <th>away_TOI</th>\n",
       "      <th>away_FF/60</th>\n",
       "      <th>away_FA/60</th>\n",
       "      <th>away_FF%</th>\n",
       "      <th>away_GF/60</th>\n",
       "      <th>away_GA/60</th>\n",
       "      <th>away_GF%</th>\n",
       "      <th>away_xGF/60</th>\n",
       "      <th>away_xGA/60</th>\n",
       "      <th>away_xGF%</th>\n",
       "      <th>away_HDCF/60</th>\n",
       "      <th>away_HDCA/60</th>\n",
       "      <th>away_HDCF%</th>\n",
       "      <th>away_HDSH%</th>\n",
       "      <th>away_HDSV%</th>\n",
       "      <th>away_SH%</th>\n",
       "      <th>away_SV%</th>\n",
       "      <th>away_PDO</th>\n",
       "      <th>away_TOI_pp</th>\n",
       "      <th>away_GF/60_pp</th>\n",
       "      <th>away_xGF/60_pp</th>\n",
       "      <th>away_TOI_pk</th>\n",
       "      <th>away_GA/60_pk</th>\n",
       "      <th>away_xGA/60_pk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>53.900000</td>\n",
       "      <td>32.64</td>\n",
       "      <td>53.24</td>\n",
       "      <td>38.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.32</td>\n",
       "      <td>3.08</td>\n",
       "      <td>30.01</td>\n",
       "      <td>3.87</td>\n",
       "      <td>14.71</td>\n",
       "      <td>20.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>80.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>90.78</td>\n",
       "      <td>0.908</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.50</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>53.900000</td>\n",
       "      <td>53.24</td>\n",
       "      <td>32.64</td>\n",
       "      <td>62.00</td>\n",
       "      <td>3.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>3.08</td>\n",
       "      <td>1.32</td>\n",
       "      <td>69.99</td>\n",
       "      <td>14.71</td>\n",
       "      <td>3.87</td>\n",
       "      <td>79.17</td>\n",
       "      <td>19.62</td>\n",
       "      <td>100.00</td>\n",
       "      <td>9.22</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.092</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.716667</td>\n",
       "      <td>45.29</td>\n",
       "      <td>48.69</td>\n",
       "      <td>48.19</td>\n",
       "      <td>5.07</td>\n",
       "      <td>3.98</td>\n",
       "      <td>56.03</td>\n",
       "      <td>2.96</td>\n",
       "      <td>2.74</td>\n",
       "      <td>51.95</td>\n",
       "      <td>10.43</td>\n",
       "      <td>14.41</td>\n",
       "      <td>41.98</td>\n",
       "      <td>41.18</td>\n",
       "      <td>58.00</td>\n",
       "      <td>17.65</td>\n",
       "      <td>87.78</td>\n",
       "      <td>1.054</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.34</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.44</td>\n",
       "      <td>45.716667</td>\n",
       "      <td>48.69</td>\n",
       "      <td>45.29</td>\n",
       "      <td>51.81</td>\n",
       "      <td>3.98</td>\n",
       "      <td>5.07</td>\n",
       "      <td>43.97</td>\n",
       "      <td>2.74</td>\n",
       "      <td>2.96</td>\n",
       "      <td>48.05</td>\n",
       "      <td>14.41</td>\n",
       "      <td>10.43</td>\n",
       "      <td>58.02</td>\n",
       "      <td>42.00</td>\n",
       "      <td>58.82</td>\n",
       "      <td>12.22</td>\n",
       "      <td>82.35</td>\n",
       "      <td>0.946</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.44</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.966667</td>\n",
       "      <td>52.57</td>\n",
       "      <td>39.43</td>\n",
       "      <td>57.14</td>\n",
       "      <td>1.18</td>\n",
       "      <td>1.32</td>\n",
       "      <td>47.18</td>\n",
       "      <td>3.38</td>\n",
       "      <td>2.28</td>\n",
       "      <td>59.66</td>\n",
       "      <td>17.55</td>\n",
       "      <td>9.25</td>\n",
       "      <td>65.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>83.09</td>\n",
       "      <td>4.01</td>\n",
       "      <td>95.83</td>\n",
       "      <td>0.998</td>\n",
       "      <td>4.483333</td>\n",
       "      <td>13.38</td>\n",
       "      <td>10.26</td>\n",
       "      <td>4.533333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.10</td>\n",
       "      <td>47.966667</td>\n",
       "      <td>39.43</td>\n",
       "      <td>52.57</td>\n",
       "      <td>42.86</td>\n",
       "      <td>1.32</td>\n",
       "      <td>1.18</td>\n",
       "      <td>52.82</td>\n",
       "      <td>2.28</td>\n",
       "      <td>3.38</td>\n",
       "      <td>40.34</td>\n",
       "      <td>9.25</td>\n",
       "      <td>17.55</td>\n",
       "      <td>34.51</td>\n",
       "      <td>16.91</td>\n",
       "      <td>100.00</td>\n",
       "      <td>4.17</td>\n",
       "      <td>95.99</td>\n",
       "      <td>1.002</td>\n",
       "      <td>4.533333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.10</td>\n",
       "      <td>4.483333</td>\n",
       "      <td>13.38</td>\n",
       "      <td>10.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.500000</td>\n",
       "      <td>40.03</td>\n",
       "      <td>26.81</td>\n",
       "      <td>59.89</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.64</td>\n",
       "      <td>51.44</td>\n",
       "      <td>4.27</td>\n",
       "      <td>1.42</td>\n",
       "      <td>75.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>5.52</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.055</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>28.57</td>\n",
       "      <td>6.45</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>8.70</td>\n",
       "      <td>12.10</td>\n",
       "      <td>42.500000</td>\n",
       "      <td>26.81</td>\n",
       "      <td>40.03</td>\n",
       "      <td>40.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>48.56</td>\n",
       "      <td>1.42</td>\n",
       "      <td>4.27</td>\n",
       "      <td>24.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>94.48</td>\n",
       "      <td>0.945</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>8.70</td>\n",
       "      <td>12.10</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>28.57</td>\n",
       "      <td>6.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.533333</td>\n",
       "      <td>28.64</td>\n",
       "      <td>43.96</td>\n",
       "      <td>39.44</td>\n",
       "      <td>2.93</td>\n",
       "      <td>1.54</td>\n",
       "      <td>65.57</td>\n",
       "      <td>1.83</td>\n",
       "      <td>2.38</td>\n",
       "      <td>43.39</td>\n",
       "      <td>10.54</td>\n",
       "      <td>6.01</td>\n",
       "      <td>63.69</td>\n",
       "      <td>19.56</td>\n",
       "      <td>100.00</td>\n",
       "      <td>15.28</td>\n",
       "      <td>94.28</td>\n",
       "      <td>1.096</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>16.67</td>\n",
       "      <td>13.03</td>\n",
       "      <td>8.750000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.88</td>\n",
       "      <td>39.533333</td>\n",
       "      <td>43.96</td>\n",
       "      <td>28.64</td>\n",
       "      <td>60.56</td>\n",
       "      <td>1.54</td>\n",
       "      <td>2.93</td>\n",
       "      <td>34.43</td>\n",
       "      <td>2.38</td>\n",
       "      <td>1.83</td>\n",
       "      <td>56.61</td>\n",
       "      <td>6.01</td>\n",
       "      <td>10.54</td>\n",
       "      <td>36.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>80.44</td>\n",
       "      <td>5.72</td>\n",
       "      <td>84.72</td>\n",
       "      <td>0.904</td>\n",
       "      <td>8.750000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.88</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>16.67</td>\n",
       "      <td>13.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3257</th>\n",
       "      <td>0.597561</td>\n",
       "      <td>0.463415</td>\n",
       "      <td>54.450000</td>\n",
       "      <td>32.22</td>\n",
       "      <td>30.41</td>\n",
       "      <td>51.44</td>\n",
       "      <td>2.06</td>\n",
       "      <td>2.28</td>\n",
       "      <td>47.45</td>\n",
       "      <td>2.21</td>\n",
       "      <td>1.37</td>\n",
       "      <td>61.63</td>\n",
       "      <td>11.32</td>\n",
       "      <td>5.80</td>\n",
       "      <td>66.12</td>\n",
       "      <td>51.77</td>\n",
       "      <td>50.19</td>\n",
       "      <td>12.11</td>\n",
       "      <td>88.68</td>\n",
       "      <td>1.008</td>\n",
       "      <td>2.566667</td>\n",
       "      <td>23.38</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.57</td>\n",
       "      <td>54.450000</td>\n",
       "      <td>30.41</td>\n",
       "      <td>32.22</td>\n",
       "      <td>48.56</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.06</td>\n",
       "      <td>52.55</td>\n",
       "      <td>1.37</td>\n",
       "      <td>2.21</td>\n",
       "      <td>38.37</td>\n",
       "      <td>5.80</td>\n",
       "      <td>11.32</td>\n",
       "      <td>33.88</td>\n",
       "      <td>49.81</td>\n",
       "      <td>48.23</td>\n",
       "      <td>11.32</td>\n",
       "      <td>87.89</td>\n",
       "      <td>0.992</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.57</td>\n",
       "      <td>2.566667</td>\n",
       "      <td>23.38</td>\n",
       "      <td>1.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>53.783333</td>\n",
       "      <td>39.31</td>\n",
       "      <td>51.30</td>\n",
       "      <td>43.38</td>\n",
       "      <td>2.06</td>\n",
       "      <td>2.36</td>\n",
       "      <td>46.65</td>\n",
       "      <td>2.56</td>\n",
       "      <td>3.93</td>\n",
       "      <td>39.47</td>\n",
       "      <td>13.96</td>\n",
       "      <td>12.29</td>\n",
       "      <td>53.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>7.13</td>\n",
       "      <td>93.16</td>\n",
       "      <td>1.003</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.50</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.36</td>\n",
       "      <td>53.783333</td>\n",
       "      <td>51.30</td>\n",
       "      <td>39.31</td>\n",
       "      <td>56.62</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.06</td>\n",
       "      <td>53.35</td>\n",
       "      <td>3.93</td>\n",
       "      <td>2.56</td>\n",
       "      <td>60.53</td>\n",
       "      <td>12.29</td>\n",
       "      <td>13.96</td>\n",
       "      <td>46.81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>6.84</td>\n",
       "      <td>92.87</td>\n",
       "      <td>0.997</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.36</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.469512</td>\n",
       "      <td>52.416667</td>\n",
       "      <td>48.21</td>\n",
       "      <td>26.93</td>\n",
       "      <td>64.16</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>2.42</td>\n",
       "      <td>1.42</td>\n",
       "      <td>62.98</td>\n",
       "      <td>11.58</td>\n",
       "      <td>4.54</td>\n",
       "      <td>71.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>6.82</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1.068</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.69</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.05</td>\n",
       "      <td>52.416667</td>\n",
       "      <td>26.93</td>\n",
       "      <td>48.21</td>\n",
       "      <td>35.84</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.42</td>\n",
       "      <td>2.42</td>\n",
       "      <td>37.02</td>\n",
       "      <td>4.54</td>\n",
       "      <td>11.58</td>\n",
       "      <td>28.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>93.18</td>\n",
       "      <td>0.932</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.05</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>0.347561</td>\n",
       "      <td>0.591463</td>\n",
       "      <td>48.750000</td>\n",
       "      <td>31.75</td>\n",
       "      <td>58.08</td>\n",
       "      <td>35.34</td>\n",
       "      <td>5.73</td>\n",
       "      <td>5.25</td>\n",
       "      <td>52.18</td>\n",
       "      <td>1.60</td>\n",
       "      <td>3.01</td>\n",
       "      <td>34.78</td>\n",
       "      <td>4.30</td>\n",
       "      <td>13.88</td>\n",
       "      <td>23.67</td>\n",
       "      <td>52.52</td>\n",
       "      <td>51.84</td>\n",
       "      <td>25.11</td>\n",
       "      <td>83.60</td>\n",
       "      <td>1.087</td>\n",
       "      <td>5.950000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.61</td>\n",
       "      <td>3.950000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.76</td>\n",
       "      <td>48.750000</td>\n",
       "      <td>58.08</td>\n",
       "      <td>31.75</td>\n",
       "      <td>64.66</td>\n",
       "      <td>5.25</td>\n",
       "      <td>5.73</td>\n",
       "      <td>47.82</td>\n",
       "      <td>3.01</td>\n",
       "      <td>1.60</td>\n",
       "      <td>65.22</td>\n",
       "      <td>13.88</td>\n",
       "      <td>4.30</td>\n",
       "      <td>76.33</td>\n",
       "      <td>48.16</td>\n",
       "      <td>47.48</td>\n",
       "      <td>16.40</td>\n",
       "      <td>74.89</td>\n",
       "      <td>0.913</td>\n",
       "      <td>3.950000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.76</td>\n",
       "      <td>5.950000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>0.542683</td>\n",
       "      <td>0.365854</td>\n",
       "      <td>55.400000</td>\n",
       "      <td>39.14</td>\n",
       "      <td>40.72</td>\n",
       "      <td>49.02</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "      <td>63.82</td>\n",
       "      <td>2.71</td>\n",
       "      <td>2.64</td>\n",
       "      <td>50.69</td>\n",
       "      <td>12.27</td>\n",
       "      <td>14.91</td>\n",
       "      <td>45.14</td>\n",
       "      <td>12.52</td>\n",
       "      <td>81.25</td>\n",
       "      <td>14.91</td>\n",
       "      <td>92.14</td>\n",
       "      <td>1.071</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.95</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>163.64</td>\n",
       "      <td>36.03</td>\n",
       "      <td>55.400000</td>\n",
       "      <td>40.72</td>\n",
       "      <td>39.14</td>\n",
       "      <td>50.98</td>\n",
       "      <td>2.31</td>\n",
       "      <td>4.07</td>\n",
       "      <td>36.18</td>\n",
       "      <td>2.64</td>\n",
       "      <td>2.71</td>\n",
       "      <td>49.31</td>\n",
       "      <td>14.91</td>\n",
       "      <td>12.27</td>\n",
       "      <td>54.86</td>\n",
       "      <td>18.75</td>\n",
       "      <td>87.48</td>\n",
       "      <td>7.86</td>\n",
       "      <td>85.09</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>163.64</td>\n",
       "      <td>36.03</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1312 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      home_pts_pct  away_pts_pct   home_TOI  home_FF/60  home_FA/60  home_FF%  \\\n",
       "1950      0.000000      1.000000  53.900000       32.64       53.24     38.00   \n",
       "1951      1.000000      0.000000  45.716667       45.29       48.69     48.19   \n",
       "1952      1.000000      0.000000  47.966667       52.57       39.43     57.14   \n",
       "1953      1.000000      0.000000  42.500000       40.03       26.81     59.89   \n",
       "1954      1.000000      0.000000  39.533333       28.64       43.96     39.44   \n",
       "...            ...           ...        ...         ...         ...       ...   \n",
       "3257      0.597561      0.463415  54.450000       32.22       30.41     51.44   \n",
       "3258      0.634146      0.560976  53.783333       39.31       51.30     43.38   \n",
       "3259      0.370370      0.469512  52.416667       48.21       26.93     64.16   \n",
       "3260      0.347561      0.591463  48.750000       31.75       58.08     35.34   \n",
       "3261      0.542683      0.365854  55.400000       39.14       40.72     49.02   \n",
       "\n",
       "      home_GF/60  home_GA/60  home_GF%  home_xGF/60  home_xGA/60  home_xGF%  \\\n",
       "1950        0.00        3.54      0.00         1.32         3.08      30.01   \n",
       "1951        5.07        3.98     56.03         2.96         2.74      51.95   \n",
       "1952        1.18        1.32     47.18         3.38         2.28      59.66   \n",
       "1953        1.39        0.00    100.00         1.74         1.64      51.44   \n",
       "1954        2.93        1.54     65.57         1.83         2.38      43.39   \n",
       "...          ...         ...       ...          ...          ...        ...   \n",
       "3257        2.06        2.28     47.45         2.21         1.37      61.63   \n",
       "3258        2.06        2.36     46.65         2.56         3.93      39.47   \n",
       "3259        2.21        0.00    100.00         2.42         1.42      62.98   \n",
       "3260        5.73        5.25     52.18         1.60         3.01      34.78   \n",
       "3261        4.07        2.31     63.82         2.71         2.64      50.69   \n",
       "\n",
       "      home_HDCF/60  home_HDCA/60  home_HDCF%  home_HDSH%  home_HDSV%  \\\n",
       "1950          3.87         14.71       20.83        0.00       80.38   \n",
       "1951         10.43         14.41       41.98       41.18       58.00   \n",
       "1952         17.55          9.25       65.49        0.00       83.09   \n",
       "1953          4.27          1.42       75.12        0.00      100.00   \n",
       "1954         10.54          6.01       63.69       19.56      100.00   \n",
       "...            ...           ...         ...         ...         ...   \n",
       "3257         11.32          5.80       66.12       51.77       50.19   \n",
       "3258         13.96         12.29       53.19        0.00      100.00   \n",
       "3259         11.58          4.54       71.82        0.00      100.00   \n",
       "3260          4.30         13.88       23.67       52.52       51.84   \n",
       "3261         12.27         14.91       45.14       12.52       81.25   \n",
       "\n",
       "      home_SH%  home_SV%  home_PDO  home_TOI_pp  home_GF/60_pp  \\\n",
       "1950      0.00     90.78     0.908     2.000000           0.00   \n",
       "1951     17.65     87.78     1.054     6.000000           0.00   \n",
       "1952      4.01     95.83     0.998     4.483333          13.38   \n",
       "1953      5.52    100.00     1.055     6.300000          28.57   \n",
       "1954     15.28     94.28     1.096     7.200000          16.67   \n",
       "...        ...       ...       ...          ...            ...   \n",
       "3257     12.11     88.68     1.008     2.566667          23.38   \n",
       "3258      7.13     93.16     1.003     2.000000           0.00   \n",
       "3259      6.82    100.00     1.068     2.000000           0.00   \n",
       "3260     25.11     83.60     1.087     5.950000           0.00   \n",
       "3261     14.91     92.14     1.071     2.000000           0.00   \n",
       "\n",
       "      home_xGF/60_pp  home_TOI_pk  home_GA/60_pk  home_xGA/60_pk   away_TOI  \\\n",
       "1950            6.50     2.000000           0.00            0.00  53.900000   \n",
       "1951           12.34     4.700000           0.00            3.44  45.716667   \n",
       "1952           10.26     4.533333           0.00            5.10  47.966667   \n",
       "1953            6.45     6.900000           8.70           12.10  42.500000   \n",
       "1954           13.03     8.750000           0.00            9.88  39.533333   \n",
       "...              ...          ...            ...             ...        ...   \n",
       "3257            1.87     2.000000           0.00            7.57  54.450000   \n",
       "3258           12.50     4.000000           0.00           17.36  53.783333   \n",
       "3259           12.69     1.600000           0.00           11.05  52.416667   \n",
       "3260           15.61     3.950000           0.00            9.76  48.750000   \n",
       "3261            5.95     0.366667         163.64           36.03  55.400000   \n",
       "\n",
       "      away_FF/60  away_FA/60  away_FF%  away_GF/60  away_GA/60  away_GF%  \\\n",
       "1950       53.24       32.64     62.00        3.54        0.00    100.00   \n",
       "1951       48.69       45.29     51.81        3.98        5.07     43.97   \n",
       "1952       39.43       52.57     42.86        1.32        1.18     52.82   \n",
       "1953       26.81       40.03     40.11        0.00        1.39      0.00   \n",
       "1954       43.96       28.64     60.56        1.54        2.93     34.43   \n",
       "...          ...         ...       ...         ...         ...       ...   \n",
       "3257       30.41       32.22     48.56        2.28        2.06     52.55   \n",
       "3258       51.30       39.31     56.62        2.36        2.06     53.35   \n",
       "3259       26.93       48.21     35.84        0.00        2.21      0.00   \n",
       "3260       58.08       31.75     64.66        5.25        5.73     47.82   \n",
       "3261       40.72       39.14     50.98        2.31        4.07     36.18   \n",
       "\n",
       "      away_xGF/60  away_xGA/60  away_xGF%  away_HDCF/60  away_HDCA/60  \\\n",
       "1950         3.08         1.32      69.99         14.71          3.87   \n",
       "1951         2.74         2.96      48.05         14.41         10.43   \n",
       "1952         2.28         3.38      40.34          9.25         17.55   \n",
       "1953         1.64         1.74      48.56          1.42          4.27   \n",
       "1954         2.38         1.83      56.61          6.01         10.54   \n",
       "...           ...          ...        ...           ...           ...   \n",
       "3257         1.37         2.21      38.37          5.80         11.32   \n",
       "3258         3.93         2.56      60.53         12.29         13.96   \n",
       "3259         1.42         2.42      37.02          4.54         11.58   \n",
       "3260         3.01         1.60      65.22         13.88          4.30   \n",
       "3261         2.64         2.71      49.31         14.91         12.27   \n",
       "\n",
       "      away_HDCF%  away_HDSH%  away_HDSV%  away_SH%  away_SV%  away_PDO  \\\n",
       "1950       79.17       19.62      100.00      9.22    100.00     1.092   \n",
       "1951       58.02       42.00       58.82     12.22     82.35     0.946   \n",
       "1952       34.51       16.91      100.00      4.17     95.99     1.002   \n",
       "1953       24.88        0.00      100.00      0.00     94.48     0.945   \n",
       "1954       36.31        0.00       80.44      5.72     84.72     0.904   \n",
       "...          ...         ...         ...       ...       ...       ...   \n",
       "3257       33.88       49.81       48.23     11.32     87.89     0.992   \n",
       "3258       46.81        0.00      100.00      6.84     92.87     0.997   \n",
       "3259       28.18        0.00      100.00      0.00     93.18     0.932   \n",
       "3260       76.33       48.16       47.48     16.40     74.89     0.913   \n",
       "3261       54.86       18.75       87.48      7.86     85.09     0.929   \n",
       "\n",
       "      away_TOI_pp  away_GF/60_pp  away_xGF/60_pp  away_TOI_pk  away_GA/60_pk  \\\n",
       "1950     2.000000           0.00            0.00     2.000000           0.00   \n",
       "1951     4.700000           0.00            3.44     6.000000           0.00   \n",
       "1952     4.533333           0.00            5.10     4.483333          13.38   \n",
       "1953     6.900000           8.70           12.10     6.300000          28.57   \n",
       "1954     8.750000           0.00            9.88     7.200000          16.67   \n",
       "...           ...            ...             ...          ...            ...   \n",
       "3257     2.000000           0.00            7.57     2.566667          23.38   \n",
       "3258     4.000000           0.00           17.36     2.000000           0.00   \n",
       "3259     1.600000           0.00           11.05     2.000000           0.00   \n",
       "3260     3.950000           0.00            9.76     5.950000           0.00   \n",
       "3261     0.366667         163.64           36.03     2.000000           0.00   \n",
       "\n",
       "      away_xGA/60_pk  \n",
       "1950            6.50  \n",
       "1951           12.34  \n",
       "1952           10.26  \n",
       "1953            6.45  \n",
       "1954           13.03  \n",
       "...              ...  \n",
       "3257            1.87  \n",
       "3258           12.50  \n",
       "3259           12.69  \n",
       "3260           15.61  \n",
       "3261            5.95  \n",
       "\n",
       "[1312 rows x 50 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c06972d",
   "metadata": {},
   "source": [
    "- Currently we don't have any categorical features\n",
    "- will leave the code for categorical features in but commented out in case it's needed in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f7d886f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign training sets of numeric and categorical columns to respective variables\n",
    "num_features = X_train.select_dtypes(['int', 'float']).columns\n",
    "cat_features = X_train.select_dtypes(['object']).columns # no cat features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "db03475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish pipelines for each feature type\n",
    "numeric_pipeline = Pipeline([('ss', StandardScaler())])\n",
    "\n",
    "nominal_pipeline = Pipeline([\n",
    "    ('onehotenc', OneHotEncoder(handle_unknown = 'ignore')), \n",
    "    ('onehotnorm', MaxAbsScaler())])\n",
    "\n",
    "# declare scoring metric list\n",
    "scoring = ['neg_log_loss', 'accuracy']\n",
    "\n",
    "# declare random stame value\n",
    "rnd = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "86ea95a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the column transformer \n",
    "ct = ColumnTransformer(transformers=\n",
    "    [(\"numpipe\", numeric_pipeline, num_features),\n",
    "     (\"nominalpipe\", nominal_pipeline, cat_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3877e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build baseline log reg pipepline \n",
    "steps = [('preprocess', ct), \n",
    "         ('logreg', LogisticRegression(random_state = rnd, max_iter=10000))]\n",
    "\n",
    "base_log_reg_pipeline = Pipeline(steps)\n",
    "\n",
    "base_log_reg_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the pipeline \n",
    "base_log_y_pred = base_log_reg_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7a317409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.856\n",
      "Test F1 score: 0.855\n",
      "Test AUC-ROC score: 0.935\n",
      "Test log loss score: 0.339\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the accuracy of the predictions\n",
    "accuracy = accuracy_score(y_test, base_log_y_pred)\n",
    "print(f'Test accuracy: {accuracy:.3f}')\n",
    "\n",
    "# Calculate the F1 score for the test set\n",
    "f1 = f1_score(y_test, base_log_y_pred,average='macro')\n",
    "print(f'Test F1 score: {f1:.3f}')\n",
    "\n",
    "# Calculate the AUC-ROC score for the test set\n",
    "auc_roc = roc_auc_score(y_test, base_log_reg_pipeline.predict_proba(X_test)[:, 1])\n",
    "print(f'Test AUC-ROC score: {auc_roc:.3f}')\n",
    "\n",
    "# Calculate the Log Loss score for the test set\n",
    "log_loss_score = log_loss(y_test, base_log_reg_pipeline.predict_proba(X_test)[:, 1])\n",
    "print(f'Test log loss score: {log_loss_score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "196c1e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.85       608\n",
      "           1       0.87      0.86      0.86       704\n",
      "\n",
      "    accuracy                           0.86      1312\n",
      "   macro avg       0.86      0.86      0.86      1312\n",
      "weighted avg       0.86      0.86      0.86      1312\n",
      "\n",
      "Test log loss score: 0.339\n",
      "Test AUC-ROC score: 0.935\n"
     ]
    }
   ],
   "source": [
    "# Calculate the confusion matrix\n",
    "label_names = ['0: Away Team Win', '1: Home Team Win']\n",
    "cm = confusion_matrix(y_test, base_log_y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_names)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print the classification report\n",
    "report = classification_report(y_test, base_log_y_pred)\n",
    "print(f'Classification report:\\n{report}')\n",
    "# Calculate the Log Loss score for the test set\n",
    "log_loss_score = log_loss(y_test, base_log_reg_pipeline.predict_proba(X_test)[:, 1])\n",
    "print(f'Test log loss score: {log_loss_score:.3f}')# Calculate the AUC-ROC score for the test set\n",
    "# Calculate the AUC-ROC score for the test set\n",
    "auc_roc = roc_auc_score(y_test, base_log_reg_pipeline.predict_proba(X_test)[:, 1])\n",
    "print(f'Test AUC-ROC score: {auc_roc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e7387d",
   "metadata": {},
   "source": [
    "### Logistic Regression Grid Search v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2e0fc8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up grid search\n",
    "# pipeline steps remain the same\n",
    "steps = [('preprocess', ct), \n",
    "         ('logreg', LogisticRegression(random_state=42, max_iter=10000))]\n",
    "\n",
    "log_cv_pipeline = Pipeline(steps)\n",
    "\n",
    "log_cv_params = {'logreg__solver' : ['liblinear', 'lbfgs', 'newton-cg'],\n",
    "                'logreg__penalty': ['None', 'l1', 'l2'],\n",
    "                'logreg__C': [0.1, 10, 20, 100],\n",
    "                'logreg__class_weight': [None] }\n",
    "\n",
    "log_cv = GridSearchCV(log_cv_pipeline, param_grid=log_cv_params, cv=5, scoring=scoring, \n",
    "                      refit = 'neg_log_loss', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "36f2bded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "100 fits failed out of a total of 180.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "60 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1160, in fit\n",
      "    self._validate_params()\n",
      "  File \"/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/base.py\", line 581, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l1', 'l2', 'none' (deprecated), 'elasticnet'} or None. Got 'None' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [        nan         nan         nan -0.36110491         nan         nan\n",
      " -0.36358108 -0.36354139 -0.36354269         nan         nan         nan\n",
      " -0.36456138         nan         nan -0.36502673 -0.36502191 -0.36501932\n",
      "         nan         nan         nan -0.3648649          nan         nan\n",
      " -0.36507764 -0.36507434 -0.36507759         nan         nan         nan\n",
      " -0.36509922         nan         nan -0.36513531 -0.36509444 -0.36509635]\n",
      "  warnings.warn(\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan 0.84512821        nan        nan\n",
      " 0.84153846 0.84102564 0.84102564        nan        nan        nan\n",
      " 0.84564103        nan        nan 0.84410256 0.84410256 0.84410256\n",
      "        nan        nan        nan 0.84461538        nan        nan\n",
      " 0.84410256 0.84410256 0.84410256        nan        nan        nan\n",
      " 0.84358974        nan        nan 0.84410256 0.84410256 0.84410256]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;preprocess&#x27;,\n",
       "                                        ColumnTransformer(transformers=[(&#x27;numpipe&#x27;,\n",
       "                                                                         Pipeline(steps=[(&#x27;ss&#x27;,\n",
       "                                                                                          StandardScaler())]),\n",
       "                                                                         Index([&#x27;home_pts_pct&#x27;, &#x27;away_pts_pct&#x27;, &#x27;home_TOI&#x27;, &#x27;home_FF/60&#x27;, &#x27;home_FA/60&#x27;,\n",
       "       &#x27;home_FF%&#x27;, &#x27;home_GF/60&#x27;, &#x27;home_GA/60&#x27;, &#x27;home_GF%&#x27;, &#x27;home_xGF/60&#x27;,\n",
       "       &#x27;home_xGA/60&#x27;, &#x27;home_xGF%&#x27;, &#x27;home_HDCF/60&#x27;, &#x27;home_HDCA/60&#x27;,\n",
       "       &#x27;home_HDCF%&#x27;, &#x27;h...\n",
       "                                                                                         (&#x27;onehotnorm&#x27;,\n",
       "                                                                                          MaxAbsScaler())]),\n",
       "                                                                         Index([], dtype=&#x27;object&#x27;))])),\n",
       "                                       (&#x27;logreg&#x27;,\n",
       "                                        LogisticRegression(max_iter=10000,\n",
       "                                                           random_state=42))]),\n",
       "             param_grid={&#x27;logreg__C&#x27;: [0.1, 10, 20, 100],\n",
       "                         &#x27;logreg__class_weight&#x27;: [None],\n",
       "                         &#x27;logreg__penalty&#x27;: [&#x27;None&#x27;, &#x27;l1&#x27;, &#x27;l2&#x27;],\n",
       "                         &#x27;logreg__solver&#x27;: [&#x27;liblinear&#x27;, &#x27;lbfgs&#x27;, &#x27;newton-cg&#x27;]},\n",
       "             refit=&#x27;neg_log_loss&#x27;, scoring=[&#x27;neg_log_loss&#x27;, &#x27;accuracy&#x27;],\n",
       "             verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;preprocess&#x27;,\n",
       "                                        ColumnTransformer(transformers=[(&#x27;numpipe&#x27;,\n",
       "                                                                         Pipeline(steps=[(&#x27;ss&#x27;,\n",
       "                                                                                          StandardScaler())]),\n",
       "                                                                         Index([&#x27;home_pts_pct&#x27;, &#x27;away_pts_pct&#x27;, &#x27;home_TOI&#x27;, &#x27;home_FF/60&#x27;, &#x27;home_FA/60&#x27;,\n",
       "       &#x27;home_FF%&#x27;, &#x27;home_GF/60&#x27;, &#x27;home_GA/60&#x27;, &#x27;home_GF%&#x27;, &#x27;home_xGF/60&#x27;,\n",
       "       &#x27;home_xGA/60&#x27;, &#x27;home_xGF%&#x27;, &#x27;home_HDCF/60&#x27;, &#x27;home_HDCA/60&#x27;,\n",
       "       &#x27;home_HDCF%&#x27;, &#x27;h...\n",
       "                                                                                         (&#x27;onehotnorm&#x27;,\n",
       "                                                                                          MaxAbsScaler())]),\n",
       "                                                                         Index([], dtype=&#x27;object&#x27;))])),\n",
       "                                       (&#x27;logreg&#x27;,\n",
       "                                        LogisticRegression(max_iter=10000,\n",
       "                                                           random_state=42))]),\n",
       "             param_grid={&#x27;logreg__C&#x27;: [0.1, 10, 20, 100],\n",
       "                         &#x27;logreg__class_weight&#x27;: [None],\n",
       "                         &#x27;logreg__penalty&#x27;: [&#x27;None&#x27;, &#x27;l1&#x27;, &#x27;l2&#x27;],\n",
       "                         &#x27;logreg__solver&#x27;: [&#x27;liblinear&#x27;, &#x27;lbfgs&#x27;, &#x27;newton-cg&#x27;]},\n",
       "             refit=&#x27;neg_log_loss&#x27;, scoring=[&#x27;neg_log_loss&#x27;, &#x27;accuracy&#x27;],\n",
       "             verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocess&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;numpipe&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;ss&#x27;,\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  Index([&#x27;home_pts_pct&#x27;, &#x27;away_pts_pct&#x27;, &#x27;home_TOI&#x27;, &#x27;home_FF/60&#x27;, &#x27;home_FA/60&#x27;,\n",
       "       &#x27;home_FF%&#x27;, &#x27;home_GF/60&#x27;, &#x27;home_GA/60&#x27;, &#x27;home_GF%&#x27;, &#x27;home_xGF/60&#x27;,\n",
       "       &#x27;home_xGA/60&#x27;, &#x27;home_xGF%&#x27;, &#x27;home_HDCF/60&#x27;, &#x27;home_HDCA/60&#x27;,\n",
       "       &#x27;home_HDCF%&#x27;, &#x27;home_HDSH%&#x27;, &#x27;home_HDSV%&#x27;, &#x27;hom...\n",
       "       &#x27;away_HDCA/60&#x27;, &#x27;away_HDCF%&#x27;, &#x27;away_HDSH%&#x27;, &#x27;away_HDSV%&#x27;, &#x27;away_SH%&#x27;,\n",
       "       &#x27;away_SV%&#x27;, &#x27;away_PDO&#x27;, &#x27;away_TOI_pp&#x27;, &#x27;away_GF/60_pp&#x27;,\n",
       "       &#x27;away_xGF/60_pp&#x27;, &#x27;away_TOI_pk&#x27;, &#x27;away_GA/60_pk&#x27;, &#x27;away_xGA/60_pk&#x27;],\n",
       "      dtype=&#x27;object&#x27;)),\n",
       "                                                 (&#x27;nominalpipe&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;onehotenc&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)),\n",
       "                                                                  (&#x27;onehotnorm&#x27;,\n",
       "                                                                   MaxAbsScaler())]),\n",
       "                                                  Index([], dtype=&#x27;object&#x27;))])),\n",
       "                (&#x27;logreg&#x27;,\n",
       "                 LogisticRegression(max_iter=10000, random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocess: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;numpipe&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;ss&#x27;, StandardScaler())]),\n",
       "                                 Index([&#x27;home_pts_pct&#x27;, &#x27;away_pts_pct&#x27;, &#x27;home_TOI&#x27;, &#x27;home_FF/60&#x27;, &#x27;home_FA/60&#x27;,\n",
       "       &#x27;home_FF%&#x27;, &#x27;home_GF/60&#x27;, &#x27;home_GA/60&#x27;, &#x27;home_GF%&#x27;, &#x27;home_xGF/60&#x27;,\n",
       "       &#x27;home_xGA/60&#x27;, &#x27;home_xGF%&#x27;, &#x27;home_HDCF/60&#x27;, &#x27;home_HDCA/60&#x27;,\n",
       "       &#x27;home_HDCF%&#x27;, &#x27;home_HDSH%&#x27;, &#x27;home_HDSV%&#x27;, &#x27;home_SH%&#x27;, &#x27;home_SV%&#x27;,\n",
       "       &#x27;home_PDO&#x27;, &#x27;...\n",
       "       &#x27;away_GF%&#x27;, &#x27;away_xGF/60&#x27;, &#x27;away_xGA/60&#x27;, &#x27;away_xGF%&#x27;, &#x27;away_HDCF/60&#x27;,\n",
       "       &#x27;away_HDCA/60&#x27;, &#x27;away_HDCF%&#x27;, &#x27;away_HDSH%&#x27;, &#x27;away_HDSV%&#x27;, &#x27;away_SH%&#x27;,\n",
       "       &#x27;away_SV%&#x27;, &#x27;away_PDO&#x27;, &#x27;away_TOI_pp&#x27;, &#x27;away_GF/60_pp&#x27;,\n",
       "       &#x27;away_xGF/60_pp&#x27;, &#x27;away_TOI_pk&#x27;, &#x27;away_GA/60_pk&#x27;, &#x27;away_xGA/60_pk&#x27;],\n",
       "      dtype=&#x27;object&#x27;)),\n",
       "                                (&#x27;nominalpipe&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;onehotenc&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)),\n",
       "                                                 (&#x27;onehotnorm&#x27;,\n",
       "                                                  MaxAbsScaler())]),\n",
       "                                 Index([], dtype=&#x27;object&#x27;))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">numpipe</label><div class=\"sk-toggleable__content\"><pre>Index([&#x27;home_pts_pct&#x27;, &#x27;away_pts_pct&#x27;, &#x27;home_TOI&#x27;, &#x27;home_FF/60&#x27;, &#x27;home_FA/60&#x27;,\n",
       "       &#x27;home_FF%&#x27;, &#x27;home_GF/60&#x27;, &#x27;home_GA/60&#x27;, &#x27;home_GF%&#x27;, &#x27;home_xGF/60&#x27;,\n",
       "       &#x27;home_xGA/60&#x27;, &#x27;home_xGF%&#x27;, &#x27;home_HDCF/60&#x27;, &#x27;home_HDCA/60&#x27;,\n",
       "       &#x27;home_HDCF%&#x27;, &#x27;home_HDSH%&#x27;, &#x27;home_HDSV%&#x27;, &#x27;home_SH%&#x27;, &#x27;home_SV%&#x27;,\n",
       "       &#x27;home_PDO&#x27;, &#x27;home_TOI_pp&#x27;, &#x27;home_GF/60_pp&#x27;, &#x27;home_xGF/60_pp&#x27;,\n",
       "       &#x27;home_TOI_pk&#x27;, &#x27;home_GA/60_pk&#x27;, &#x27;home_xGA/60_pk&#x27;, &#x27;away_TOI&#x27;,\n",
       "       &#x27;away_FF/60&#x27;, &#x27;away_FA/60&#x27;, &#x27;away_FF%&#x27;, &#x27;away_GF/60&#x27;, &#x27;away_GA/60&#x27;,\n",
       "       &#x27;away_GF%&#x27;, &#x27;away_xGF/60&#x27;, &#x27;away_xGA/60&#x27;, &#x27;away_xGF%&#x27;, &#x27;away_HDCF/60&#x27;,\n",
       "       &#x27;away_HDCA/60&#x27;, &#x27;away_HDCF%&#x27;, &#x27;away_HDSH%&#x27;, &#x27;away_HDSV%&#x27;, &#x27;away_SH%&#x27;,\n",
       "       &#x27;away_SV%&#x27;, &#x27;away_PDO&#x27;, &#x27;away_TOI_pp&#x27;, &#x27;away_GF/60_pp&#x27;,\n",
       "       &#x27;away_xGF/60_pp&#x27;, &#x27;away_TOI_pk&#x27;, &#x27;away_GA/60_pk&#x27;, &#x27;away_xGA/60_pk&#x27;],\n",
       "      dtype=&#x27;object&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">nominalpipe</label><div class=\"sk-toggleable__content\"><pre>Index([], dtype=&#x27;object&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MaxAbsScaler</label><div class=\"sk-toggleable__content\"><pre>MaxAbsScaler()</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=10000, random_state=42)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('preprocess',\n",
       "                                        ColumnTransformer(transformers=[('numpipe',\n",
       "                                                                         Pipeline(steps=[('ss',\n",
       "                                                                                          StandardScaler())]),\n",
       "                                                                         Index(['home_pts_pct', 'away_pts_pct', 'home_TOI', 'home_FF/60', 'home_FA/60',\n",
       "       'home_FF%', 'home_GF/60', 'home_GA/60', 'home_GF%', 'home_xGF/60',\n",
       "       'home_xGA/60', 'home_xGF%', 'home_HDCF/60', 'home_HDCA/60',\n",
       "       'home_HDCF%', 'h...\n",
       "                                                                                         ('onehotnorm',\n",
       "                                                                                          MaxAbsScaler())]),\n",
       "                                                                         Index([], dtype='object'))])),\n",
       "                                       ('logreg',\n",
       "                                        LogisticRegression(max_iter=10000,\n",
       "                                                           random_state=42))]),\n",
       "             param_grid={'logreg__C': [0.1, 10, 20, 100],\n",
       "                         'logreg__class_weight': [None],\n",
       "                         'logreg__penalty': ['None', 'l1', 'l2'],\n",
       "                         'logreg__solver': ['liblinear', 'lbfgs', 'newton-cg']},\n",
       "             refit='neg_log_loss', scoring=['neg_log_loss', 'accuracy'],\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "6226c346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.36110490714177435"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ef82e2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_logreg__C</th>\n",
       "      <th>param_logreg__class_weight</th>\n",
       "      <th>param_logreg__penalty</th>\n",
       "      <th>param_logreg__solver</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_neg_log_loss</th>\n",
       "      <th>split1_test_neg_log_loss</th>\n",
       "      <th>split2_test_neg_log_loss</th>\n",
       "      <th>split3_test_neg_log_loss</th>\n",
       "      <th>split4_test_neg_log_loss</th>\n",
       "      <th>mean_test_neg_log_loss</th>\n",
       "      <th>std_test_neg_log_loss</th>\n",
       "      <th>rank_test_neg_log_loss</th>\n",
       "      <th>split0_test_accuracy</th>\n",
       "      <th>split1_test_accuracy</th>\n",
       "      <th>split2_test_accuracy</th>\n",
       "      <th>split3_test_accuracy</th>\n",
       "      <th>split4_test_accuracy</th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>rank_test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.007481</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>0.002450</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>{'logreg__C': 0.1, 'logreg__class_weight': Non...</td>\n",
       "      <td>-0.364385</td>\n",
       "      <td>-0.381929</td>\n",
       "      <td>-0.353010</td>\n",
       "      <td>-0.358388</td>\n",
       "      <td>-0.347812</td>\n",
       "      <td>-0.361105</td>\n",
       "      <td>0.011782</td>\n",
       "      <td>1</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.838462</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.851282</td>\n",
       "      <td>0.845128</td>\n",
       "      <td>0.014377</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.012287</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'logreg__C': 0.1, 'logreg__class_weight': Non...</td>\n",
       "      <td>-0.364035</td>\n",
       "      <td>-0.382297</td>\n",
       "      <td>-0.353220</td>\n",
       "      <td>-0.365068</td>\n",
       "      <td>-0.353087</td>\n",
       "      <td>-0.363541</td>\n",
       "      <td>0.010679</td>\n",
       "      <td>2</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>0.848718</td>\n",
       "      <td>0.835897</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.851282</td>\n",
       "      <td>0.841026</td>\n",
       "      <td>0.014230</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.019296</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.002733</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'logreg__C': 0.1, 'logreg__class_weight': Non...</td>\n",
       "      <td>-0.364036</td>\n",
       "      <td>-0.382297</td>\n",
       "      <td>-0.353221</td>\n",
       "      <td>-0.365067</td>\n",
       "      <td>-0.353092</td>\n",
       "      <td>-0.363543</td>\n",
       "      <td>0.010678</td>\n",
       "      <td>3</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>0.848718</td>\n",
       "      <td>0.835897</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.851282</td>\n",
       "      <td>0.841026</td>\n",
       "      <td>0.014230</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.007769</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.002387</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.1</td>\n",
       "      <td>None</td>\n",
       "      <td>l2</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>{'logreg__C': 0.1, 'logreg__class_weight': Non...</td>\n",
       "      <td>-0.364051</td>\n",
       "      <td>-0.381692</td>\n",
       "      <td>-0.353778</td>\n",
       "      <td>-0.364755</td>\n",
       "      <td>-0.353630</td>\n",
       "      <td>-0.363581</td>\n",
       "      <td>0.010244</td>\n",
       "      <td>4</td>\n",
       "      <td>0.812821</td>\n",
       "      <td>0.848718</td>\n",
       "      <td>0.835897</td>\n",
       "      <td>0.856410</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.841538</td>\n",
       "      <td>0.016005</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.038720</td>\n",
       "      <td>0.010321</td>\n",
       "      <td>0.002492</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>{'logreg__C': 10, 'logreg__class_weight': None...</td>\n",
       "      <td>-0.366220</td>\n",
       "      <td>-0.389667</td>\n",
       "      <td>-0.347670</td>\n",
       "      <td>-0.366922</td>\n",
       "      <td>-0.352327</td>\n",
       "      <td>-0.364561</td>\n",
       "      <td>0.014654</td>\n",
       "      <td>5</td>\n",
       "      <td>0.823077</td>\n",
       "      <td>0.856410</td>\n",
       "      <td>0.848718</td>\n",
       "      <td>0.851282</td>\n",
       "      <td>0.848718</td>\n",
       "      <td>0.845641</td>\n",
       "      <td>0.011626</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.066629</td>\n",
       "      <td>0.015668</td>\n",
       "      <td>0.002533</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>20</td>\n",
       "      <td>None</td>\n",
       "      <td>l1</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>{'logreg__C': 20, 'logreg__class_weight': None...</td>\n",
       "      <td>-0.366554</td>\n",
       "      <td>-0.389896</td>\n",
       "      <td>-0.348218</td>\n",
       "      <td>-0.367236</td>\n",
       "      <td>-0.352420</td>\n",
       "      <td>-0.364865</td>\n",
       "      <td>0.014608</td>\n",
       "      <td>6</td>\n",
       "      <td>0.823077</td>\n",
       "      <td>0.856410</td>\n",
       "      <td>0.848718</td>\n",
       "      <td>0.848718</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.844615</td>\n",
       "      <td>0.011305</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.033673</td>\n",
       "      <td>0.002121</td>\n",
       "      <td>0.002547</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>l2</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>{'logreg__C': 10, 'logreg__class_weight': None...</td>\n",
       "      <td>-0.366775</td>\n",
       "      <td>-0.389855</td>\n",
       "      <td>-0.348647</td>\n",
       "      <td>-0.367368</td>\n",
       "      <td>-0.352452</td>\n",
       "      <td>-0.365019</td>\n",
       "      <td>0.014501</td>\n",
       "      <td>7</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.856410</td>\n",
       "      <td>0.848718</td>\n",
       "      <td>0.848718</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.844103</td>\n",
       "      <td>0.012286</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.029668</td>\n",
       "      <td>0.002587</td>\n",
       "      <td>0.002362</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'logreg__C': 10, 'logreg__class_weight': None...</td>\n",
       "      <td>-0.366769</td>\n",
       "      <td>-0.389852</td>\n",
       "      <td>-0.348670</td>\n",
       "      <td>-0.367366</td>\n",
       "      <td>-0.352451</td>\n",
       "      <td>-0.365022</td>\n",
       "      <td>0.014495</td>\n",
       "      <td>8</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.856410</td>\n",
       "      <td>0.848718</td>\n",
       "      <td>0.848718</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.844103</td>\n",
       "      <td>0.012286</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.014572</td>\n",
       "      <td>0.000545</td>\n",
       "      <td>0.002386</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>l2</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>{'logreg__C': 10, 'logreg__class_weight': None...</td>\n",
       "      <td>-0.366801</td>\n",
       "      <td>-0.389851</td>\n",
       "      <td>-0.348659</td>\n",
       "      <td>-0.367363</td>\n",
       "      <td>-0.352459</td>\n",
       "      <td>-0.365027</td>\n",
       "      <td>0.014496</td>\n",
       "      <td>9</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.856410</td>\n",
       "      <td>0.848718</td>\n",
       "      <td>0.848718</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.844103</td>\n",
       "      <td>0.012286</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.037975</td>\n",
       "      <td>0.006118</td>\n",
       "      <td>0.002385</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>20</td>\n",
       "      <td>None</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'logreg__C': 20, 'logreg__class_weight': None...</td>\n",
       "      <td>-0.366860</td>\n",
       "      <td>-0.389953</td>\n",
       "      <td>-0.348690</td>\n",
       "      <td>-0.367429</td>\n",
       "      <td>-0.352440</td>\n",
       "      <td>-0.365074</td>\n",
       "      <td>0.014531</td>\n",
       "      <td>10</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.856410</td>\n",
       "      <td>0.848718</td>\n",
       "      <td>0.848718</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.844103</td>\n",
       "      <td>0.012286</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "3        0.007481      0.000686         0.002450        0.000141   \n",
       "7        0.012287      0.000808         0.002703        0.000108   \n",
       "8        0.019296      0.000165         0.002733        0.000205   \n",
       "6        0.007769      0.000310         0.002387        0.000075   \n",
       "12       0.038720      0.010321         0.002492        0.000160   \n",
       "21       0.066629      0.015668         0.002533        0.000114   \n",
       "17       0.033673      0.002121         0.002547        0.000063   \n",
       "16       0.029668      0.002587         0.002362        0.000035   \n",
       "15       0.014572      0.000545         0.002386        0.000059   \n",
       "25       0.037975      0.006118         0.002385        0.000063   \n",
       "\n",
       "   param_logreg__C param_logreg__class_weight param_logreg__penalty  \\\n",
       "3              0.1                       None                    l1   \n",
       "7              0.1                       None                    l2   \n",
       "8              0.1                       None                    l2   \n",
       "6              0.1                       None                    l2   \n",
       "12              10                       None                    l1   \n",
       "21              20                       None                    l1   \n",
       "17              10                       None                    l2   \n",
       "16              10                       None                    l2   \n",
       "15              10                       None                    l2   \n",
       "25              20                       None                    l2   \n",
       "\n",
       "   param_logreg__solver                                             params  \\\n",
       "3             liblinear  {'logreg__C': 0.1, 'logreg__class_weight': Non...   \n",
       "7                 lbfgs  {'logreg__C': 0.1, 'logreg__class_weight': Non...   \n",
       "8             newton-cg  {'logreg__C': 0.1, 'logreg__class_weight': Non...   \n",
       "6             liblinear  {'logreg__C': 0.1, 'logreg__class_weight': Non...   \n",
       "12            liblinear  {'logreg__C': 10, 'logreg__class_weight': None...   \n",
       "21            liblinear  {'logreg__C': 20, 'logreg__class_weight': None...   \n",
       "17            newton-cg  {'logreg__C': 10, 'logreg__class_weight': None...   \n",
       "16                lbfgs  {'logreg__C': 10, 'logreg__class_weight': None...   \n",
       "15            liblinear  {'logreg__C': 10, 'logreg__class_weight': None...   \n",
       "25                lbfgs  {'logreg__C': 20, 'logreg__class_weight': None...   \n",
       "\n",
       "    split0_test_neg_log_loss  split1_test_neg_log_loss  \\\n",
       "3                  -0.364385                 -0.381929   \n",
       "7                  -0.364035                 -0.382297   \n",
       "8                  -0.364036                 -0.382297   \n",
       "6                  -0.364051                 -0.381692   \n",
       "12                 -0.366220                 -0.389667   \n",
       "21                 -0.366554                 -0.389896   \n",
       "17                 -0.366775                 -0.389855   \n",
       "16                 -0.366769                 -0.389852   \n",
       "15                 -0.366801                 -0.389851   \n",
       "25                 -0.366860                 -0.389953   \n",
       "\n",
       "    split2_test_neg_log_loss  split3_test_neg_log_loss  \\\n",
       "3                  -0.353010                 -0.358388   \n",
       "7                  -0.353220                 -0.365068   \n",
       "8                  -0.353221                 -0.365067   \n",
       "6                  -0.353778                 -0.364755   \n",
       "12                 -0.347670                 -0.366922   \n",
       "21                 -0.348218                 -0.367236   \n",
       "17                 -0.348647                 -0.367368   \n",
       "16                 -0.348670                 -0.367366   \n",
       "15                 -0.348659                 -0.367363   \n",
       "25                 -0.348690                 -0.367429   \n",
       "\n",
       "    split4_test_neg_log_loss  mean_test_neg_log_loss  std_test_neg_log_loss  \\\n",
       "3                  -0.347812               -0.361105               0.011782   \n",
       "7                  -0.353087               -0.363541               0.010679   \n",
       "8                  -0.353092               -0.363543               0.010678   \n",
       "6                  -0.353630               -0.363581               0.010244   \n",
       "12                 -0.352327               -0.364561               0.014654   \n",
       "21                 -0.352420               -0.364865               0.014608   \n",
       "17                 -0.352452               -0.365019               0.014501   \n",
       "16                 -0.352451               -0.365022               0.014495   \n",
       "15                 -0.352459               -0.365027               0.014496   \n",
       "25                 -0.352440               -0.365074               0.014531   \n",
       "\n",
       "    rank_test_neg_log_loss  split0_test_accuracy  split1_test_accuracy  \\\n",
       "3                        1              0.820513              0.853846   \n",
       "7                        2              0.815385              0.848718   \n",
       "8                        3              0.815385              0.848718   \n",
       "6                        4              0.812821              0.848718   \n",
       "12                       5              0.823077              0.856410   \n",
       "21                       6              0.823077              0.856410   \n",
       "17                       7              0.820513              0.856410   \n",
       "16                       8              0.820513              0.856410   \n",
       "15                       9              0.820513              0.856410   \n",
       "25                      10              0.820513              0.856410   \n",
       "\n",
       "    split2_test_accuracy  split3_test_accuracy  split4_test_accuracy  \\\n",
       "3               0.838462              0.861538              0.851282   \n",
       "7               0.835897              0.853846              0.851282   \n",
       "8               0.835897              0.853846              0.851282   \n",
       "6               0.835897              0.856410              0.853846   \n",
       "12              0.848718              0.851282              0.848718   \n",
       "21              0.848718              0.848718              0.846154   \n",
       "17              0.848718              0.848718              0.846154   \n",
       "16              0.848718              0.848718              0.846154   \n",
       "15              0.848718              0.848718              0.846154   \n",
       "25              0.848718              0.848718              0.846154   \n",
       "\n",
       "    mean_test_accuracy  std_test_accuracy  rank_test_accuracy  \n",
       "3             0.845128           0.014377                   2  \n",
       "7             0.841026           0.014230                  15  \n",
       "8             0.841026           0.014230                  15  \n",
       "6             0.841538           0.016005                  14  \n",
       "12            0.845641           0.011626                   1  \n",
       "21            0.844615           0.011305                   3  \n",
       "17            0.844103           0.012286                   4  \n",
       "16            0.844103           0.012286                   4  \n",
       "15            0.844103           0.012286                   4  \n",
       "25            0.844103           0.012286                   4  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_results = pd.DataFrame(log_cv.cv_results_).sort_values('mean_test_neg_log_loss', ascending=False)\n",
    "log_results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c153223a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13b8d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5194b563",
   "metadata": {},
   "source": [
    "### Logistic Regression Grid Search v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8ce45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression CV\n",
    "scores = []\n",
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "2e08a5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 13.1 µs\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Model 0. Current Top Score: -0.33204738294671493\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Model 1. Current Top Score: -0.33204738294671493\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Model 2. Current Top Score: -0.33204738294671493\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Model 3. Current Top Score: -0.33204738294671493\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Model 4. Current Top Score: -0.33204738294671493\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Model 5. Current Top Score: -0.33204738294671493\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Model 6. Current Top Score: -0.33204738294671493\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Model 7. Current Top Score: -0.33204738294671493\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Model 8. Current Top Score: -0.33204738294671493\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Model 9. Current Top Score: -0.33204738294671493\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Model 10. Current Top Score: -0.33204738294671493\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "# Regression CV\n",
    "scores = []\n",
    "models = []\n",
    "\n",
    "# Parameters\n",
    "steps = [('preprocess', ct), \n",
    "         ('logreg', LogisticRegression(random_state=42, max_iter=10000))]\n",
    "\n",
    "log_cv_pipeline = Pipeline(steps)\n",
    "\n",
    "logreg_params = {'logreg__solver' : ['liblinear'],\n",
    "                'logreg__penalty': ['l1', 'l2'],\n",
    "                'logreg__C': [0.1, 10, 20, 100],\n",
    "                'logreg__max_iter': [100,1000,10000] }\n",
    "\n",
    "# Model\n",
    "logreg_top_model_pipe = Pipeline(steps)\n",
    "\n",
    "logreg_top_model = GridSearchCV(estimator=logreg_top_model_pipe, param_grid=logreg_params, \n",
    "                                scoring=scoring, refit = 'neg_log_loss', cv=5, verbose=1)\n",
    "logreg_top_model.fit(X_train, y_train)\n",
    "\n",
    "# Displaying Data\n",
    "y_pred = logreg_top_model.predict(X_test)\n",
    "logreg_top_score = logreg_top_model.score(X_test, y_test)\n",
    "\n",
    "# Model Selection\n",
    "for i in range(0, 11):\n",
    "    print(f\"Model {i}. Current Top Score: {logreg_top_score}\")\n",
    "            \n",
    "    # Model Building\n",
    "    logreg_cur_pipe = Pipeline(steps)\n",
    "    logreg_cur_gs = GridSearchCV(estimator=logreg_cur_pipe, param_grid=logreg_params, \n",
    "                                 scoring=scoring, refit = 'neg_log_loss', cv=5, verbose=1)\n",
    "    logreg_cur_gs.fit(X_train, y_train)\n",
    "    \n",
    "    # Comparing and Replacing Data\n",
    "    y_pred = logreg_cur_gs.predict(X_test)\n",
    "    logreg_cur_score = logreg_cur_gs.score(X_test, y_test)\n",
    "    \n",
    "    if logreg_cur_score > logreg_top_score:\n",
    "        logreg_top_model = logreg_cur_gs\n",
    "        logreg_top_score = logreg_cur_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "0aaaded1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Top Score: -0.33204738294671493\n",
      "\n",
      "Best Parameters:\n",
      "logreg__C: 0.1\n",
      "logreg__max_iter: 100\n",
      "logreg__penalty: l1\n",
      "logreg__solver: liblinear\n",
      "\n",
      "Cross Validation\n",
      "Summary Statistics:\n",
      "               0\n",
      "count  10.000000\n",
      "mean   -0.342232\n",
      "std     0.028360\n",
      "min    -0.387646\n",
      "25%    -0.365821\n",
      "50%    -0.336683\n",
      "75%    -0.324910\n",
      "max    -0.303423\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Append top score and top model\n",
    "scores.append(logreg_top_score)\n",
    "models.append(logreg_top_model)\n",
    "        \n",
    "# Displaying Data\n",
    "print(f\"\\nLogistic Regression Top Score: {logreg_top_score}\\n\")\n",
    "print(\"Best Parameters:\")\n",
    "for key, value in list(logreg_top_model.best_params_.items()): print(f\"{key}: {value}\")\n",
    "\n",
    "logreg_cv = cross_val_score(logreg_top_model, X = modeling_df.drop(['season_id', 'Home_Team_Won'], axis=1), \n",
    "                            y = modeling_df['Home_Team_Won'], cv=10, n_jobs=-1)\n",
    "logreg_cv_df = pd.DataFrame(logreg_cv)\n",
    "print(\"\\nCross Validation\\nSummary Statistics:\")\n",
    "print(f\"{logreg_cv_df.describe().to_string()}\\n\")\n",
    "\n",
    "sns.displot(logreg_cv, bins=10)\n",
    "sns.set_style(\"ticks\")\n",
    "plt.title(\"Cross Validation Score Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a680d83",
   "metadata": {},
   "source": [
    "### Gradient Booster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708e41bf",
   "metadata": {},
   "source": [
    "booster 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a59345ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_pipeline = Pipeline(steps=[('preprocess', ct),\n",
    "                      ('gb', GradientBoostingClassifier(random_state=42))])\n",
    "\n",
    "gb_params = {'gb__n_estimators': [200, 300, 400],\n",
    "         'gb__learning_rate': [.001,.01, .1],\n",
    "         'gb__max_depth' : [3,5]}\n",
    "\n",
    "gb_cv = GridSearchCV(gb_pipeline, param_grid=gb_params, cv=5, scoring=scoring, refit='neg_log_loss', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e6bed843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;preprocess&#x27;,\n",
       "                                        ColumnTransformer(transformers=[(&#x27;numpipe&#x27;,\n",
       "                                                                         Pipeline(steps=[(&#x27;ss&#x27;,\n",
       "                                                                                          StandardScaler())]),\n",
       "                                                                         Index([&#x27;home_pts_pct&#x27;, &#x27;away_pts_pct&#x27;, &#x27;home_TOI&#x27;, &#x27;home_FF/60&#x27;, &#x27;home_FA/60&#x27;,\n",
       "       &#x27;home_FF%&#x27;, &#x27;home_GF/60&#x27;, &#x27;home_GA/60&#x27;, &#x27;home_GF%&#x27;, &#x27;home_xGF/60&#x27;,\n",
       "       &#x27;home_xGA/60&#x27;, &#x27;home_xGF%&#x27;, &#x27;home_HDCF/60&#x27;, &#x27;home_HDCA/60&#x27;,\n",
       "       &#x27;home_HDCF%&#x27;, &#x27;h...\n",
       "                                                                         Pipeline(steps=[(&#x27;onehotenc&#x27;,\n",
       "                                                                                          OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)),\n",
       "                                                                                         (&#x27;onehotnorm&#x27;,\n",
       "                                                                                          MaxAbsScaler())]),\n",
       "                                                                         Index([], dtype=&#x27;object&#x27;))])),\n",
       "                                       (&#x27;gb&#x27;,\n",
       "                                        GradientBoostingClassifier(random_state=42))]),\n",
       "             param_grid={&#x27;gb__learning_rate&#x27;: [0.001, 0.01, 0.1],\n",
       "                         &#x27;gb__max_depth&#x27;: [3, 5],\n",
       "                         &#x27;gb__n_estimators&#x27;: [200, 300, 400]},\n",
       "             refit=&#x27;neg_log_loss&#x27;, scoring=[&#x27;neg_log_loss&#x27;, &#x27;accuracy&#x27;],\n",
       "             verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;preprocess&#x27;,\n",
       "                                        ColumnTransformer(transformers=[(&#x27;numpipe&#x27;,\n",
       "                                                                         Pipeline(steps=[(&#x27;ss&#x27;,\n",
       "                                                                                          StandardScaler())]),\n",
       "                                                                         Index([&#x27;home_pts_pct&#x27;, &#x27;away_pts_pct&#x27;, &#x27;home_TOI&#x27;, &#x27;home_FF/60&#x27;, &#x27;home_FA/60&#x27;,\n",
       "       &#x27;home_FF%&#x27;, &#x27;home_GF/60&#x27;, &#x27;home_GA/60&#x27;, &#x27;home_GF%&#x27;, &#x27;home_xGF/60&#x27;,\n",
       "       &#x27;home_xGA/60&#x27;, &#x27;home_xGF%&#x27;, &#x27;home_HDCF/60&#x27;, &#x27;home_HDCA/60&#x27;,\n",
       "       &#x27;home_HDCF%&#x27;, &#x27;h...\n",
       "                                                                         Pipeline(steps=[(&#x27;onehotenc&#x27;,\n",
       "                                                                                          OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)),\n",
       "                                                                                         (&#x27;onehotnorm&#x27;,\n",
       "                                                                                          MaxAbsScaler())]),\n",
       "                                                                         Index([], dtype=&#x27;object&#x27;))])),\n",
       "                                       (&#x27;gb&#x27;,\n",
       "                                        GradientBoostingClassifier(random_state=42))]),\n",
       "             param_grid={&#x27;gb__learning_rate&#x27;: [0.001, 0.01, 0.1],\n",
       "                         &#x27;gb__max_depth&#x27;: [3, 5],\n",
       "                         &#x27;gb__n_estimators&#x27;: [200, 300, 400]},\n",
       "             refit=&#x27;neg_log_loss&#x27;, scoring=[&#x27;neg_log_loss&#x27;, &#x27;accuracy&#x27;],\n",
       "             verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocess&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;numpipe&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;ss&#x27;,\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  Index([&#x27;home_pts_pct&#x27;, &#x27;away_pts_pct&#x27;, &#x27;home_TOI&#x27;, &#x27;home_FF/60&#x27;, &#x27;home_FA/60&#x27;,\n",
       "       &#x27;home_FF%&#x27;, &#x27;home_GF/60&#x27;, &#x27;home_GA/60&#x27;, &#x27;home_GF%&#x27;, &#x27;home_xGF/60&#x27;,\n",
       "       &#x27;home_xGA/60&#x27;, &#x27;home_xGF%&#x27;, &#x27;home_HDCF/60&#x27;, &#x27;home_HDCA/60&#x27;,\n",
       "       &#x27;home_HDCF%&#x27;, &#x27;home_HDSH%&#x27;, &#x27;home_HDSV%&#x27;, &#x27;hom...\n",
       "       &#x27;away_HDCA/60&#x27;, &#x27;away_HDCF%&#x27;, &#x27;away_HDSH%&#x27;, &#x27;away_HDSV%&#x27;, &#x27;away_SH%&#x27;,\n",
       "       &#x27;away_SV%&#x27;, &#x27;away_PDO&#x27;, &#x27;away_TOI_pp&#x27;, &#x27;away_GF/60_pp&#x27;,\n",
       "       &#x27;away_xGF/60_pp&#x27;, &#x27;away_TOI_pk&#x27;, &#x27;away_GA/60_pk&#x27;, &#x27;away_xGA/60_pk&#x27;],\n",
       "      dtype=&#x27;object&#x27;)),\n",
       "                                                 (&#x27;nominalpipe&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;onehotenc&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)),\n",
       "                                                                  (&#x27;onehotnorm&#x27;,\n",
       "                                                                   MaxAbsScaler())]),\n",
       "                                                  Index([], dtype=&#x27;object&#x27;))])),\n",
       "                (&#x27;gb&#x27;, GradientBoostingClassifier(random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocess: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;numpipe&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;ss&#x27;, StandardScaler())]),\n",
       "                                 Index([&#x27;home_pts_pct&#x27;, &#x27;away_pts_pct&#x27;, &#x27;home_TOI&#x27;, &#x27;home_FF/60&#x27;, &#x27;home_FA/60&#x27;,\n",
       "       &#x27;home_FF%&#x27;, &#x27;home_GF/60&#x27;, &#x27;home_GA/60&#x27;, &#x27;home_GF%&#x27;, &#x27;home_xGF/60&#x27;,\n",
       "       &#x27;home_xGA/60&#x27;, &#x27;home_xGF%&#x27;, &#x27;home_HDCF/60&#x27;, &#x27;home_HDCA/60&#x27;,\n",
       "       &#x27;home_HDCF%&#x27;, &#x27;home_HDSH%&#x27;, &#x27;home_HDSV%&#x27;, &#x27;home_SH%&#x27;, &#x27;home_SV%&#x27;,\n",
       "       &#x27;home_PDO&#x27;, &#x27;...\n",
       "       &#x27;away_GF%&#x27;, &#x27;away_xGF/60&#x27;, &#x27;away_xGA/60&#x27;, &#x27;away_xGF%&#x27;, &#x27;away_HDCF/60&#x27;,\n",
       "       &#x27;away_HDCA/60&#x27;, &#x27;away_HDCF%&#x27;, &#x27;away_HDSH%&#x27;, &#x27;away_HDSV%&#x27;, &#x27;away_SH%&#x27;,\n",
       "       &#x27;away_SV%&#x27;, &#x27;away_PDO&#x27;, &#x27;away_TOI_pp&#x27;, &#x27;away_GF/60_pp&#x27;,\n",
       "       &#x27;away_xGF/60_pp&#x27;, &#x27;away_TOI_pk&#x27;, &#x27;away_GA/60_pk&#x27;, &#x27;away_xGA/60_pk&#x27;],\n",
       "      dtype=&#x27;object&#x27;)),\n",
       "                                (&#x27;nominalpipe&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;onehotenc&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)),\n",
       "                                                 (&#x27;onehotnorm&#x27;,\n",
       "                                                  MaxAbsScaler())]),\n",
       "                                 Index([], dtype=&#x27;object&#x27;))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">numpipe</label><div class=\"sk-toggleable__content\"><pre>Index([&#x27;home_pts_pct&#x27;, &#x27;away_pts_pct&#x27;, &#x27;home_TOI&#x27;, &#x27;home_FF/60&#x27;, &#x27;home_FA/60&#x27;,\n",
       "       &#x27;home_FF%&#x27;, &#x27;home_GF/60&#x27;, &#x27;home_GA/60&#x27;, &#x27;home_GF%&#x27;, &#x27;home_xGF/60&#x27;,\n",
       "       &#x27;home_xGA/60&#x27;, &#x27;home_xGF%&#x27;, &#x27;home_HDCF/60&#x27;, &#x27;home_HDCA/60&#x27;,\n",
       "       &#x27;home_HDCF%&#x27;, &#x27;home_HDSH%&#x27;, &#x27;home_HDSV%&#x27;, &#x27;home_SH%&#x27;, &#x27;home_SV%&#x27;,\n",
       "       &#x27;home_PDO&#x27;, &#x27;home_TOI_pp&#x27;, &#x27;home_GF/60_pp&#x27;, &#x27;home_xGF/60_pp&#x27;,\n",
       "       &#x27;home_TOI_pk&#x27;, &#x27;home_GA/60_pk&#x27;, &#x27;home_xGA/60_pk&#x27;, &#x27;away_TOI&#x27;,\n",
       "       &#x27;away_FF/60&#x27;, &#x27;away_FA/60&#x27;, &#x27;away_FF%&#x27;, &#x27;away_GF/60&#x27;, &#x27;away_GA/60&#x27;,\n",
       "       &#x27;away_GF%&#x27;, &#x27;away_xGF/60&#x27;, &#x27;away_xGA/60&#x27;, &#x27;away_xGF%&#x27;, &#x27;away_HDCF/60&#x27;,\n",
       "       &#x27;away_HDCA/60&#x27;, &#x27;away_HDCF%&#x27;, &#x27;away_HDSH%&#x27;, &#x27;away_HDSV%&#x27;, &#x27;away_SH%&#x27;,\n",
       "       &#x27;away_SV%&#x27;, &#x27;away_PDO&#x27;, &#x27;away_TOI_pp&#x27;, &#x27;away_GF/60_pp&#x27;,\n",
       "       &#x27;away_xGF/60_pp&#x27;, &#x27;away_TOI_pk&#x27;, &#x27;away_GA/60_pk&#x27;, &#x27;away_xGA/60_pk&#x27;],\n",
       "      dtype=&#x27;object&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">nominalpipe</label><div class=\"sk-toggleable__content\"><pre>Index([], dtype=&#x27;object&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MaxAbsScaler</label><div class=\"sk-toggleable__content\"><pre>MaxAbsScaler()</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-27\" type=\"checkbox\" ><label for=\"sk-estimator-id-27\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('preprocess',\n",
       "                                        ColumnTransformer(transformers=[('numpipe',\n",
       "                                                                         Pipeline(steps=[('ss',\n",
       "                                                                                          StandardScaler())]),\n",
       "                                                                         Index(['home_pts_pct', 'away_pts_pct', 'home_TOI', 'home_FF/60', 'home_FA/60',\n",
       "       'home_FF%', 'home_GF/60', 'home_GA/60', 'home_GF%', 'home_xGF/60',\n",
       "       'home_xGA/60', 'home_xGF%', 'home_HDCF/60', 'home_HDCA/60',\n",
       "       'home_HDCF%', 'h...\n",
       "                                                                         Pipeline(steps=[('onehotenc',\n",
       "                                                                                          OneHotEncoder(handle_unknown='ignore')),\n",
       "                                                                                         ('onehotnorm',\n",
       "                                                                                          MaxAbsScaler())]),\n",
       "                                                                         Index([], dtype='object'))])),\n",
       "                                       ('gb',\n",
       "                                        GradientBoostingClassifier(random_state=42))]),\n",
       "             param_grid={'gb__learning_rate': [0.001, 0.01, 0.1],\n",
       "                         'gb__max_depth': [3, 5],\n",
       "                         'gb__n_estimators': [200, 300, 400]},\n",
       "             refit='neg_log_loss', scoring=['neg_log_loss', 'accuracy'],\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "bdfc0a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.30937368575177937"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "13c30cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_gb__learning_rate</th>\n",
       "      <th>param_gb__max_depth</th>\n",
       "      <th>param_gb__n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_neg_log_loss</th>\n",
       "      <th>split1_test_neg_log_loss</th>\n",
       "      <th>split2_test_neg_log_loss</th>\n",
       "      <th>split3_test_neg_log_loss</th>\n",
       "      <th>split4_test_neg_log_loss</th>\n",
       "      <th>mean_test_neg_log_loss</th>\n",
       "      <th>std_test_neg_log_loss</th>\n",
       "      <th>rank_test_neg_log_loss</th>\n",
       "      <th>split0_test_accuracy</th>\n",
       "      <th>split1_test_accuracy</th>\n",
       "      <th>split2_test_accuracy</th>\n",
       "      <th>split3_test_accuracy</th>\n",
       "      <th>split4_test_accuracy</th>\n",
       "      <th>mean_test_accuracy</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>rank_test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.444213</td>\n",
       "      <td>0.006379</td>\n",
       "      <td>0.005080</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>{'gb__learning_rate': 0.1, 'gb__max_depth': 3,...</td>\n",
       "      <td>-0.353748</td>\n",
       "      <td>-0.300160</td>\n",
       "      <td>-0.270922</td>\n",
       "      <td>-0.319852</td>\n",
       "      <td>-0.302186</td>\n",
       "      <td>-0.309374</td>\n",
       "      <td>0.027180</td>\n",
       "      <td>1</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.858974</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.009316</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7.811809</td>\n",
       "      <td>0.052949</td>\n",
       "      <td>0.011170</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>400</td>\n",
       "      <td>{'gb__learning_rate': 0.01, 'gb__max_depth': 5...</td>\n",
       "      <td>-0.386269</td>\n",
       "      <td>-0.285233</td>\n",
       "      <td>-0.282272</td>\n",
       "      <td>-0.319555</td>\n",
       "      <td>-0.296626</td>\n",
       "      <td>-0.313991</td>\n",
       "      <td>0.038444</td>\n",
       "      <td>2</td>\n",
       "      <td>0.841026</td>\n",
       "      <td>0.889744</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>0.858974</td>\n",
       "      <td>0.864103</td>\n",
       "      <td>0.864615</td>\n",
       "      <td>0.015756</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.850873</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>0.007882</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>{'gb__learning_rate': 0.01, 'gb__max_depth': 3...</td>\n",
       "      <td>-0.360809</td>\n",
       "      <td>-0.294642</td>\n",
       "      <td>-0.301058</td>\n",
       "      <td>-0.321980</td>\n",
       "      <td>-0.298693</td>\n",
       "      <td>-0.315436</td>\n",
       "      <td>0.024580</td>\n",
       "      <td>3</td>\n",
       "      <td>0.851282</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.879487</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.874359</td>\n",
       "      <td>0.870256</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.661794</td>\n",
       "      <td>0.002410</td>\n",
       "      <td>0.006125</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>{'gb__learning_rate': 0.1, 'gb__max_depth': 3,...</td>\n",
       "      <td>-0.371076</td>\n",
       "      <td>-0.313759</td>\n",
       "      <td>-0.278061</td>\n",
       "      <td>-0.333219</td>\n",
       "      <td>-0.310330</td>\n",
       "      <td>-0.321289</td>\n",
       "      <td>0.030550</td>\n",
       "      <td>4</td>\n",
       "      <td>0.864103</td>\n",
       "      <td>0.856410</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.865128</td>\n",
       "      <td>0.006803</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.873220</td>\n",
       "      <td>0.070303</td>\n",
       "      <td>0.008990</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>{'gb__learning_rate': 0.01, 'gb__max_depth': 5...</td>\n",
       "      <td>-0.389613</td>\n",
       "      <td>-0.295039</td>\n",
       "      <td>-0.293435</td>\n",
       "      <td>-0.327070</td>\n",
       "      <td>-0.305907</td>\n",
       "      <td>-0.322213</td>\n",
       "      <td>0.035774</td>\n",
       "      <td>5</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.887179</td>\n",
       "      <td>0.864103</td>\n",
       "      <td>0.858974</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.018131</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.722074</td>\n",
       "      <td>0.105064</td>\n",
       "      <td>0.006596</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>{'gb__learning_rate': 0.01, 'gb__max_depth': 3...</td>\n",
       "      <td>-0.371390</td>\n",
       "      <td>-0.309679</td>\n",
       "      <td>-0.319898</td>\n",
       "      <td>-0.338080</td>\n",
       "      <td>-0.314745</td>\n",
       "      <td>-0.330759</td>\n",
       "      <td>0.022464</td>\n",
       "      <td>6</td>\n",
       "      <td>0.841026</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.012092</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.880302</td>\n",
       "      <td>0.005427</td>\n",
       "      <td>0.007233</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>{'gb__learning_rate': 0.1, 'gb__max_depth': 3,...</td>\n",
       "      <td>-0.393722</td>\n",
       "      <td>-0.326428</td>\n",
       "      <td>-0.291357</td>\n",
       "      <td>-0.349422</td>\n",
       "      <td>-0.326142</td>\n",
       "      <td>-0.337414</td>\n",
       "      <td>0.033716</td>\n",
       "      <td>7</td>\n",
       "      <td>0.864103</td>\n",
       "      <td>0.864103</td>\n",
       "      <td>0.874359</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.867179</td>\n",
       "      <td>0.003768</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.878138</td>\n",
       "      <td>0.005881</td>\n",
       "      <td>0.007028</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>{'gb__learning_rate': 0.01, 'gb__max_depth': 5...</td>\n",
       "      <td>-0.400536</td>\n",
       "      <td>-0.315743</td>\n",
       "      <td>-0.314402</td>\n",
       "      <td>-0.346330</td>\n",
       "      <td>-0.326365</td>\n",
       "      <td>-0.340675</td>\n",
       "      <td>0.032038</td>\n",
       "      <td>8</td>\n",
       "      <td>0.817949</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.874359</td>\n",
       "      <td>0.864103</td>\n",
       "      <td>0.864103</td>\n",
       "      <td>0.861026</td>\n",
       "      <td>0.022842</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.430673</td>\n",
       "      <td>0.006111</td>\n",
       "      <td>0.005441</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>{'gb__learning_rate': 0.01, 'gb__max_depth': 3...</td>\n",
       "      <td>-0.397539</td>\n",
       "      <td>-0.342401</td>\n",
       "      <td>-0.353935</td>\n",
       "      <td>-0.371804</td>\n",
       "      <td>-0.351061</td>\n",
       "      <td>-0.363348</td>\n",
       "      <td>0.019590</td>\n",
       "      <td>9</td>\n",
       "      <td>0.828205</td>\n",
       "      <td>0.874359</td>\n",
       "      <td>0.851282</td>\n",
       "      <td>0.848718</td>\n",
       "      <td>0.853846</td>\n",
       "      <td>0.851282</td>\n",
       "      <td>0.014685</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.961067</td>\n",
       "      <td>0.027220</td>\n",
       "      <td>0.006896</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>{'gb__learning_rate': 0.1, 'gb__max_depth': 5,...</td>\n",
       "      <td>-0.470260</td>\n",
       "      <td>-0.321982</td>\n",
       "      <td>-0.325812</td>\n",
       "      <td>-0.379522</td>\n",
       "      <td>-0.385615</td>\n",
       "      <td>-0.376638</td>\n",
       "      <td>0.053711</td>\n",
       "      <td>10</td>\n",
       "      <td>0.858974</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.872821</td>\n",
       "      <td>0.014916</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "12       2.444213      0.006379         0.005080        0.000139   \n",
       "11       7.811809      0.052949         0.011170        0.000452   \n",
       "8        4.850873      0.004295         0.007882        0.000156   \n",
       "13       3.661794      0.002410         0.006125        0.000042   \n",
       "10       5.873220      0.070303         0.008990        0.000171   \n",
       "7        3.722074      0.105064         0.006596        0.000162   \n",
       "14       4.880302      0.005427         0.007233        0.000179   \n",
       "9        3.878138      0.005881         0.007028        0.000146   \n",
       "6        2.430673      0.006111         0.005441        0.000086   \n",
       "15       3.961067      0.027220         0.006896        0.000207   \n",
       "\n",
       "   param_gb__learning_rate param_gb__max_depth param_gb__n_estimators  \\\n",
       "12                     0.1                   3                    200   \n",
       "11                    0.01                   5                    400   \n",
       "8                     0.01                   3                    400   \n",
       "13                     0.1                   3                    300   \n",
       "10                    0.01                   5                    300   \n",
       "7                     0.01                   3                    300   \n",
       "14                     0.1                   3                    400   \n",
       "9                     0.01                   5                    200   \n",
       "6                     0.01                   3                    200   \n",
       "15                     0.1                   5                    200   \n",
       "\n",
       "                                               params  \\\n",
       "12  {'gb__learning_rate': 0.1, 'gb__max_depth': 3,...   \n",
       "11  {'gb__learning_rate': 0.01, 'gb__max_depth': 5...   \n",
       "8   {'gb__learning_rate': 0.01, 'gb__max_depth': 3...   \n",
       "13  {'gb__learning_rate': 0.1, 'gb__max_depth': 3,...   \n",
       "10  {'gb__learning_rate': 0.01, 'gb__max_depth': 5...   \n",
       "7   {'gb__learning_rate': 0.01, 'gb__max_depth': 3...   \n",
       "14  {'gb__learning_rate': 0.1, 'gb__max_depth': 3,...   \n",
       "9   {'gb__learning_rate': 0.01, 'gb__max_depth': 5...   \n",
       "6   {'gb__learning_rate': 0.01, 'gb__max_depth': 3...   \n",
       "15  {'gb__learning_rate': 0.1, 'gb__max_depth': 5,...   \n",
       "\n",
       "    split0_test_neg_log_loss  split1_test_neg_log_loss  \\\n",
       "12                 -0.353748                 -0.300160   \n",
       "11                 -0.386269                 -0.285233   \n",
       "8                  -0.360809                 -0.294642   \n",
       "13                 -0.371076                 -0.313759   \n",
       "10                 -0.389613                 -0.295039   \n",
       "7                  -0.371390                 -0.309679   \n",
       "14                 -0.393722                 -0.326428   \n",
       "9                  -0.400536                 -0.315743   \n",
       "6                  -0.397539                 -0.342401   \n",
       "15                 -0.470260                 -0.321982   \n",
       "\n",
       "    split2_test_neg_log_loss  split3_test_neg_log_loss  \\\n",
       "12                 -0.270922                 -0.319852   \n",
       "11                 -0.282272                 -0.319555   \n",
       "8                  -0.301058                 -0.321980   \n",
       "13                 -0.278061                 -0.333219   \n",
       "10                 -0.293435                 -0.327070   \n",
       "7                  -0.319898                 -0.338080   \n",
       "14                 -0.291357                 -0.349422   \n",
       "9                  -0.314402                 -0.346330   \n",
       "6                  -0.353935                 -0.371804   \n",
       "15                 -0.325812                 -0.379522   \n",
       "\n",
       "    split4_test_neg_log_loss  mean_test_neg_log_loss  std_test_neg_log_loss  \\\n",
       "12                 -0.302186               -0.309374               0.027180   \n",
       "11                 -0.296626               -0.313991               0.038444   \n",
       "8                  -0.298693               -0.315436               0.024580   \n",
       "13                 -0.310330               -0.321289               0.030550   \n",
       "10                 -0.305907               -0.322213               0.035774   \n",
       "7                  -0.314745               -0.330759               0.022464   \n",
       "14                 -0.326142               -0.337414               0.033716   \n",
       "9                  -0.326365               -0.340675               0.032038   \n",
       "6                  -0.351061               -0.363348               0.019590   \n",
       "15                 -0.385615               -0.376638               0.053711   \n",
       "\n",
       "    rank_test_neg_log_loss  split0_test_accuracy  split1_test_accuracy  \\\n",
       "12                       1              0.866667              0.861538   \n",
       "11                       2              0.841026              0.889744   \n",
       "8                        3              0.851282              0.884615   \n",
       "13                       4              0.864103              0.856410   \n",
       "10                       5              0.830769              0.887179   \n",
       "7                        6              0.841026              0.876923   \n",
       "14                       7              0.864103              0.864103   \n",
       "9                        8              0.817949              0.884615   \n",
       "6                        9              0.828205              0.874359   \n",
       "15                      10              0.858974              0.900000   \n",
       "\n",
       "    split2_test_accuracy  split3_test_accuracy  split4_test_accuracy  \\\n",
       "12              0.884615              0.858974              0.861538   \n",
       "11              0.869231              0.858974              0.864103   \n",
       "8               0.879487              0.861538              0.874359   \n",
       "13              0.876923              0.861538              0.866667   \n",
       "10              0.864103              0.858974              0.866667   \n",
       "7               0.866667              0.853846              0.861538   \n",
       "14              0.874359              0.866667              0.866667   \n",
       "9               0.874359              0.864103              0.864103   \n",
       "6               0.851282              0.848718              0.853846   \n",
       "15              0.876923              0.866667              0.861538   \n",
       "\n",
       "    mean_test_accuracy  std_test_accuracy  rank_test_accuracy  \n",
       "12            0.866667           0.009316                   6  \n",
       "11            0.864615           0.015756                   8  \n",
       "8             0.870256           0.012200                   2  \n",
       "13            0.865128           0.006803                   7  \n",
       "10            0.861538           0.018131                   9  \n",
       "7             0.860000           0.012092                  11  \n",
       "14            0.867179           0.003768                   5  \n",
       "9             0.861026           0.022842                  10  \n",
       "6             0.851282           0.014685                  12  \n",
       "15            0.872821           0.014916                   1  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_results = pd.DataFrame(gb_cv.cv_results_).sort_values('mean_test_neg_log_loss', ascending=False)\n",
    "gb_results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935c374c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69448163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039e270f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26365c86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8f2df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# monitor training performance\n",
    "from numpy import loadtxt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "# load data\n",
    "dataset = loadtxt('pima-indians-diabetes.csv', delimiter=\",\")\n",
    "# split data into X and y\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n",
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "eval_set = [(X_test, y_test)]\n",
    "model.fit(X_train, y_train, eval_metric=\"error\", eval_set=eval_set, verbose=True)\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4837947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_cl = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "cad37e07",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[181], line 24\u001b[0m\n\u001b[1;32m     13\u001b[0m parameters_grid \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m], \n\u001b[1;32m     14\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.3\u001b[39m],\n\u001b[1;32m     15\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m150\u001b[39m], \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtree_method\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhist\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     21\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_metric\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Define the model to use with GridSearch\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m xgboost \u001b[38;5;241m=\u001b[39m \u001b[43mxgb\u001b[49m\u001b[38;5;241m.\u001b[39mXGBClassifier()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Use GridSearchCV along XGBoost to find the best parameter combination\u001b[39;00m\n\u001b[1;32m     27\u001b[0m xgb_gscv \u001b[38;5;241m=\u001b[39m GridSearchCV(xgboost, parameters_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneg_log_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_train_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xgb' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/jamesbenasuli/miniforge3/envs/sports/lib/python3.8/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Defining the baseline model  without tuning any parameter\n",
    "model = XGBClassifier(random_state=42)\n",
    "results = cross_validate(model, X_train, y_train, cv=5,\n",
    "                         return_train_score=True, scoring=\"neg_log_loss\")\n",
    "                         \n",
    "# Getting the mean score of the 5 folds (either from train and test)\n",
    "baseline_mean_train_score = np.multiply(results[\"train_score\"], -1).mean()\n",
    "baseline_mean_test_score = np.multiply(results[\"test_score\"], -1).mean()\n",
    "\n",
    "#Step 2: Use GridSearchCV for improving the baseline\n",
    "\n",
    "# Defining the parameters grid\n",
    "parameters_grid = {'max_depth': [4, 5, 6], \n",
    "                   'learning_rate': [0.1, 0.2, 0.3],\n",
    "                   'n_estimators': [50, 100, 150], \n",
    "                   'gamma': [0, 20], \n",
    "                   'subsample': [0.8, 1], \n",
    "                   'colsample_bytree': [0.8, 1], \n",
    "                   'lambda': [0, 0.1, 1],\n",
    "                   'tree_method': [\"hist\"],\n",
    "                   'eval_metric': [\"mae\"]}\n",
    "\n",
    "# Define the model to use with GridSearch\n",
    "xgboost = XGBClassifier(random_state=42)\n",
    "\n",
    "# Use GridSearchCV along XGBoost to find the best parameter combination\n",
    "xgb_gscv = GridSearchCV(xgboost, parameters_grid, cv=5, scoring=\"neg_log_loss\", return_train_score=True)\n",
    "xgb_gscv.fit(X_train, y_train)\n",
    "\n",
    "# Get and process the results\n",
    "cv_results = xgb_gscv.cv_results_\n",
    "cv_results = pd.concat([pd.DataFrame(cv_results[\"params\"]),\n",
    "                        pd.DataFrame(cv_results[\"mean_test_score\"], columns=[\"test_mean_mae\"]),\n",
    "                        pd.DataFrame(cv_results[\"mean_train_score\"], columns=[\"train_mean_mae\"])],\n",
    "                       axis=1)\n",
    "cv_results.sort_values(by=\"test_mean_mae\", ascending=False)\n",
    "\n",
    "# Now the baseline has changed\n",
    "best_score = abs(cv_results.sort_values(by=\"test_mean_mae\", ascending=False).iloc[0][\"test_mean_mae\"])\n",
    "\n",
    "#Step 3: Tune parameters individually\n",
    "\n",
    "\n",
    "def get_cross_validate_scores(df_scores, feature, value, model, X_train, y_train, best_score, metric=\"neg_mean_absolute_error\"):\n",
    "    # Train the model with cross_validation and get the results\n",
    "    results = cross_validate(model, X_train, y_train, cv=5,\n",
    "                             return_train_score=True, scoring=metric)\n",
    "    # Calculate the metrics\n",
    "    mean_train_score = np.multiply(results[\"train_score\"], -1).mean()\n",
    "    mean_test_score = np.multiply(results[\"test_score\"], -1).mean()\n",
    "    \n",
    "    # Create different rows to add to the scores.\n",
    "    print(f\"{feature}: {value} - Train Score: {mean_train_score} - Test Score: {mean_test_score}\")\n",
    "    \n",
    "    train_row_df = pd.DataFrame.from_dict({\"value\": value,\n",
    "                                           \"train_test\": \"train\",\n",
    "                                           \"score\": (results[\"train_score\"] * -1)})\n",
    "    test_row_df = pd.DataFrame.from_dict({\"value\": value,\n",
    "                                          \"train_test\": \"test\",\n",
    "                                          \"score\": (results[\"test_score\"] * -1)})\n",
    "    baseline_row_df = pd.DataFrame.from_dict({\"value\": value,\n",
    "                                              \"train_test\": \"best_score\",\n",
    "                                              \"score\": [best_score]})\n",
    "    # Add the scores obtained to the main dataframe\n",
    "    df_scores = pd.concat([df_scores, train_row_df, test_row_df, baseline_row_df])\n",
    "    \n",
    "    return df_scores\n",
    "\n",
    "def plot_parameter_scores(feature, df_scores):\n",
    "    # Create the plot\n",
    "    parameter_plot = sns.relplot(data=df_scores, x=\"value\", y=\"score\", kind=\"line\", hue=\"train_test\")\n",
    "    \n",
    "    # Set the titles and axis labels\n",
    "    parameter_plot.set_titles(f\"{feature} Parameter\")\n",
    "    parameter_plot.set_ylabels(\"MAE\", clear_inner=False)\n",
    "    parameter_plot.set_xlabels(f\"{feature}\", clear_inner=False)\n",
    "    \n",
    "# Defining the best parameters found with GridSearchCV\n",
    "gscv_best_params = {\"colsample_bytree\": 0.8, \"eval_metric\": \"mae\",\n",
    "                    \"gamma\": 0, \"lambda\": 1, \"learning_rate\": 0.1, \n",
    "                    \"max_depth\": 4, \"n_estimators\": 150, \"subsample\": 1.0, \n",
    "                    \"random_state\":42}\n",
    "\n",
    "# Defining, for each parameter, a name without special characters and an array with the values to test\n",
    "main_parameters_dict = {\"n_estimators\": [\"N Estimators\", [125, 150, 175, 200, 225]]}\n",
    "\n",
    "# Iterating over the parameter dictionary\n",
    "for parameter, arguments in main_parameters_dict.items():\n",
    "    # Create an empty dataframe for the scores\n",
    "    df_scores = pd.DataFrame(columns=[\"value\", \"train_test\", \"score\"])\n",
    "    \n",
    "    print(f\"{arguments[0]}\\n\")\n",
    "    \n",
    "    # Depending on the parameter, different models are built\n",
    "    for parameter_value in arguments[1]:\n",
    "        # Getting the best parameters found with GridSearchCV\n",
    "        params_to_use = gscv_best_params.copy()\n",
    "        # Adding the individual value of the parameter that we want to test\n",
    "        params_to_use[parameter] = parameter_value\n",
    "        \n",
    "        # Create the XGBoost model with the parameters\n",
    "        model = xgb.XGBClassifier().set_params(**params_to_use)\n",
    "        \n",
    "        # Adding the cross validate scores of a parameter value to the df_scores dataframe\n",
    "        df_scores = get_cross_validate_scores(df_scores=df_scores, feature=arguments[0], value=parameter_value, model=model,\n",
    "                                              X_train=X_train, y_train=y_train, best_score=best_score)\n",
    "    \n",
    "    # Plot the parameter scores\n",
    "    plot_parameter_scores(arguments[0], df_scores)\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a7d52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Repeat steps 2 and 3.\n",
    "\n",
    "# Using the information gathered before to define the parameters grid\n",
    "parameters_grid = {'max_depth': [4, 5], \n",
    "                   'learning_rate': [0.05, 0.1, 0.15],\n",
    "                   'n_estimators': [150, 300, 450], \n",
    "                   'gamma': [0, 20], \n",
    "                   'subsample': [0.7, 1], \n",
    "                   'colsample_bytree': [0.7, 1], \n",
    "                   'lambda': [0, 0.1, 1],\n",
    "                   'tree_method': [\"hist\"],\n",
    "                   'eval_metric': [\"mae\"]}\n",
    "\n",
    "xgboost = xgb.XGBRegressor()\n",
    "\n",
    "xgb_gscv = GridSearchCV(xgboost, parameters_grid, cv=5, scoring=\"neg_mean_absolute_error\", return_train_score=True)\n",
    "xgb_gscv.fit(X_train, y_train)\n",
    "\n",
    "cv_results = xgb_gscv.cv_results_\n",
    "cv_results = pd.concat([pd.DataFrame(cv_results[\"params\"]),\n",
    "                        pd.DataFrame(cv_results[\"mean_test_score\"], columns=[\"test_mean_mae\"]),\n",
    "                        pd.DataFrame(cv_results[\"mean_train_score\"], columns=[\"train_mean_mae\"])],\n",
    "                       axis=1)\n",
    "cv_results.sort_values(by=\"test_mean_mae\", ascending=False)\n",
    "\n",
    "# Step 5: Use the test set to validate the model.\n",
    "\n",
    "#Finally, we need to validate the model built with the test set \n",
    "# (holdout test, no validation test) of step 0.\n",
    "\n",
    "# Calculate the Mean Absolute Error for the Test Set\n",
    "test_mae = mean_absolute_error(y_true=y_test, y_pred=xgb_gscv.best_estimator_.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f47c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dbb24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build baseline trees pipeplin\n",
    "steps = [('preprocess', ct), \n",
    "         ('rf', RandomForestClassifier(random_state = 42))]\n",
    "\n",
    "rf_pipeline = Pipeline(steps)\n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bed9649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the accuracy of the predictions\n",
    "accuracy = accuracy_score(y_test, base_log_y_pred)\n",
    "print(f'Test accuracy: {accuracy:.3f}')\n",
    "\n",
    "# Calculate the F1 score for the test set\n",
    "f1 = f1_score(y_test, base_log_y_pred,average='macro')\n",
    "print(f'Test F1 score: {f1:.3f}')\n",
    "\n",
    "# Calculate the AUC-ROC score for the test set\n",
    "auc_roc = roc_auc_score(y_test, base_log_reg_pipeline.predict_proba(X_test)[:, 1])\n",
    "print(f'Test AUC-ROC score: {auc_roc:.3f}')\n",
    "\n",
    "# Calculate the Log Loss score for the test set\n",
    "log_loss_score = log_loss(y_test, base_log_reg_pipeline.predict_proba(X_test)[:, 1])\n",
    "print(f'Test log loss score: {log_loss_score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380b3595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4167a94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c617ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0. Current Top Score: 1.0\n",
      "Model 1. Current Top Score: 1.0\n",
      "Model 2. Current Top Score: 1.0\n",
      "Model 3. Current Top Score: 1.0\n",
      "Model 4. Current Top Score: 1.0\n",
      "Model 5. Current Top Score: 1.0\n",
      "Model 6. Current Top Score: 1.0\n",
      "Model 7. Current Top Score: 1.0\n",
      "Model 8. Current Top Score: 1.0\n",
      "Model 9. Current Top Score: 1.0\n",
      "Model 10. Current Top Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression w/ Grid Search\n",
    "\n",
    "# Parameters\n",
    "c = [0.1, 1, 10, 100]\n",
    "max_iter = [100, 1000]\n",
    "solver = ['liblinear']\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "logreg_params = {\n",
    "    'logreg__C': c,\n",
    "    'logreg__max_iter': max_iter,\n",
    "    'logreg__solver': solver,\n",
    "    'logreg__penalty': penalty\n",
    "}\n",
    "\n",
    "# Model\n",
    "logreg_top_model_pipe = Pipeline(steps=[(\"logreg\", LogisticRegression(n_jobs=1))], verbose=False)\n",
    "logreg_top_model = GridSearchCV(estimator=logreg_top_model_pipe, param_grid=logreg_params, scoring='accuracy', cv=5, verbose=0)\n",
    "logreg_top_model.fit(X_train, y_train)\n",
    "\n",
    "# Displaying Data\n",
    "y_pred = logreg_top_model.predict(X_test)\n",
    "logreg_top_score = logreg_top_model.score(X_test, y_test)\n",
    "\n",
    "# Model Selection\n",
    "for i in range(0, 11):\n",
    "    print(f\"Model {i}. Current Top Score: {logreg_top_score}\")\n",
    "        \n",
    "    # Split X and y\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, shuffle=True)\n",
    "    \n",
    "    # Model Building\n",
    "    logreg_cur_pipe = Pipeline(steps=[(\"logreg\", LogisticRegression(n_jobs=1))], verbose=False)\n",
    "    logreg_cur_gs = GridSearchCV(estimator=logreg_cur_pipe, param_grid=logreg_params, scoring='accuracy', cv=5, verbose=0)\n",
    "    logreg_cur_gs.fit(X_train, y_train)\n",
    "    \n",
    "    # Comparing and Replacing Data\n",
    "    y_pred = logreg_cur_gs.predict(X_test)\n",
    "    logreg_cur_score = logreg_cur_gs.score(X_test, y_test)\n",
    "    \n",
    "    if logreg_cur_score > logreg_top_score:\n",
    "        logreg_top_model = logreg_cur_gs\n",
    "        logreg_top_score = logreg_cur_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85310291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510ee8dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af6e5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe316de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The scorers can be either one of the predefined metric strings or a scorer\n",
    "# callable, like the one returned by make_scorer\n",
    "scoring = {\"AUC\": \"roc_auc\", \"Accuracy\": make_scorer(accuracy_score)}\n",
    "\n",
    "# Setting refit='AUC', refits an estimator on the whole dataset with the\n",
    "# parameter setting that has the best cross-validated AUC score.\n",
    "# That estimator is made available at ``gs.best_estimator_`` along with\n",
    "# parameters like ``gs.best_score_``, ``gs.best_params_`` and\n",
    "# ``gs.best_index_``\n",
    "gs = GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    param_grid={\"min_samples_split\": range(2, 403, 20)},\n",
    "    scoring=scoring,\n",
    "    refit=\"AUC\",\n",
    "    n_jobs=2,\n",
    "    return_train_score=True,\n",
    ")\n",
    "gs.fit(X, y)\n",
    "results = gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6889fcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_hastie_10_2(n_samples=8000, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a43ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc768a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "X, y = make_classification(random_state=rng)\n",
    "rf = RandomForestClassifier(random_state=rng)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "rf.fit(X_train, y_train).score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55cf486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0217b03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d086e0aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sports",
   "language": "python",
   "name": "sports"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
